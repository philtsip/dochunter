[
  {
    "url": "https://python.langchain.com/docs/templates/",
    "title": "Templates | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Templates anthropic-iterative-search basic-critique-revise Bedrock JCVD 🕺🥋 cassandra-entomology-rag cassandra-synonym-caching Chain-of-Note (Wikipedia) Chat Bot Feedback Template cohere-librarian csv-agent elastic-query-generator extraction-anthropic-functions extraction-openai-functions guardrails-output-parser Hybrid Search in Weaviate hyde llama2-functions mongo-parent-document-retrieval neo4j-advanced-rag neo4j-cypher-ft neo4j-cypher-memory neo4j_cypher neo4j-generation neo4j-parent neo4j-semantic-layer neo4j-vector-memory nvidia-rag-canonical OpenAI Functions Agent - Gmail openai-functions-agent openai-functions-tool-retrieval-agent pii-protected-chatbot pirate-speak-configurable pirate-speak plate-chain propositional-retrieval python-lint rag-astradb rag-aws-bedrock rag-aws-kendra rag-chroma-multi-modal-multi-vector rag-chroma-multi-modal rag-chroma-private rag-chroma rag-codellama-fireworks rag-conversation-zep rag-conversation rag-elasticsearch rag-fusion rag-gemini-multi-modal rag-google-cloud-sensitive-data-protection rag-google-cloud-vertexai-search rag-gpt-crawler rag-matching-engine rag-momento-vector-index rag-mongo RAG with Multiple Indexes (Fusion) RAG with Multiple Indexes (Routing) rag-multi-modal-local rag-multi-modal-mv-local rag-ollama-multi-query rag-opensearch rag-pinecone-multi-query rag-pinecone-rerank rag-pinecone rag-redis rag-self-query rag-semi-structured rag-singlestoredb rag_supabase rag-timescale-conversation RAG with Timescale Vector using hybrid search rag-vectara-multiquery rag-vectara rag-weaviate research-assistant retrieval-agent rewrite_retrieve_read Langchain - Robocorp Action Server self-query-qdrant self-query-supabase skeleton-of-thought solo-performance-prompting-agent sql-llama2 sql-llamacpp sql-ollama sql-pgvector sql-research-assistant stepback-qa-prompting summarize-anthropic vertexai-chuck-norris xml-agent Templates Templates  Highlighting a few different categories of templates  ⭐ Popular​  These are some of the more popular templates to get started with.  Retrieval Augmented Generation Chatbot: Build a chatbot over your data. Defaults to OpenAI and Pinecone. Extraction with OpenAI Functions: Do extraction of structured data from unstructured data. Uses OpenAI function calling. Local Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. OpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com. 📥 Advanced Retrieval​  These templates cover advanced retrieval techniques, which can be used for chat and QA over databases or documents.  Reranking: This retrieval technique uses Cohere's reranking endpoint to rerank documents from an initial retrieval step. Anthropic Iterative Search: This retrieval technique uses iterative prompting to determine what to retrieve and whether the retriever documents are good enough. Parent Document Retrieval using Neo4j or MongoDB: This retrieval technique stores embeddings for smaller chunks, but then returns larger chunks to pass to the model for generation. Semi-Structured RAG: The template shows how to do retrieval over semi-structured data (e.g. data that involves both text and tables). Temporal RAG: The template shows how to do hybrid search over data with a time-based component using Timescale Vector. 🔍Advanced Retrieval - Query Transformation​  A selection of advanced retrieval methods that involve transforming the original user query, which can improve retrieval quality.  Hypothetical Document Embeddings: A retrieval technique that generates a hypothetical document for a given query, and then uses the embedding of that document to do semantic search. Paper. Rewrite-Retrieve-Read: A retrieval technique that rewrites a given query before passing it to a search engine. Paper. Step-back QA Prompting: A retrieval technique that generates a \"step-back\" question and then retrieves documents relevant to both that question and the original question. Paper. RAG-Fusion: A retrieval technique that generates multiple queries and then reranks the retrieved documents using reciprocal rank fusion. Article. Multi-Query Retriever: This retrieval technique uses an LLM to generate multiple queries and then fetches documents for all queries. 🧠Advanced Retrieval - Query Construction​  A selection of advanced retrieval methods that involve constructing a query in a separate DSL from natural language, which enable natural language chat over various structured databases.  Elastic Query Generator: Generate elastic search queries from natural language. Neo4j Cypher Generation: Generate cypher statements from natural language. Available with a \"full text\" option as well. Supabase Self Query: Parse a natural language query into a semantic query as well as a metadata filter for Supabase. 🦙 OSS Models​  These templates use OSS models, which enable privacy for sensitive data.  Local Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. SQL Question Answering (Replicate): Question answering over a SQL database, using Llama2 hosted on Replicate. SQL Question Answering (LlamaCpp): Question answering over a SQL database, using Llama2 through LlamaCpp. SQL Question Answering (Ollama): Question answering over a SQL database, using Llama2 through Ollama. ⛏️ Extraction​  These templates extract data in a structured format based upon a user-specified schema.  Extraction Using OpenAI Functions: Extract information from text using OpenAI Function Calling. Extraction Using Anthropic Functions: Extract information from text using a LangChain wrapper around the Anthropic endpoints intended to simulate function calling. Extract BioTech Plate Data: Extract microplate data from messy Excel spreadsheets into a more normalized format. ⛏️Summarization and tagging​  These templates summarize or categorize documents and text.  Summarization using Anthropic: Uses Anthropic's Claude2 to summarize long documents. 🤖 Agents​  These templates build chatbots that can take actions, helping to automate tasks.  OpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com. 🚨 Safety and evaluation​  These templates enable moderation or evaluation of LLM outputs.  Guardrails Output Parser: Use guardrails-ai to validate LLM output. Chatbot Feedback: Use LangSmith to evaluate chatbot responses. Next anthropic-iterative-search ⭐ Popular 📥 Advanced Retrieval 🔍Advanced Retrieval - Query Transformation 🧠Advanced Retrieval - Query Construction 🦙 OSS Models ⛏️ Extraction ⛏️Summarization and tagging 🤖 Agents 🚨 Safety and evaluation Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/get_started",
    "title": "Get started | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Get started Get started  Get started with LangChain  📄️ Introduction  LangChain is a framework for developing applications powered by language models. It enables applications that:  📄️ Installation  Official release  📄️ Quickstart  In this quickstart we'll show you how to:  📄️ Security  LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.  Next Introduction Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/get_started",
    "title": "Get started | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageGet started Get started  LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.  Basic example: prompt + model + output parser​  The most basic and common use case is chaining a prompt template and a model together. To see how this works, let’s create a chain that takes a topic and generates a joke:  %pip install –upgrade –quiet langchain-core langchain-community langchain-openai  from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI  prompt = ChatPromptTemplate.from_template(\"tell me a short joke about {topic}\") model = ChatOpenAI(model=\"gpt-4\") output_parser = StrOutputParser()  chain = prompt | model | output_parser  chain.invoke({\"topic\": \"ice cream\"})  \"Why don't ice creams ever get invited to parties?\\n\\nBecause they always drip when things heat up!\"   Notice this line of this code, where we piece together then different components into a single chain using LCEL:  chain = prompt | model | output_parser   The | symbol is similar to a unix pipe operator, which chains together the different components feeds the output from one component as input into the next component.  In this chain the user input is passed to the prompt template, then the prompt template output is passed to the model, then the model output is passed to the output parser. Let’s take a look at each component individually to really understand what’s going on.  1. Prompt​  prompt is a BasePromptTemplate, which means it takes in a dictionary of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.  prompt_value = prompt.invoke({\"topic\": \"ice cream\"}) prompt_value  ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])  prompt_value.to_messages()  [HumanMessage(content='tell me a short joke about ice cream')]  prompt_value.to_string()  'Human: tell me a short joke about ice cream'  2. Model​  The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.  message = model.invoke(prompt_value) message  AIMessage(content=\"Why don't ice creams ever get invited to parties?\\n\\nBecause they always bring a melt down!\")   If our model was an LLM, it would output a string.  from langchain_openai.llms import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\") llm.invoke(prompt_value)  '\\n\\nRobot: Why did the ice cream truck break down? Because it had a meltdown!'  3. Output parser​  And lastly we pass our model output to the output_parser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StrOutputParser specifically simple converts any input into a string.  output_parser.invoke(message)  \"Why did the ice cream go to therapy? \\n\\nBecause it had too many toppings and couldn't find its cone-fidence!\"  4. Entire Pipeline​  To follow the steps along:  We pass in user input on the desired topic as {\"topic\": \"ice cream\"} The prompt component takes the user input, which is then used to construct a PromptValue after using the topic to construct the prompt. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. Dict PromptValue ChatMessage String Input: topic=ice cream PromptTemplate ChatModel StrOutputParser Result  Note that if you’re curious about the output of any components, you can always test out a smaller version of the chain such as prompt or prompt | model to see the intermediate results:  input = {\"topic\": \"ice cream\"}  prompt.invoke(input) # > ChatPromptValue(messages=[HumanMessage(content='tell me a short joke about ice cream')])  (prompt | model).invoke(input) # > AIMessage(content=\"Why did the ice cream go to therapy?\\nBecause it had too many toppings and couldn't cone-trol itself!\")  RAG Search Example​  For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.  # Requires: # pip install langchain docarray tiktoken  from langchain_community.vectorstores import DocArrayInMemorySearch from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnableParallel, RunnablePassthrough from langchain_openai.chat_models import ChatOpenAI from langchain_openai.embeddings import OpenAIEmbeddings  vectorstore = DocArrayInMemorySearch.from_texts(     [\"harrison worked at kensho\", \"bears like to eat honey\"],     embedding=OpenAIEmbeddings(), ) retriever = vectorstore.as_retriever()  template = \"\"\"Answer the question based only on the following context: {context}  Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(template) model = ChatOpenAI() output_parser = StrOutputParser()  setup_and_retrieval = RunnableParallel(     {\"context\": retriever, \"question\": RunnablePassthrough()} ) chain = setup_and_retrieval | prompt | model | output_parser  chain.invoke(\"where did harrison work?\")   In this case, the composed chain is:  chain = setup_and_retrieval | prompt | model | output_parser   To explain this, we first can see that the prompt template above takes in context and question as values to be substituted in the prompt. Before building the prompt template, we want to retrieve relevant documents to the search and include them as part of the context.  As a preliminary step, we’ve setup the retriever using an in memory store, which can retrieve documents based on a query. This is a runnable component as well that can be chained together with other components, but you can also try to run it separately:  retriever.invoke(\"where did harrison work?\")   We then use the RunnableParallel to prepare the expected inputs into the prompt by using the entries for the retrieved documents as well as the original user question, using the retriever for document search, and RunnablePassthrough to pass the user’s question:  setup_and_retrieval = RunnableParallel(     {\"context\": retriever, \"question\": RunnablePassthrough()} )   To review, the complete chain is:  setup_and_retrieval = RunnableParallel(     {\"context\": retriever, \"question\": RunnablePassthrough()} ) chain = setup_and_retrieval | prompt | model | output_parser   With the flow being:  The first steps create a RunnableParallel object with two entries. The first entry, context will include the document results fetched by the retriever. The second entry, question will contain the user’s original question. To pass on the question, we use RunnablePassthrough to copy this entry. Feed the dictionary from the step above to the prompt component. It then takes the user input which is question as well as the retrieved document which is context to construct a prompt and output a PromptValue. The model component takes the generated prompt, and passes into the OpenAI LLM model for evaluation. The generated output from the model is a ChatMessage object. Finally, the output_parser component takes in a ChatMessage, and transforms this into a Python string, which is returned from the invoke method. Question Question context=retrieved docs question=Question PromptValue ChatMessage String Question RunnableParallel Retriever RunnablePassThrough PromptTemplate ChatModel StrOutputParser Result Next steps​  We recommend reading our Why use LCEL section next to see a side-by-side comparison of the code needed to produce common functionality with and without LCEL.  Previous LangChain Expression Language (LCEL) Next Why use LCEL Basic example: prompt + model + output parser 1. Prompt 2. Model 3. Output parser 4. Entire Pipeline RAG Search Example Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/get_started/introduction",
    "title": "Introduction | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Get startedIntroduction Introduction  LangChain is a framework for developing applications powered by language models. It enables applications that:  Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)  This framework consists of several parts.  LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks. LangServe: A library for deploying LangChain chains as a REST API. LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.  Together, these products simplify the entire application lifecycle:  Develop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. Deploy: Turn any chain into an API with LangServe. LangChain Libraries​  The main value props of the LangChain packages are:  Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks  Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.  The LangChain libraries themselves are made up of several different packages.  langchain-core: Base abstractions and LangChain Expression Language. langchain-community: Third party integrations. langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. Get started​  Here’s how to install LangChain, set up your environment, and start building.  We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.  Read up on our Security best practices to make sure you're developing safely with LangChain.  NOTE  These docs focus on the Python LangChain library. Head here for docs on the JavaScript LangChain library.  LangChain Expression Language (LCEL)​  LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.  Overview: LCEL and its benefits Interface: The standard interface for LCEL objects How-to: Key features of LCEL Cookbook: Example code for accomplishing common tasks Modules​  LangChain provides standard, extendable interfaces and integrations for the following modules:  Model I/O​  Interface with language models  Retrieval​  Interface with application-specific data  Agents​  Let models choose which tools to use given high-level directives  Examples, ecosystem, and resources​ Use cases​  Walkthroughs and techniques for common end-to-end use cases, like:  Document question answering Chatbots Analyzing structured data and much more... Integrations​  LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.  Guides​  Best practices for developing with LangChain.  API reference​  Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental Python packages.  Developer's guide​  Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.  Community​  Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM’s.  Previous Get started Next Installation LangChain Libraries Get started LangChain Expression Language (LCEL) Modules Examples, ecosystem, and resources Use cases Integrations Guides API reference Developer's guide Community Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/templates",
    "title": "Templates | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Templates anthropic-iterative-search basic-critique-revise Bedrock JCVD 🕺🥋 cassandra-entomology-rag cassandra-synonym-caching Chain-of-Note (Wikipedia) Chat Bot Feedback Template cohere-librarian csv-agent elastic-query-generator extraction-anthropic-functions extraction-openai-functions guardrails-output-parser Hybrid Search in Weaviate hyde llama2-functions mongo-parent-document-retrieval neo4j-advanced-rag neo4j-cypher-ft neo4j-cypher-memory neo4j_cypher neo4j-generation neo4j-parent neo4j-semantic-layer neo4j-vector-memory nvidia-rag-canonical OpenAI Functions Agent - Gmail openai-functions-agent openai-functions-tool-retrieval-agent pii-protected-chatbot pirate-speak-configurable pirate-speak plate-chain propositional-retrieval python-lint rag-astradb rag-aws-bedrock rag-aws-kendra rag-chroma-multi-modal-multi-vector rag-chroma-multi-modal rag-chroma-private rag-chroma rag-codellama-fireworks rag-conversation-zep rag-conversation rag-elasticsearch rag-fusion rag-gemini-multi-modal rag-google-cloud-sensitive-data-protection rag-google-cloud-vertexai-search rag-gpt-crawler rag-matching-engine rag-momento-vector-index rag-mongo RAG with Multiple Indexes (Fusion) RAG with Multiple Indexes (Routing) rag-multi-modal-local rag-multi-modal-mv-local rag-ollama-multi-query rag-opensearch rag-pinecone-multi-query rag-pinecone-rerank rag-pinecone rag-redis rag-self-query rag-semi-structured rag-singlestoredb rag_supabase rag-timescale-conversation RAG with Timescale Vector using hybrid search rag-vectara-multiquery rag-vectara rag-weaviate research-assistant retrieval-agent rewrite_retrieve_read Langchain - Robocorp Action Server self-query-qdrant self-query-supabase skeleton-of-thought solo-performance-prompting-agent sql-llama2 sql-llamacpp sql-ollama sql-pgvector sql-research-assistant stepback-qa-prompting summarize-anthropic vertexai-chuck-norris xml-agent Templates Templates  Highlighting a few different categories of templates  ⭐ Popular​  These are some of the more popular templates to get started with.  Retrieval Augmented Generation Chatbot: Build a chatbot over your data. Defaults to OpenAI and Pinecone. Extraction with OpenAI Functions: Do extraction of structured data from unstructured data. Uses OpenAI function calling. Local Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. OpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com. 📥 Advanced Retrieval​  These templates cover advanced retrieval techniques, which can be used for chat and QA over databases or documents.  Reranking: This retrieval technique uses Cohere's reranking endpoint to rerank documents from an initial retrieval step. Anthropic Iterative Search: This retrieval technique uses iterative prompting to determine what to retrieve and whether the retriever documents are good enough. Parent Document Retrieval using Neo4j or MongoDB: This retrieval technique stores embeddings for smaller chunks, but then returns larger chunks to pass to the model for generation. Semi-Structured RAG: The template shows how to do retrieval over semi-structured data (e.g. data that involves both text and tables). Temporal RAG: The template shows how to do hybrid search over data with a time-based component using Timescale Vector. 🔍Advanced Retrieval - Query Transformation​  A selection of advanced retrieval methods that involve transforming the original user query, which can improve retrieval quality.  Hypothetical Document Embeddings: A retrieval technique that generates a hypothetical document for a given query, and then uses the embedding of that document to do semantic search. Paper. Rewrite-Retrieve-Read: A retrieval technique that rewrites a given query before passing it to a search engine. Paper. Step-back QA Prompting: A retrieval technique that generates a \"step-back\" question and then retrieves documents relevant to both that question and the original question. Paper. RAG-Fusion: A retrieval technique that generates multiple queries and then reranks the retrieved documents using reciprocal rank fusion. Article. Multi-Query Retriever: This retrieval technique uses an LLM to generate multiple queries and then fetches documents for all queries. 🧠Advanced Retrieval - Query Construction​  A selection of advanced retrieval methods that involve constructing a query in a separate DSL from natural language, which enable natural language chat over various structured databases.  Elastic Query Generator: Generate elastic search queries from natural language. Neo4j Cypher Generation: Generate cypher statements from natural language. Available with a \"full text\" option as well. Supabase Self Query: Parse a natural language query into a semantic query as well as a metadata filter for Supabase. 🦙 OSS Models​  These templates use OSS models, which enable privacy for sensitive data.  Local Retrieval Augmented Generation: Build a chatbot over your data. Uses only local tooling: Ollama, GPT4all, Chroma. SQL Question Answering (Replicate): Question answering over a SQL database, using Llama2 hosted on Replicate. SQL Question Answering (LlamaCpp): Question answering over a SQL database, using Llama2 through LlamaCpp. SQL Question Answering (Ollama): Question answering over a SQL database, using Llama2 through Ollama. ⛏️ Extraction​  These templates extract data in a structured format based upon a user-specified schema.  Extraction Using OpenAI Functions: Extract information from text using OpenAI Function Calling. Extraction Using Anthropic Functions: Extract information from text using a LangChain wrapper around the Anthropic endpoints intended to simulate function calling. Extract BioTech Plate Data: Extract microplate data from messy Excel spreadsheets into a more normalized format. ⛏️Summarization and tagging​  These templates summarize or categorize documents and text.  Summarization using Anthropic: Uses Anthropic's Claude2 to summarize long documents. 🤖 Agents​  These templates build chatbots that can take actions, helping to automate tasks.  OpenAI Functions Agent: Build a chatbot that can take actions. Uses OpenAI function calling and Tavily. XML Agent: Build a chatbot that can take actions. Uses Anthropic and You.com. 🚨 Safety and evaluation​  These templates enable moderation or evaluation of LLM outputs.  Guardrails Output Parser: Use guardrails-ai to validate LLM output. Chatbot Feedback: Use LangSmith to evaluate chatbot responses. Next anthropic-iterative-search ⭐ Popular 📥 Advanced Retrieval 🔍Advanced Retrieval - Query Transformation 🧠Advanced Retrieval - Query Construction 🦙 OSS Models ⛏️ Extraction ⛏️Summarization and tagging 🤖 Agents 🚨 Safety and evaluation Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/packages",
    "title": "📕 Package Versioning | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K 📕 Package Versioning  As of now, LangChain has an ad hoc release process: releases are cut with high frequency by a maintainer and published to PyPI. The different packages are versioned slightly differently.  langchain-core​  langchain-core is currently on version 0.1.x.  As langchain-core contains the base abstractions and runtime for the whole LangChain ecosystem, we will communicate any breaking changes with advance notice and version bumps. The exception for this is anything marked with the beta decorator (you can see this in the API reference and will see warnings when using such functionality). The reason for beta features is that given the rate of change of the field, being able to move quickly is still a priority.  Minor version increases will occur for:  Breaking changes for any public interfaces marked as beta.  Patch version increases will occur for:  Bug fixes New features Any changes to private interfaces Any changes to beta features langchain​  langchain is currently on version 0.1.x  Minor version increases will occur for:  Breaking changes for any public interfaces NOT marked as beta.  Patch version increases will occur for:  Bug fixes New features Any changes to private interfaces Any changes to beta features  We are targeting February 2024 for a release of langchain v0.2, which will have some breaking changes to legacy Chains and Agents. Additionally, we will remove langchain-community as a dependency and stop re-exporting integrations that have been moved to langchain-community.  langchain-community​  langchain-community is currently on version 0.0.x  All changes will be accompanied by a patch version increase.  langchain-experimental​  langchain-experimental is currently on version 0.0.x  All changes will be accompanied by a patch version increase.  Partner Packages​  Partner packages are versioned independently.  langchain-core langchain langchain-community langchain-experimental Partner Packages Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/additional_resources/tutorials",
    "title": "Tutorials | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Tutorials  Below are links to tutorials and courses on LangChain. For written guides on common use cases for LangChain, check out the use cases guides.  ⛓ icon marks a new addition [last update 2023-09-21]  LangChain on Wikipedia​ Books​ ⛓Generative AI with LangChain by Ben Auffrath, ©️ 2023 Packt Publishing​ DeepLearning.AI courses​  by Harrison Chase and Andrew Ng  LangChain for LLM Application Development LangChain Chat with Your Data ⛓ Functions, Tools and Agents with LangChain Handbook​  LangChain AI Handbook By James Briggs and Francisco Ingham  Short Tutorials​  LangChain Explained in 13 Minutes | QuickStart Tutorial for Beginners by Rabbitmetrics  LangChain Crash Course: Build an AutoGPT app in 25 minutes by Nicholas Renotte  LangChain Crash Course - Build apps with language models by Patrick Loeber  Tutorials​ LangChain for Gen AI and LLMs by James Briggs​ #1 Getting Started with GPT-3 vs. Open Source LLMs #2 Prompt Templates for GPT 3.5 and other LLMs #3 LLM Chains using GPT 3.5 and other LLMs LangChain Data Loaders, Tokenizers, Chunking, and Datasets - Data Prep 101 #4 Chatbot Memory for Chat-GPT, Davinci + other LLMs #5 Chat with OpenAI in LangChain #6 Fixing LLM Hallucinations with Retrieval Augmentation in LangChain #7 LangChain Agents Deep Dive with GPT 3.5 #8 Create Custom Tools for Chatbots in LangChain #9 Build Conversational Agents with Vector DBs Using NEW MPT-7B in Hugging Face and LangChain MPT-30B Chatbot with LangChain ⛓ Fine-tuning OpenAI's GPT 3.5 for LangChain Agents ⛓ Chatbots with RAG: LangChain Full Walkthrough LangChain 101 by Greg Kamradt (Data Indy)​ What Is LangChain? - LangChain + ChatGPT Overview Quickstart Guide Beginner's Guide To 7 Essential Concepts Beginner's Guide To 9 Use Cases Agents Overview + Google Searches OpenAI + Wolfram Alpha Ask Questions On Your Custom (or Private) Files Connect Google Drive Files To OpenAI YouTube Transcripts + OpenAI Question A 300 Page Book (w/ OpenAI + Pinecone) Workaround OpenAI's Token Limit With Chain Types Build Your Own OpenAI + LangChain Web App in 23 Minutes Working With The New ChatGPT API OpenAI + LangChain Wrote Me 100 Custom Sales Emails Structured Output From OpenAI (Clean Dirty Data) Connect OpenAI To +5,000 Tools (LangChain + Zapier) Use LLMs To Extract Data From Text (Expert Mode) Extract Insights From Interview Transcripts Using LLMs 5 Levels Of LLM Summarizing: Novice to Expert Control Tone & Writing Style Of Your LLM Output Build Your Own AI Twitter Bot Using LLMs ChatGPT made my interview questions for me (Streamlit + LangChain) Function Calling via ChatGPT API - First Look With LangChain Extract Topics From Video/Audio With LLMs (Topic Modeling w/ LangChain) LangChain How to and guides by Sam Witteveen​ LangChain Basics - LLMs & PromptTemplates with Colab LangChain Basics - Tools and Chains ChatGPT API Announcement & Code Walkthrough with LangChain Conversations with Memory (explanation & code walkthrough) Chat with Flan20B Using Hugging Face Models locally (code walkthrough) PAL: Program-aided Language Models with LangChain code Building a Summarization System with LangChain and GPT-3 - Part 1 Building a Summarization System with LangChain and GPT-3 - Part 2 Microsoft's Visual ChatGPT using LangChain LangChain Agents - Joining Tools and Chains with Decisions Comparing LLMs with LangChain Using Constitutional AI in LangChain Talking to Alpaca with LangChain - Creating an Alpaca Chatbot Talk to your CSV & Excel with LangChain BabyAGI: Discover the Power of Task-Driven Autonomous Agents! Improve your BabyAGI with LangChain Master PDF Chat with LangChain - Your essential guide to queries on documents Using LangChain with DuckDuckGO, Wikipedia & PythonREPL Tools Building Custom Tools and Agents with LangChain (gpt-3.5-turbo) LangChain Retrieval QA Over Multiple Files with ChromaDB LangChain Retrieval QA with Instructor Embeddings & ChromaDB for PDFs LangChain + Retrieval Local LLMs for Retrieval QA - No OpenAI!!! Camel + LangChain for Synthetic Data & Market Research Information Extraction with LangChain & Kor Converting a LangChain App from OpenAI to OpenSource Using LangChain Output Parsers to get what you want out of LLMs Building a LangChain Custom Medical Agent with Memory Understanding ReACT with LangChain OpenAI Functions + LangChain : Building a Multi Tool Agent What can you do with 16K tokens in LangChain? Tagging and Extraction - Classification using OpenAI Functions HOW to Make Conversational Form with LangChain ⛓ Claude-2 meets LangChain! ⛓ PaLM 2 Meets LangChain ⛓ LLaMA2 with LangChain - Basics | LangChain TUTORIAL ⛓ Serving LLaMA2 with Replicate ⛓ NEW LangChain Expression Language ⛓ Building a RCI Chain for Agents with LangChain Expression Language ⛓ How to Run LLaMA-2-70B on the Together AI ⛓ RetrievalQA with LLaMA 2 70b & Chroma DB ⛓ How to use BGE Embeddings for LangChain ⛓ How to use Custom Prompts for RetrievalQA on LLaMA-2 7B LangChain by Prompt Engineering​ LangChain Crash Course — All You Need to Know to Build Powerful Apps with LLMs Working with MULTIPLE PDF Files in LangChain: ChatGPT for your Data ChatGPT for YOUR OWN PDF files with LangChain Talk to YOUR DATA without OpenAI APIs: LangChain LangChain: PDF Chat App (GUI) | ChatGPT for Your PDF FILES LangFlow: Build Chatbots without Writing Code LangChain: Giving Memory to LLMs BEST OPEN Alternative to OPENAI's EMBEDDINGs for Retrieval QA: LangChain LangChain: Run Language Models Locally - Hugging Face Models ⛓ Slash API Costs: Mastering Caching for LLM Applications ⛓ Avoid PROMPT INJECTION with Constitutional AI - LangChain LangChain by Chat with data​ LangChain Beginner's Tutorial for Typescript/Javascript GPT-4 Tutorial: How to Chat With Multiple PDF Files (~1000 pages of Tesla's 10-K Annual Reports) GPT-4 & LangChain Tutorial: How to Chat With A 56-Page PDF Document (w/Pinecone) LangChain & Supabase Tutorial: How to Build a ChatGPT Chatbot For Your Website LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin) Codebase Analysis​ Codebase Analysis: Langchain Agents  ⛓ icon marks a new addition [last update 2023-09-21]  LangChain on Wikipedia Books DeepLearning.AI courses Handbook Short Tutorials Tutorials LangChain for Gen AI and LLMs by James Briggs LangChain 101 by Greg Kamradt (Data Indy) LangChain How to and guides by Sam Witteveen LangChain by Prompt Engineering LangChain by Chat with data Codebase Analysis Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/langsmith/",
    "title": "LangSmith | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangSmith Walkthrough LangGraph LangSmith LangSmith  LangSmith helps you trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.  Check out the interactive walkthrough to get started.  For more information, please refer to the LangSmith documentation.  For tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow, check out the LangSmith Cookbook. Some of the guides therein include:  Leveraging user feedback in your JS application (link). Building an automated feedback pipeline (link). How to evaluate and audit your RAG workflows (link). How to fine-tune an LLM on real usage data (link). How to use the LangChain Hub to version your prompts (link) Previous 🦜️🏓 LangServe Next LangSmith Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/",
    "title": "Providers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores Providers Providers 📄️ Anthropic  All functionality related to Anthropic models.  📄️ AWS  The LangChain integrations related to Amazon AWS platform.  📄️ Google  All functionality related to Google Cloud Platform and other Google products.  📄️ Hugging Face  All functionality related to the Hugging Face Platform.  📄️ Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  📄️ OpenAI  All functionality related to OpenAI  🗃️ More  201 items  Next Anthropic Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/",
    "title": "Q&A with RAG | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAG Q&A with RAG Overview​  One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.  What is RAG?​  RAG is a technique for augmenting LLM knowledge with additional data.  LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).  LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.  Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:  Q&A over structured data (e.g., SQL) Q&A over code (e.g., Python) RAG Architecture​  A typical RAG application has two main components:  Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.  Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.  The most common full sequence from raw data to answer looks like:  Indexing​ Load: First we need to load our data. This is done with DocumentLoaders. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.  Retrieval and generation​ Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data  Table of contents​ Quickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart. Returning sources: How to return the source documents used in a particular generation. Streaming: How to stream final answers as well as intermediate steps. Adding chat history: How to add chat history to a Q&A app. Per-user retrieval: How to do retrieval when each user has their own private data. Using agents: How to use agents for Q&A. Using local models: How to use local models for Q&A. Previous Use cases Next Quickstart Overview What is RAG? RAG Architecture Table of contents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/changelog",
    "title": "Changelog | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Changelog langchain-core langchain Changelog Changelog 📄️ langchain-core  0.1.7 (Jan 5, 2024)  📄️ langchain  0.1.0 (Jan 5, 2024)  Next langchain-core Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/agents/",
    "title": "Agents | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Quickstart Concepts Agent Types How-to Tools Chains More LangServe LangSmith LangGraph ModulesAgents Agents  The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.  Quickstart​  For a quick start to working with agents, please check out this getting started guide. This covers basics like initializing an agent, creating tools, and adding memory.  Concepts​  There are several key concepts to understand when building agents: Agents, AgentExecutor, Tools, Toolkits. For an in depth explanation, please check out this conceptual guide  Agent Types​  There are many different types of agents to use. For a overview of the different types and when to use them, please check out this section.  Tools​  Agents are only as good as the tools they have. For a comprehensive guide on tools, please see this section.  How To Guides​  Agents have a lot of related functionality! Check out comprehensive guides including:  Building a custom agent Streaming (of both intermediate steps and tokens Building an agent that returns structured output Lots functionality around using AgentExecutor, including: using it as an iterator, handle parsing errors, returning intermediate steps, capping the max number of iterations, and timeouts for agents Previous Indexing Next Quickstart Quickstart Concepts Agent Types Tools How To Guides Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/model_io/",
    "title": "Model I/O | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Concepts Prompts Quickstart LLMs Chat Models Output Parsers Retrieval Agents Chains More LangServe LangSmith LangGraph ModulesModel I/O Model I/O  The core element of any language model application is...the model. LangChain gives you the building blocks to interface with any language model.  Conceptual Guide​  A conceptual explanation of messages, prompts, LLMs vs ChatModels, and output parsers. You should read this before getting started.  Quickstart​  Covers the basics of getting started working with different types of models. You should walk through [this section] if you want to get an overview of the functionality.  Prompts​  This section deep dives into the different types of prompt templates and how to use them.  LLMs​  This section covers functionality related to the LLM class. This is a type of model that takes a text string as input and returns a text string.  ChatModels​  This section covers functionality related to the ChatModel class. This is a type of model that takes a list of messages as input and returns a message.  Output Parsers​  Output parsers are responsible for transforming the output of LLMs and ChatModels into more structured data. This section covers the different types of output parsers.  Previous Modules Next Concepts Conceptual Guide Quickstart Prompts LLMs ChatModels Output Parsers Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases",
    "title": "Q&A with RAG | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAG Q&A with RAG Overview​  One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or RAG.  What is RAG?​  RAG is a technique for augmenting LLM knowledge with additional data.  LLMs can reason about wide-ranging topics, but their knowledge is limited to the public data up to a specific point in time that they were trained on. If you want to build AI applications that can reason about private data or data introduced after a model’s cutoff date, you need to augment the knowledge of the model with the specific information it needs. The process of bringing the appropriate information and inserting it into the model prompt is known as Retrieval Augmented Generation (RAG).  LangChain has a number of components designed to help build Q&A applications, and RAG applications more generally.  Note: Here we focus on Q&A for unstructured data. Two RAG use cases which we cover elsewhere are:  Q&A over structured data (e.g., SQL) Q&A over code (e.g., Python) RAG Architecture​  A typical RAG application has two main components:  Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.  Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.  The most common full sequence from raw data to answer looks like:  Indexing​ Load: First we need to load our data. This is done with DocumentLoaders. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model.  Retrieval and generation​ Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data  Table of contents​ Quickstart: We recommend starting here. Many of the following guides assume you fully understand the architecture shown in the Quickstart. Returning sources: How to return the source documents used in a particular generation. Streaming: How to stream final answers as well as intermediate steps. Adding chat history: How to add chat history to a Q&A app. Per-user retrieval: How to do retrieval when each user has their own private data. Using agents: How to use agents for Q&A. Using local models: How to use local models for Q&A. Previous Use cases Next Quickstart Overview What is RAG? RAG Architecture Table of contents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/chains",
    "title": "Chains | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph ModulesChains Chains  Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.  LCEL is great for constructing your own chains, but it’s also nice to have chains that you can use off-the-shelf. There are two types of off-the-shelf chains that LangChain supports:  Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL.  [Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are rather standalone classes.  We are working creating methods that create LCEL versions of all chains. We are doing this for a few reasons.  Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL.  These chains natively support streaming, async, and batch out of the box.  These chains automatically get observability at each step.  This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.  LCEL Chains​  Below is a table of all LCEL chain constructors. In addition, we report on:  Chain Constructor  The constructor function for this chain. These are all methods that return LCEL runnables. We also link to the API documentation.  Function Calling  Whether this requires OpenAI function calling.  Other Tools  What other tools (if any) are used in this chain.  When to Use  Our commentary on when to use this chain.  Chain Constructor Function Calling Other Tools When to Use create_stuff_documents_chain   This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using. create_openai_fn_runnable ✅  If you want to use OpenAI function calling to OPTIONALLY structured an output response. You may pass in multiple functions for it call, but it does not have to call it. create_structured_output_runnable ✅  If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response. load_query_constructor_runnable   Can be used to generates queries. You must specify a list of allowed operations, and then will return a runnable that converts a natural language query into those allowed operations. create_sql_query_chain  SQL Database If you want to construct a query for a SQL database from natural language. create_history_aware_retriever  Retriever This chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever. create_retrieval_chain  Retriever This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response Legacy Chains​  Below we report on the legacy chain types that exist. We will maintain support for these until we are able to create a LCEL alternative. We report on:  Chain  Name of the chain, or name of the constructor method. If constructor method, this will return a Chain subclass.  Function Calling  Whether this requires OpenAI Function Calling.  Other Tools  Other tools used in the chain.  When to Use  Our commentary on when to use.  Chain Function Calling Other Tools When to Use APIChain  Requests Wrapper This chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respond OpenAPIEndpointChain  OpenAPI Spec Similar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpoints ConversationalRetrievalChain  Retriever This chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond. StuffDocumentsChain   This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using. ReduceDocumentsChain   This chain combines documents by iterative reducing them. It groups documents into chunks (less than some context length) then passes them into an LLM. It then takes the responses and continues to do this until it can fit everything into one final LLM call. Useful when you have a lot of documents, you want to have the LLM run over all of them, and you can do in parallel. MapReduceDocumentsChain   This chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents. RefineDocumentsChain   This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls). MapRerankDocumentsChain   This calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do). ConstitutionalChain   This chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this when you want to enforce that a chain’s answer follows some principles. LLMChain    ElasticsearchDatabaseChain  ElasticSearch Instance This chain converts a natural language question to an ElasticSearch query, and then runs it, and then summarizes the response. This is useful for when you want to ask natural language questions of an Elastic Search database FlareChain   This implements FLARE, an advanced retrieval technique. It is primarily meant as an exploratory advanced retrieval method. ArangoGraphQAChain  Arango Graph This chain constructs an Arango query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. GraphCypherQAChain  A graph that works with Cypher query language This chain constructs an Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. FalkorDBGraphQAChain  Falkor Database This chain constructs a FalkorDB query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. HugeGraphQAChain  HugeGraph This chain constructs an HugeGraph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. KuzuQAChain  Kuzu Graph This chain constructs a Kuzu Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. NebulaGraphQAChain  Nebula Graph This chain constructs a Nebula Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. NeptuneOpenCypherQAChain  Neptune Graph This chain constructs an Neptune Graph query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. GraphSparqlChain  Graph that works with SparQL This chain constructs an SparQL query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond. LLMMath   This chain converts a user question to a math problem and then executes it (using numexpr) LLMCheckerChain   This chain uses a second LLM call to varify its initial answer. Use this when you to have an extra layer of validation on the initial LLM call. LLMSummarizationChecker   This chain creates a summary using a sequence of LLM calls to make sure it is extra correct. Use this over the normal summarization chain when you are okay with multiple LLM calls (eg you care more about accuracy than speed/cost). create_citation_fuzzy_match_chain ✅  Uses OpenAI function calling to answer questions and cite its sources. create_extraction_chain ✅  Uses OpenAI Function calling to extract information from text. create_extraction_chain_pydantic ✅  Uses OpenAI function calling to extract information from text into a Pydantic model. Compared to create_extraction_chain this has a tighter integration with Pydantic. get_openapi_chain ✅ OpenAPI Spec Uses OpenAI function calling to query an OpenAPI. create_qa_with_structure_chain ✅  Uses OpenAI function calling to do question answering over text and respond in a specific format. create_qa_with_sources_chain ✅  Uses OpenAI function calling to answer questions with citations. QAGenerationChain   Creates both questions and answers from documents. Can be used to generate question/answer pairs for evaluation of retrieval projects. RetrievalQAWithSourcesChain  Retriever Does question answering over retrieved documents, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over load_qa_with_sources_chain when you want to use a retriever to fetch the relevant document as part of the chain (rather than pass them in). load_qa_with_sources_chain  Retriever Does question answering over documents you pass in, and cites it sources. Use this when you want the answer response to have sources in the text response. Use this over RetrievalQAWithSources when you want to pass in the documents directly (rather than rely on a retriever to get them). RetrievalQA  Retriever This chain first does a retrieval step to fetch relevant documents, then passes those documents into an LLM to generate a respoinse. MultiPromptChain   This chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one. MultiRetrievalQAChain  Retriever This chain routes input between multiple retrievers. Use this when you have multiple potential retrievers you could fetch relevant documents from and want to route to just one. EmbeddingRouterChain   This chain uses embedding similarity to route incoming queries. LLMRouterChain   This chain uses an LLM to route between potential options. load_summarize_chain    LLMRequestsChain   This chain constructs a URL from user input, gets data at that URL, and then summarizes the response. Compared to APIChain, this chain is not focused on a single API spec but is more general Previous Tools as OpenAI Functions Next [Beta] Memory Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/debugging",
    "title": "Debugging | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Debugging Debugging  If you're building with LLMs, at some point something will break, and you'll need to debug. A model call will fail, or the model output will be misformatted, or there will be some nested model calls and it won't be clear where along the way an incorrect output was created.  Here are a few different tools and functionalities to aid in debugging.  Tracing​  Platforms with tracing capabilities like LangSmith and WandB are the most comprehensive solutions for debugging. These platforms make it easy to not only log and visualize LLM apps, but also to actively debug, test and refine them.  For anyone building production-grade LLM applications, we highly recommend using a platform like this.  set_debug and set_verbose​  If you're prototyping in Jupyter Notebooks or running Python scripts, it can be helpful to print out the intermediate steps of a Chain run.  There are a number of ways to enable printing at varying degrees of verbosity.  Let's suppose we have a simple agent, and want to visualize the actions it takes and tool outputs it receives. Without any debugging, here's what we see:  from langchain.agents import AgentType, initialize_agent, load_tools from langchain_openai import ChatOpenAI  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0) tools = load_tools([\"ddg-search\", \"llm-math\"], llm=llm) agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION)  agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")      'The director of the 2023 film Oppenheimer is Christopher Nolan and he is approximately 19345 days old in 2023.'  set_debug(True)​  Setting the global debug flag will cause all LangChain components with callback support (chains, models, agents, tools, retrievers) to print the inputs they receive and outputs they generate. This is the most verbose setting and will fully log raw inputs and outputs.  from langchain.globals import set_debug  set_debug(True)  agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")  Console output set_verbose(True)​  Setting the verbose flag will print out inputs and outputs in a slightly more readable format and will skip logging certain raw outputs (like the token usage stats for an LLM call) so that you can focus on application logic.  from langchain.globals import set_verbose  set_verbose(True)  agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")  Console output Chain(..., verbose=True)​  You can also scope verbosity down to a single object, in which case only the inputs and outputs to that object are printed (along with any additional callbacks calls made specifically by that object).  # Passing verbose=True to initialize_agent will pass that along to the AgentExecutor (which is a Chain). agent = initialize_agent(     tools,      llm,      agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,     verbose=True, )  agent.run(\"Who directed the 2023 film Oppenheimer and what is their age? What is their age in days (assume 365 days per year)?\")  Console output Other callbacks​  Callbacks are what we use to execute any functionality within a component outside the primary component logic. All of the above solutions use Callbacks under the hood to log intermediate steps of components. There are a number of Callbacks relevant for debugging that come with LangChain out of the box, like the FileCallbackHandler. You can also implement your own callbacks to execute custom functionality.  See here for more info on Callbacks, how to use them, and customize them.  Next Deployment Tracing set_debug and set_verbose set_debug(True) set_verbose(True) Chain(..., verbose=True) Other callbacks Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/langgraph",
    "title": "🦜🕸️LangGraph | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangGraph 🦜🕸️LangGraph  ⚡ Building language agents as graphs ⚡  Overview​  LangGraph is a library for building stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain. It extends the LangChain Expression Language with the ability to coordinate multiple chains (or actors) across multiple steps of computation in a cyclic manner. It is inspired by Pregel and Apache Beam. The current interface exposed is one inspired by NetworkX.  The main use is for adding cycles to your LLM application. Crucially, this is NOT a DAG framework. If you want to build a DAG, you should use just use LangChain Expression Language.  Cycles are important for agent-like behaviors, where you call an LLM in a loop, asking it what action to take next.  Installation​ pip install langgraph  Quick Start​  Here we will go over an example of creating a simple agent that uses chat models and function calling. This agent will represent all state as a list of messages.  We will need to install some LangChain packages, as well as Tavily to use as an example tool.  pip install -U langchain langchain_openai tavily-python   We also need to export some environment variables needed for our agent.  export OPENAI_API_KEY=sk-... export TAVILY_API_KEY=tvly-...   Optionally, we can set up LangSmith for best-in-class observability.  export LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_API_KEY=ls__... export LANGCHAIN_ENDPOINT=https://api.langchain.plus  Set up the tools​  We will first define the tools we want to use. For this simple example, we will use a built-in search tool via Tavily. However, it is really easy to create your own tools - see documentation here on how to do that.  from langchain_community.tools.tavily_search import TavilySearchResults  tools = [TavilySearchResults(max_results=1)]   We can now wrap these tools in a simple ToolExecutor. This is a real simple class that takes in a ToolInvocation and calls that tool, returning the output. A ToolInvocation is any class with tool and tool_input attribute.  from langgraph.prebuilt import ToolExecutor  tool_executor = ToolExecutor(tools)  Set up the model​  Now we need to load the chat model we want to use. Importantly, this should satisfy two criteria:  It should work with messages. We will represent all agent state in the form of messages, so it needs to be able to work well with them. It should work with OpenAI function calling. This means it should either be an OpenAI model or a model that exposes a similar interface.  Note: these model requirements are not requirements for using LangGraph - they are just requirements for this one example.  from langchain_openai import ChatOpenAI  # We will set streaming=True so that we can stream tokens # See the streaming section for more information on this. model = ChatOpenAI(temperature=0, streaming=True)   After we've done this, we should make sure the model knows that it has these tools available to call. We can do this by converting the LangChain tools into the format for OpenAI function calling, and then bind them to the model class.  from langchain.tools.render import format_tool_to_openai_function  functions = [format_tool_to_openai_function(t) for t in tools] model = model.bind_functions(functions)  Define the agent state​  The main type of graph in langgraph is the StatefulGraph. This graph is parameterized by a state object that it passes around to each node. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with.  For this example, the state we will track will just be a list of messages. We want each node to just add messages to that list. Therefore, we will use a TypedDict with one key (messages) and annotate it so that the messages attribute is always added to.  from typing import TypedDict, Annotated, Sequence import operator from langchain_core.messages import BaseMessage   class AgentState(TypedDict):     messages: Annotated[Sequence[BaseMessage], operator.add]  Define the nodes​  We now need to define a few different nodes in our graph. In langgraph, a node can be either a function or a runnable. There are two main nodes we need for this:  The agent: responsible for deciding what (if any) actions to take. A function to invoke tools: if the agent decides to take an action, this node will then execute that action.  We will also need to define some edges. Some of these edges may be conditional. The reason they are conditional is that based on the output of a node, one of several paths may be taken. The path that is taken is not known until that node is run (the LLM decides).  Conditional Edge: after the agent is called, we should either: a. If the agent said to take an action, then the function to invoke tools should be called b. If the agent said that it was finished, then it should finish Normal Edge: after the tools are invoked, it should always go back to the agent to decide what to do next  Let's define the nodes, as well as a function to decide how what conditional edge to take.  from langgraph.prebuilt import ToolInvocation import json from langchain_core.messages import FunctionMessage  # Define the function that determines whether to continue or not def should_continue(state):     messages = state['messages']     last_message = messages[-1]     # If there is no function call, then we finish     if \"function_call\" not in last_message.additional_kwargs:         return \"end\"     # Otherwise if there is, we continue     else:         return \"continue\"  # Define the function that calls the model def call_model(state):     messages = state['messages']     response = model.invoke(messages)     # We return a list, because this will get added to the existing list     return {\"messages\": [response]}  # Define the function to execute tools def call_tool(state):     messages = state['messages']     # Based on the continue condition     # we know the last message involves a function call     last_message = messages[-1]     # We construct an ToolInvocation from the function_call     action = ToolInvocation(         tool=last_message.additional_kwargs[\"function_call\"][\"name\"],         tool_input=json.loads(last_message.additional_kwargs[\"function_call\"][\"arguments\"]),     )     # We call the tool_executor and get back a response     response = tool_executor.invoke(action)     # We use the response to create a FunctionMessage     function_message = FunctionMessage(content=str(response), name=action.tool)     # We return a list, because this will get added to the existing list     return {\"messages\": [function_message]}  Define the graph​  We can now put it all together and define the graph!  from langgraph.graph import StateGraph, END # Define a new graph workflow = StateGraph(AgentState)  # Define the two nodes we will cycle between workflow.add_node(\"agent\", call_model) workflow.add_node(\"action\", call_tool)  # Set the entrypoint as `agent` # This means that this node is the first one called workflow.set_entry_point(\"agent\")  # We now add a conditional edge workflow.add_conditional_edges(     # First, we define the start node. We use `agent`.     # This means these are the edges taken after the `agent` node is called.     \"agent\",     # Next, we pass in the function that will determine which node is called next.     should_continue,     # Finally we pass in a mapping.     # The keys are strings, and the values are other nodes.     # END is a special node marking that the graph should finish.     # What will happen is we will call `should_continue`, and then the output of that     # will be matched against the keys in this mapping.     # Based on which one it matches, that node will then be called.     {         # If `tools`, then we call the tool node.         \"continue\": \"action\",         # Otherwise we finish.         \"end\": END     } )  # We now add a normal edge from `tools` to `agent`. # This means that after `tools` is called, `agent` node is called next. workflow.add_edge('action', 'agent')  # Finally, we compile it! # This compiles it into a LangChain Runnable, # meaning you can use it as you would any other runnable app = workflow.compile()  Use it!​  We can now use it! This now exposes the same interface as all other LangChain runnables. This runnable accepts a list of messages.  from langchain_core.messages import HumanMessage  inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} app.invoke(inputs)   This may take a little bit - it's making a few calls behind the scenes. In order to start seeing some intermediate results as they happen, we can use streaming - see below for more information on that.  Streaming​  LangGraph has support for several different types of streaming.  Streaming Node Output​  One of the benefits of using LangGraph is that it is easy to stream output as it's produced by each node.  inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} for output in app.stream(inputs):     # stream() yields dictionaries with output keyed by node name     for key, value in output.items():         print(f\"Output from node '{key}':\")         print(\"---\")         print(value)     print(\"\\n---\\n\")  Output from node 'agent': --- {'messages': [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}})]}  ---  Output from node 'action': --- {'messages': [FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States  Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco  San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco  Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}]\", name='tavily_search_results_json')]}  ---  Output from node 'agent': --- {'messages': [AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]}  ---  Output from node '__end__': --- {'messages': [HumanMessage(content='what is the weather in sf'), AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\\n  \"query\": \"weather in San Francisco\"\\n}', 'name': 'tavily_search_results_json'}}), FunctionMessage(content=\"[{'url': 'https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States', 'content': 'January 2024 Weather History in San Francisco California, United States  Daily Precipitation in January 2024 in San Francisco Observed Weather in January 2024 in San Francisco  San Francisco Temperature History January 2024 Hourly Temperature in January 2024 in San Francisco  Hours of Daylight and Twilight in January 2024 in San FranciscoThis report shows the past weather for San Francisco, providing a weather history for January 2024. It features all historical weather data series we have available, including the San Francisco temperature history for January 2024. You can drill down from year to month and even day level reports by clicking on the graphs.'}]\", name='tavily_search_results_json'), AIMessage(content=\"I couldn't find the current weather in San Francisco. However, you can visit [WeatherSpark](https://weatherspark.com/h/m/557/2024/1/Historical-Weather-in-January-2024-in-San-Francisco-California-United-States) to check the historical weather data for January 2024 in San Francisco.\")]}  ---  Streaming LLM Tokens​  You can also access the LLM tokens as they are produced by each node. In this case only the \"agent\" node produces LLM tokens. In order for this to work properly, you must be using an LLM that supports streaming as well as have set it when constructing the LLM (e.g. ChatOpenAI(model=\"gpt-3.5-turbo-1106\", streaming=True))  inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} async for output in app.astream_log(inputs, include_types=[\"llm\"]):     # astream_log() yields the requested logs (here LLMs) in JSONPatch format     for op in output.ops:         if op[\"path\"] == \"/streamed_output/-\":             # this is the output from .stream()             ...         elif op[\"path\"].startswith(\"/logs/\") and op[\"path\"].endswith(             \"/streamed_output/-\"         ):             # because we chose to only include LLMs, these are LLM tokens             print(op[\"value\"])  content='' additional_kwargs={'function_call': {'arguments': '', 'name': 'tavily_search_results_json'}} content='' additional_kwargs={'function_call': {'arguments': '{\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' ', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'query', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '\":', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' \"', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': 'weather', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' in', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' San', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': ' Francisco', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '\"\\n', 'name': ''}} content='' additional_kwargs={'function_call': {'arguments': '}', 'name': ''}} content='' content='' content='I' content=\"'m\" content=' sorry' content=',' content=' but' content=' I' content=' couldn' content=\"'t\" content=' find' content=' the' content=' current' content=' weather' content=' in' content=' San' content=' Francisco' content='.' content=' However' content=',' content=' you' content=' can' content=' check' content=' the' content=' historical' content=' weather' content=' data' content=' for' content=' January' content=' ' content='202' content='4' content=' in' content=' San' content=' Francisco' content=' [' content='here' content='](' content='https' content='://' content='we' content='athers' content='park' content='.com' content='/h' content='/m' content='/' content='557' content='/' content='202' content='4' content='/' content='1' content='/H' content='istorical' content='-' content='Weather' content='-in' content='-Jan' content='uary' content='-' content='202' content='4' content='-in' content='-S' content='an' content='-F' content='r' content='anc' content='isco' content='-Cal' content='ifornia' content='-' content='United' content='-' content='States' content=').' content=''  When to Use​  When should you use this versus LangChain Expression Language?  If you need cycles.  Langchain Expression Language allows you to easily define chains (DAGs) but does not have a good mechanism for adding in cycles. langgraph adds that syntax.  Examples​ ChatAgentExecutor: with function calling​  This agent executor takes a list of messages as input and outputs a list of messages. All agent state is represented as a list of messages. This specifically uses OpenAI function calling. This is recommended agent executor for newer chat based models that support function calling.  Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor.  Modifications  We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first.  Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Respond in a specific format: How to force the agent to respond in a specific format Dynamically returning tool output directly: How to dynamically let the agent choose whether to return the result of a tool directly to the user Managing agent steps: How to more explicitly manage intermediate steps that an agent takes AgentExecutor​  This agent executor uses existing LangChain agents.  Getting Started Notebook: Walks through creating this type of executor from scratch High Level Entrypoint: Walks through how to use the high level entrypoint for the chat agent executor.  Modifications  We also have a lot of examples highlighting how to slightly modify the base chat agent executor. These all build off the getting started notebook so it is recommended you start with that first.  Human-in-the-loop: How to add a human-in-the-loop component Force calling a tool first: How to always call a specific tool first Managing agent steps: How to more explicitly manage intermediate steps that an agent takes Async​  If you are running LangGraph in async workflows, you may want to create the nodes to be async by default. In order for a walkthrough on how to do that, see this documentation  Streaming Tokens​  Sometimes language models take a while to respond and you may want to stream tokens to end users. For a guide on how to do this, see this documentation  Documentation​  There are only a few new APIs to use.  StateGraph​  The main entrypoint is StateGraph.  from langgraph.graph import StateGraph   This class is responsible for constructing the graph. It exposes an interface inspired by NetworkX. This graph is parameterized by a state object that it passes around to each node.  __init__​     def __init__(self, schema: Type[Any]) -> None:   When constructing the graph, you need to pass in a schema for a state. Each node then returns operations to update that state. These operations can either SET specific attributes on the state (e.g. overwrite the existing values) or ADD to the existing attribute. Whether to set or add is denoted by annotating the state object you construct the graph with.  The recommended way to specify the schema is with a typed dictionary: from typing import TypedDict  You can then annotate the different attributes using from typing imoport Annotated. Currently, the only supported annotation is import operator; operator.add. This annotation will make it so that any node that returns this attribute ADDS that new result to the existing value.  Let's take a look at an example:  from typing import TypedDict, Annotated, Union from langchain_core.agents import AgentAction, AgentFinish import operator   class AgentState(TypedDict):    # The input string    input: str    # The outcome of a given call to the agent    # Needs `None` as a valid type, since this is what this will start as    agent_outcome: Union[AgentAction, AgentFinish, None]    # List of actions and corresponding observations    # Here we annotate this with `operator.add` to indicate that operations to    # this state should be ADDED to the existing values (not overwrite it)    intermediate_steps: Annotated[list[tuple[AgentAction, str]], operator.add]    We can then use this like:  # Initialize the StateGraph with this state graph = StateGraph(AgentState) # Create nodes and edges ... # Compile the graph app = graph.compile()  # The inputs should be a dictionary, because the state is a TypedDict inputs = {    # Let's assume this the input    \"input\": \"hi\"    # Let's assume agent_outcome is set by the graph as some point    # It doesn't need to be provided, and it will be None by default    # Let's assume `intermediate_steps` is built up over time by the graph    # It doesn't need to provided, and it will be empty list by default    # The reason `intermediate_steps` is an empty list and not `None` is because    # it's annotated with `operator.add` }  .add_node​     def add_node(self, key: str, action: RunnableLike) -> None:   This method adds a node to the graph. It takes two arguments:  key: A string representing the name of the node. This must be unique. action: The action to take when this node is called. This should either be a function or a runnable. .add_edge​     def add_edge(self, start_key: str, end_key: str) -> None:   Creates an edge from one node to the next. This means that output of the first node will be passed to the next node. It takes two arguments.  start_key: A string representing the name of the start node. This key must have already been registered in the graph. end_key: A string representing the name of the end node. This key must have already been registered in the graph. .add_conditional_edges​     def add_conditional_edges(         self,         start_key: str,         condition: Callable[..., str],         conditional_edge_mapping: Dict[str, str],     ) -> None:   This method adds conditional edges. What this means is that only one of the downstream edges will be taken, and which one that is depends on the results of the start node. This takes three arguments:  start_key: A string representing the name of the start node. This key must have already been registered in the graph. condition: A function to call to decide what to do next. The input will be the output of the start node. It should return a string that is present in conditional_edge_mapping and represents the edge to take. conditional_edge_mapping: A mapping of string to string. The keys should be strings that may be returned by condition. The values should be the downstream node to call if that condition is returned. .set_entry_point​     def set_entry_point(self, key: str) -> None:   The entrypoint to the graph. This is the node that is first called. It only takes one argument:  key: The name of the node that should be called first. .set_finish_point​     def set_finish_point(self, key: str) -> None:   This is the exit point of the graph. When this node is called, the results will be the final result from the graph. It only has one argument:  key: The name of the node that, when called, will return the results of calling it as the final output  Note: This does not need to be called if at any point you previously created an edge (conditional or normal) to END  Graph​ from langgraph.graph import Graph  graph = Graph()   This has the same interface as StateGraph with the exception that it doesn't update a state object over time, and rather relies on passing around the full state from each step. This means that whatever is returned from one node is the input to the next as is.  END​ from langgraph.graph import END   This is a special node representing the end of the graph. This means that anything passed to this node will be the final output of the graph. It can be used in two places:  As the end_key in add_edge As a value in conditional_edge_mapping as passed to add_conditional_edges Prebuilt Examples​  There are also a few methods we've added to make it easy to use common, prebuilt graphs and components.  ToolExecutor​ from langgraph.prebuilt import ToolExecutor   This is a simple helper class to help with calling tools. It is parameterized by a list of tools:  tools = [...] tool_executor = ToolExecutor(tools)   It then exposes a runnable interface. It can be used to call tools: you can pass in an AgentAction and it will look up the relevant tool and call it with the appropriate input.  chat_agent_executor.create_function_calling_executor​ from langgraph.prebuilt import chat_agent_executor   This is a helper function for creating a graph that works with a chat model that utilizes function calling. Can be created by passing in a model and a list of tools. The model must be one that supports OpenAI function calling.  from langchain_openai import ChatOpenAI from langchain_community.tools.tavily_search import TavilySearchResults from langgraph.prebuilt import chat_agent_executor from langchain_core.messages import HumanMessage  tools = [TavilySearchResults(max_results=1)] model = ChatOpenAI()  app = chat_agent_executor.create_function_calling_executor(model, tools)  inputs = {\"messages\": [HumanMessage(content=\"what is the weather in sf\")]} for s in app.stream(inputs):     print(list(s.values())[0])     print(\"----\")  create_agent_executor​ from langgraph.prebuilt import create_agent_executor   This is a helper function for creating a graph that works with LangChain Agents. Can be created by passing in an agent and a list of tools.  from langgraph.prebuilt import create_agent_executor from langchain_openai import ChatOpenAI from langchain import hub from langchain.agents import create_openai_functions_agent from langchain_community.tools.tavily_search import TavilySearchResults  tools = [TavilySearchResults(max_results=1)]  # Get the prompt to use - you can modify this! prompt = hub.pull(\"hwchase17/openai-functions-agent\")  # Choose the LLM that will drive the agent llm = ChatOpenAI(model=\"gpt-3.5-turbo-1106\")  # Construct the OpenAI Functions agent agent_runnable = create_openai_functions_agent(llm, tools, prompt)  app = create_agent_executor(agent_runnable, tools)  inputs = {\"input\": \"what is the weather in sf\", \"chat_history\": []} for s in app.stream(inputs):     print(list(s.values())[0])     print(\"----\")  Previous LangSmith Walkthrough Overview Installation Quick Start Set up the tools Set up the model Define the agent state Define the nodes Define the graph Use it! Streaming Streaming Node Output Streaming LLM Tokens When to Use Examples ChatAgentExecutor: with function calling AgentExecutor Async Streaming Tokens Documentation StateGraph Graph END Prebuilt Examples ToolExecutor chat_agent_executor.create_function_calling_executor create_agent_executor Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/security",
    "title": "Security | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Get startedSecurity Security  LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.  Best Practices​  When building such applications developers should remember to follow good security practices:  Limit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application. Anticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data. Defense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.  Risks of not doing so include, but are not limited to:  Data corruption or loss. Unauthorized access to confidential information. Compromised performance or availability of critical resources.  Example scenarios with mitigation strategies:  A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.  If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.  Reporting a Vulnerability​  Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.  Enterprise solutions​  LangChain may offer enterprise solutions for customers who have additional security requirements. Please contact us at sales@langchain.dev.  Previous Quickstart Next LangChain Expression Language (LCEL) Best Practices Reporting a Vulnerability Enterprise solutions Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/",
    "title": "LangChain Expression Language (LCEL) | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression Language LangChain Expression Language (LCEL)  LangChain Expression Language, or LCEL, is a declarative way to easily compose chains together. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully run LCEL chains with 100s of steps in production). To highlight a few of the reasons you might want to use LCEL:  Streaming support When you build your chains with LCEL you get the best possible time-to-first-token (time elapsed until the first chunk of output comes out). For some chains this means eg. we stream tokens straight from an LLM to a streaming output parser, and you get back parsed, incremental chunks of output at the same rate as the LLM provider outputs the raw tokens.  Async support Any chain built with LCEL can be called both with the synchronous API (eg. in your Jupyter notebook while prototyping) as well as with the asynchronous API (eg. in a LangServe server). This enables using the same code for prototypes and in production, with great performance, and the ability to handle many concurrent requests in the same server.  Optimized parallel execution Whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, both in the sync and the async interfaces, for the smallest possible latency.  Retries and fallbacks Configure retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost.  Access intermediate results For more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used to let end-users know something is happening, or even just to debug your chain. You can stream intermediate results, and it’s available on every LangServe server.  Input and output schemas Input and output schemas give every LCEL chain Pydantic and JSONSchema schemas inferred from the structure of your chain. This can be used for validation of inputs and outputs, and is an integral part of LangServe.  Seamless LangSmith tracing integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability.  Seamless LangServe deployment integration Any chain created with LCEL can be easily deployed using LangServe.  Previous Security Next Get started Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/get_started/installation",
    "title": "Installation | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Get startedInstallation Installation Official release​  To install LangChain run:  Pip Conda pip install langchain   This will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. By default, the dependencies needed to do that are NOT installed. You will need to install the dependencies for specific integrations separately.  From source​  If you want to install from source, you can do so by cloning the repo and be sure that the directory is PATH/TO/REPO/langchain/libs/langchain running:  pip install -e .  LangChain community​  The langchain-community package contains third-party integrations. It is automatically installed by langchain, but can also be used separately. Install with:  pip install langchain-community  LangChain core​  The langchain-core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed by langchain, but can also be used separately. Install with:  pip install langchain-core  LangChain experimental​  The langchain-experimental package holds experimental LangChain code, intended for research and experimental uses. Install with:  pip install langchain-experimental  LangServe​  LangServe helps developers deploy LangChain runnables and chains as a REST API. LangServe is automatically installed by LangChain CLI. If not using LangChain CLI, install with:  pip install \"langserve[all]\"   for both client and server dependencies. Or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.  LangChain CLI​  The LangChain CLI is useful for working with LangChain templates and other LangServe projects. Install with:  pip install langchain-cli  LangSmith SDK​  The LangSmith SDK is automatically installed by LangChain. If not using LangChain, install with:  pip install langsmith  Previous Introduction Next Quickstart Official release From source LangChain community LangChain core LangChain experimental LangServe LangChain CLI LangSmith SDK Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/langsmith",
    "title": "LangSmith | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangSmith Walkthrough LangGraph LangSmith LangSmith  LangSmith helps you trace and evaluate your language model applications and intelligent agents to help you move from prototype to production.  Check out the interactive walkthrough to get started.  For more information, please refer to the LangSmith documentation.  For tutorials and other end-to-end examples demonstrating ways to integrate LangSmith in your workflow, check out the LangSmith Cookbook. Some of the guides therein include:  Leveraging user feedback in your JS application (link). Building an automated feedback pipeline (link). How to evaluate and audit your RAG workflows (link). How to fine-tune an LLM on real usage data (link). How to use the LangChain Hub to version your prompts (link) Previous 🦜️🏓 LangServe Next LangSmith Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/how_to/",
    "title": "How to | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to RunnableParallel: Manipulating data RunnablePassthrough: Passing data through RunnableLambda: Run Custom Functions RunnableBranch: Dynamically route logic based on input Bind runtime args Configure chain internals at runtime Create a runnable with the `@chain` decorator Add fallbacks Stream custom generator functions Inspect your runnables Add message history (memory) Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageHow to How to 📄️ RunnableParallel: Manipulating data  manipulating-inputs-output}  📄️ RunnablePassthrough: Passing data through  passing-data-through}  📄️ RunnableLambda: Run Custom Functions  run-custom-functions}  📄️ RunnableBranch: Dynamically route logic based on input  dynamically-route-logic-based-on-input}  📄️ Bind runtime args  Sometimes we want to invoke a Runnable within a Runnable sequence with  📄️ Configure chain internals at runtime  Oftentimes you may want to experiment with, or even expose to the end  📄️ Create a runnable with the `@chain` decorator  You can also turn an arbitrary function into a chain by adding a  📄️ Add fallbacks  There are many possible points of failure in an LLM application, whether  📄️ Stream custom generator functions  You can use generator functions (ie. functions that use the yield  📄️ Inspect your runnables  Once you create a runnable with LCEL, you may often want to inspect it  📄️ Add message history (memory)  The RunnableWithMessageHistory let’s us add message history to certain  Previous Interface Next RunnableParallel: Manipulating data Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/",
    "title": "Modules | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Modules Modules  LangChain provides standard, extendable interfaces and external integrations for the following main modules:  Model I/O​  Interface with language models  Retrieval​  Interface with application-specific data  Agents​  Let chains choose which tools to use given high-level directives  Additional​ Chains​  Common, building block compositions  Memory​  Persist application state between runs of a chain  Callbacks​  Log and stream intermediate steps of any chain  Previous LangChain Expression Language (LCEL) Next Model I/O Additional Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/qa_structured/sql/",
    "title": "SQL | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data SQL Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A over structured dataSQL SQL  Open In Colab  Use case​  Enterprise data is often stored in SQL databases.  LLMs make it possible to interact with SQL databases using natural language.  LangChain offers SQL Chains and Agents to build and run SQL queries based on natural language prompts.  These are compatible with any SQL dialect supported by SQLAlchemy (e.g., MySQL, PostgreSQL, Oracle SQL, Databricks, SQLite).  They enable use cases such as:  Generating queries that will be run based on natural language questions Creating chatbots that can answer questions based on database data Building custom dashboards based on insights a user wants to analyze Overview​  LangChain provides tools to interact with SQL Databases:  Build SQL queries based on natural language user questions Query a SQL database using chains for query creation and execution Interact with a SQL database using agents for robust and flexible querying  Quickstart​  First, get required packages and set environment variables:  %pip install --upgrade --quiet  langchain langchain-experimental langchain-openai  # Set env var OPENAI_API_KEY or load from a .env file # import dotenv  # dotenv.load_dotenv()   The below example will use a SQLite connection with Chinook database.  Follow installation steps to create Chinook.db in the same directory as this notebook:  Save this file to the directory as Chinook_Sqlite.sql Run sqlite3 Chinook.db Run .read Chinook_Sqlite.sql Test SELECT * FROM Artist LIMIT 10;  Now, Chinhook.db is in our directory.  Let’s create a SQLDatabaseChain to create and execute SQL queries.  from langchain_community.utilities import SQLDatabase from langchain_experimental.sql import SQLDatabaseChain from langchain_openai import OpenAI  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)  db_chain.run(\"How many employees are there?\")    > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain.  'There are 8 employees.'   Note that this both creates and executes the query.  In the following sections, we will cover the 3 different use cases mentioned in the overview.  Go deeper​  You can load tabular data from other sources other than SQL Databases. For example: - Loading a CSV file - Loading a Pandas DataFrame Here you can check full list of Document Loaders  Case 1: Text-to-SQL query​ from langchain.chains import create_sql_query_chain from langchain_openai import ChatOpenAI   Let’s create the chain that will build the SQL Query:  chain = create_sql_query_chain(ChatOpenAI(temperature=0), db) response = chain.invoke({\"question\": \"How many employees are there\"}) print(response)  SELECT COUNT(*) FROM Employee   After building the SQL query based on a user question, we can execute the query:  db.run(response)  '[(8,)]'   As we can see, the SQL Query Builder chain only created the query, and we handled the query execution separately.  Go deeper​  Looking under the hood  We can look at the LangSmith trace to unpack this:  Some papers have reported good performance when prompting with:  A CREATE TABLE description for each table, which include column names, their types, etc Followed by three example rows in a SELECT statement  create_sql_query_chain adopts this the best practice (see more in this blog).   Improvements  The query builder can be improved in several ways, such as (but not limited to):  Customizing database description to your specific use case Hardcoding a few examples of questions and their corresponding SQL query in the prompt Using a vector database to include dynamic examples that are relevant to the specific user question  All these examples involve customizing the chain’s prompt.  For example, we can include a few examples in our prompt like so:  from langchain.prompts import PromptTemplate  TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format:  Question: \"Question here\" SQLQuery: \"SQL Query to run\" SQLResult: \"Result of the SQLQuery\" Answer: \"Final answer here\"  Only use the following tables:  {table_info}.  Some examples of SQL queries that correspond to questions are:  {few_shot_examples}  Question: {input}\"\"\"  CUSTOM_PROMPT = PromptTemplate(     input_variables=[\"input\", \"few_shot_examples\", \"table_info\", \"dialect\"],     template=TEMPLATE, )   We can also access this prompt in the LangChain prompt hub.  This will work with your LangSmith API key.  from langchain import hub  CUSTOM_PROMPT = hub.pull(\"rlm/text-to-sql\")  Case 2: Text-to-SQL query and execution​  We can use SQLDatabaseChain from langchain_experimental to create and run SQL queries.  from langchain_experimental.sql import SQLDatabaseChain from langchain_openai import OpenAI  llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)  db_chain.run(\"How many employees are there?\")    > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain.  'There are 8 employees.'   As we can see, we get the same result as the previous case.  Here, the chain also handles the query execution and provides a final answer based on the user question and the query result.  Be careful while using this approach as it is susceptible to SQL Injection:  The chain is executing queries that are created by an LLM, and weren’t validated e.g. records may be created, modified or deleted unintentionally_  This is why we see the SQLDatabaseChain is inside langchain_experimental.  Go deeper​  Looking under the hood  We can use the LangSmith trace to see what is happening under the hood:  As discussed above, first we create the query: text: ' SELECT COUNT(*) FROM \"Employee\";'  Then, it executes the query and passes the results to an LLM for synthesis.  Adding Sample Rows  Providing sample data can help the LLM construct correct queries when the data format is not obvious.  For example, we can tell LLM that artists are saved with their full names by providing two rows from the Track table.  db = SQLDatabase.from_uri(     \"sqlite:///Chinook.db\",     include_tables=[         \"Track\"     ],  # we include only one table to save tokens in the prompt :)     sample_rows_in_table_info=2, )   The sample rows are added to the prompt after each corresponding table’s column information.  We can use db.table_info and check which sample rows are included:  print(db.table_info)   CREATE TABLE \"Track\" (     \"TrackId\" INTEGER NOT NULL,      \"Name\" NVARCHAR(200) NOT NULL,      \"AlbumId\" INTEGER,      \"MediaTypeId\" INTEGER NOT NULL,      \"GenreId\" INTEGER,      \"Composer\" NVARCHAR(220),      \"Milliseconds\" INTEGER NOT NULL,      \"Bytes\" INTEGER,      \"UnitPrice\" NUMERIC(10, 2) NOT NULL,      PRIMARY KEY (\"TrackId\"),      FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),      FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),      FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") )  /* 2 rows from Track table: TrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice 1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99 2   Balls to the Wall   2   2   1   None    342562  5510424 0.99 */  Case 3: SQL agents​  LangChain has an SQL Agent which provides a more flexible way of interacting with SQL Databases than the SQLDatabaseChain.  The main advantages of using the SQL Agent are:  It can answer questions based on the databases’ schema as well as on the databases’ content (like describing a specific table) It can recover from errors by running a generated query, catching the traceback and regenerating it correctly  To initialize the agent, we use create_sql_agent function.  This agent contains the SQLDatabaseToolkit which contains tools to:  Create and execute queries Check query syntax Retrieve table descriptions … and more from langchain.agents import create_sql_agent  # from langchain.agents import AgentExecutor from langchain.agents.agent_types import AgentType from langchain_community.agent_toolkits import SQLDatabaseToolkit  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")  agent_executor = create_sql_agent(     llm=OpenAI(temperature=0),     toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)),     verbose=True,     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, )  Agent task example #1 - Running queries​ agent_executor.run(     \"List the total sales per country. Which country's customers spent the most?\" )    > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input:  Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer Observation:  CREATE TABLE \"Customer\" (     \"CustomerId\" INTEGER NOT NULL,      \"FirstName\" NVARCHAR(40) NOT NULL,      \"LastName\" NVARCHAR(20) NOT NULL,      \"Company\" NVARCHAR(80),      \"Address\" NVARCHAR(70),      \"City\" NVARCHAR(40),      \"State\" NVARCHAR(40),      \"Country\" NVARCHAR(40),      \"PostalCode\" NVARCHAR(10),      \"Phone\" NVARCHAR(24),      \"Fax\" NVARCHAR(24),      \"Email\" NVARCHAR(60) NOT NULL,      \"SupportRepId\" INTEGER,      PRIMARY KEY (\"CustomerId\"),      FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\") )  /* 3 rows from Customer table: CustomerId  FirstName   LastName    Company Address City    State   Country PostalCode  Phone   Fax Email   SupportRepId 1   Luís    Gonçalves   Embraer - Empresa Brasileira de Aeronáutica S.A.    Av. Brigadeiro Faria Lima, 2170 São José dos Campos SP  Brazil  12227-000   +55 (12) 3923-5555  +55 (12) 3923-5566  luisg@embraer.com.br    3 2   Leonie  Köhler  None    Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   +49 0711 2842222    None    leonekohler@surfeu.de   5 3   François    Tremblay    None    1498 rue Bélanger   Montréal    QC  Canada  H2G 1A7 +1 (514) 721-4711   None    ftremblay@gmail.com 3 */   CREATE TABLE \"Invoice\" (     \"InvoiceId\" INTEGER NOT NULL,      \"CustomerId\" INTEGER NOT NULL,      \"InvoiceDate\" DATETIME NOT NULL,      \"BillingAddress\" NVARCHAR(70),      \"BillingCity\" NVARCHAR(40),      \"BillingState\" NVARCHAR(40),      \"BillingCountry\" NVARCHAR(40),      \"BillingPostalCode\" NVARCHAR(10),      \"Total\" NUMERIC(10, 2) NOT NULL,      PRIMARY KEY (\"InvoiceId\"),      FOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\") )  /* 3 rows from Invoice table: InvoiceId   CustomerId  InvoiceDate BillingAddress  BillingCity BillingState    BillingCountry  BillingPostalCode   Total 1   2   2009-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   1.98 2   4   2009-01-02 00:00:00 Ullevålsveien 14    Oslo    None    Norway  0171    3.96 3   8   2009-01-03 00:00:00 Grétrystraat 63 Brussels    None    Belgium 1000    5.94 */ Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] Thought: I now know the final answer Final Answer: The country with the highest total sales is the USA, with a total of $523.06.  > Finished chain.  'The country with the highest total sales is the USA, with a total of $523.06.'   Looking at the LangSmith trace, we can see:  The agent is using a ReAct style prompt First, it will look at the tables: Action: sql_db_list_tables using tool sql_db_list_tables Given the tables as an observation, it thinks and then determinates the next action: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer  It then formulates the query using the schema from tool sql_db_schema Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10  It finally executes the generated query using tool sql_db_query  Agent task example #2 - Describing a Table​ agent_executor.run(\"Describe the playlisttrack table\")    > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input:  Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: The PlaylistTrack table is the most relevant to the question. Action: sql_db_schema Action Input: PlaylistTrack Observation:  CREATE TABLE \"PlaylistTrack\" (     \"PlaylistId\" INTEGER NOT NULL,      \"TrackId\" INTEGER NOT NULL,      PRIMARY KEY (\"PlaylistId\", \"TrackId\"),      FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"),      FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") )  /* 3 rows from PlaylistTrack table: PlaylistId  TrackId 1   3402 1   3389 1   3390 */ Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.  > Finished chain.  'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.'  Extending the SQL Toolkit​  Although the out-of-the-box SQL Toolkit contains the necessary tools to start working on a database, it is often the case that some extra tools may be useful for extending the agent’s capabilities. This is particularly useful when trying to use domain specific knowledge in the solution, in order to improve its overall performance.  Some examples include:  Including dynamic few shot examples Finding misspellings in proper nouns to use as column filters  We can create separate tools which tackle these specific use cases and include them as a complement to the standard SQL Toolkit. Let’s see how to include these two custom tools.  Including dynamic few-shot examples​  In order to include dynamic few-shot examples, we need a custom Retriever Tool that handles the vector database in order to retrieve the examples that are semantically similar to the user’s question.  Let’s start by creating a dictionary with some examples:  few_shots = {     \"List all artists.\": \"SELECT * FROM artists;\",     \"Find all albums for the artist 'AC/DC'.\": \"SELECT * FROM albums WHERE ArtistId = (SELECT ArtistId FROM artists WHERE Name = 'AC/DC');\",     \"List all tracks in the 'Rock' genre.\": \"SELECT * FROM tracks WHERE GenreId = (SELECT GenreId FROM genres WHERE Name = 'Rock');\",     \"Find the total duration of all tracks.\": \"SELECT SUM(Milliseconds) FROM tracks;\",     \"List all customers from Canada.\": \"SELECT * FROM customers WHERE Country = 'Canada';\",     \"How many tracks are there in the album with ID 5?\": \"SELECT COUNT(*) FROM tracks WHERE AlbumId = 5;\",     \"Find the total number of invoices.\": \"SELECT COUNT(*) FROM invoices;\",     \"List all tracks that are longer than 5 minutes.\": \"SELECT * FROM tracks WHERE Milliseconds > 300000;\",     \"Who are the top 5 customers by total purchase?\": \"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM invoices GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\",     \"Which albums are from the year 2000?\": \"SELECT * FROM albums WHERE strftime('%Y', ReleaseDate) = '2000';\",     \"How many employees are there\": 'SELECT COUNT(*) FROM \"employee\"', }   We can then create a retriever using the list of questions, assigning the target SQL query as metadata:  from langchain.schema import Document from langchain_community.vectorstores import FAISS from langchain_openai import OpenAIEmbeddings  embeddings = OpenAIEmbeddings()  few_shot_docs = [     Document(page_content=question, metadata={\"sql_query\": few_shots[question]})     for question in few_shots.keys() ] vector_db = FAISS.from_documents(few_shot_docs, embeddings) retriever = vector_db.as_retriever()   Now we can create our own custom tool and append it as a new tool in the create_sql_agent function:  from langchain_community.agent_toolkits import create_retriever_tool  tool_description = \"\"\" This tool will help you understand similar examples to adapt them to the user question. Input to this tool should be the user question. \"\"\"  retriever_tool = create_retriever_tool(     retriever, name=\"sql_get_similar_examples\", description=tool_description ) custom_tool_list = [retriever_tool]   Now we can create the agent, adjusting the standard SQL Agent suffix to consider our use case. Although the most straightforward way to handle this would be to include it just in the tool description, this is often not enough and we need to specify it in the agent prompt using the suffix argument in the constructor.  from langchain.agents import AgentType, create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit from langchain_community.utilities import SQLDatabase from langchain_openai import ChatOpenAI  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)  toolkit = SQLDatabaseToolkit(db=db, llm=llm)  custom_suffix = \"\"\" I should first get the similar examples I know. If the examples are enough to construct the query, I can build it. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\"  agent = create_sql_agent(     llm=llm,     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     extra_tools=custom_tool_list,     suffix=custom_suffix, )   Let’s try it out:  agent.run(\"How many employees do we have?\")    > Entering new AgentExecutor chain...  Invoking: `sql_get_similar_examples` with `How many employees do we have?`   [Document(page_content='How many employees are there', metadata={'sql_query': 'SELECT COUNT(*) FROM \"employee\"'}), Document(page_content='Find the total number of invoices.', metadata={'sql_query': 'SELECT COUNT(*) FROM invoices;'})] Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM employee` responded: {content}  SELECT COUNT(*) FROM employee Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee`   [(8,)]We have 8 employees.  > Finished chain.  'We have 8 employees.'   As we can see, the agent first used the sql_get_similar_examples tool in order to retrieve similar examples. As the question was very similar to other few shot examples, the agent didn’t need to use any other tool from the standard Toolkit, thus saving time and tokens.  Finding and correcting misspellings for proper nouns​  In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.  We can achieve this by creating a vector store using all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.  Let’s follow a similar approach to the few shots, but without metadata: just embedding the proper nouns and then querying to get the most similar one to the misspelled user question.  First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:  import ast import re   def run_query_save_results(db, query):     res = db.run(query)     res = [el for sub in ast.literal_eval(res) for el in sub if el]     res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]     return res   artists = run_query_save_results(db, \"SELECT Name FROM Artist\") albums = run_query_save_results(db, \"SELECT Title FROM Album\")   Now we can proceed with creating the custom retriever tool and the final agent:  from langchain_community.agent_toolkits import create_retriever_tool from langchain_community.vectorstores import FAISS from langchain_openai import OpenAIEmbeddings  texts = artists + albums  embeddings = OpenAIEmbeddings() vector_db = FAISS.from_texts(texts, embeddings) retriever = vector_db.as_retriever()  retriever_tool = create_retriever_tool(     retriever,     name=\"name_search\",     description=\"use to learn how a piece of data is actually written, can be from names, surnames addresses etc\", )  custom_tool_list = [retriever_tool]  from langchain.agents import AgentType, create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit from langchain_community.utilities import SQLDatabase from langchain_openai import ChatOpenAI  # db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)  toolkit = SQLDatabaseToolkit(db=db, llm=llm)  custom_suffix = \"\"\" If a user asks for me to filter based on proper nouns, I should first check the spelling using the name_search tool. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\"  agent = create_sql_agent(     llm=llm,     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     extra_tools=custom_tool_list,     suffix=custom_suffix, )   Let’s try it out:  agent.run(\"How many albums does alis in pains have?\")    > Entering new AgentExecutor chain...  Invoking: `name_search` with `alis in pains`   [Document(page_content='House of Pain', metadata={}), Document(page_content='Alice In Chains', metadata={}), Document(page_content='Aisha Duo', metadata={}), Document(page_content='House Of Pain', metadata={})] Invoking: `sql_db_list_tables` with `` responded: {content}  Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Invoking: `sql_db_schema` with `Album, Artist` responded: {content}   CREATE TABLE \"Album\" (     \"AlbumId\" INTEGER NOT NULL,      \"Title\" NVARCHAR(160) NOT NULL,      \"ArtistId\" INTEGER NOT NULL,      PRIMARY KEY (\"AlbumId\"),      FOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\") )  /* 3 rows from Album table: AlbumId Title   ArtistId 1   For Those About To Rock We Salute You   1 2   Balls to the Wall   2 3   Restless and Wild   2 */   CREATE TABLE \"Artist\" (     \"ArtistId\" INTEGER NOT NULL,      \"Name\" NVARCHAR(120),      PRIMARY KEY (\"ArtistId\") )  /* 3 rows from Artist table: ArtistId    Name 1   AC/DC 2   Accept 3   Aerosmith */ Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` responded: {content}  SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains' Invoking: `sql_db_query` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'`   [(1,)]Alice In Chains has 1 album in the database.  > Finished chain.  'Alice In Chains has 1 album in the database.'   As we can see, the agent used the name_search tool in order to check how to correctly query the database for this specific artist.  Go deeper​  To learn more about the SQL Agent and how it works we refer to the SQL Agent Toolkit documentation.  You can also check Agents for other document types: - Pandas Agent - CSV Agent  Elastic Search​  Going beyond the above use-case, there are integrations with other databases.  For example, we can interact with Elasticsearch analytics database.  This chain builds search queries via the Elasticsearch DSL API (filters and aggregations).  The Elasticsearch client must have permissions for index listing, mapping description and search queries.  See here for instructions on how to run Elasticsearch locally.  Make sure to install the Elasticsearch Python client before:  pip install elasticsearch  from elasticsearch import Elasticsearch from langchain.chains.elasticsearch_database import ElasticsearchDatabaseChain from langchain_openai import ChatOpenAI  # Initialize Elasticsearch python client. # See https://elasticsearch-py.readthedocs.io/en/v8.8.2/api.html#elasticsearch.Elasticsearch ELASTIC_SEARCH_SERVER = \"https://elastic:pass@localhost:9200\" db = Elasticsearch(ELASTIC_SEARCH_SERVER)   Uncomment the next cell to initially populate your db.  # customers = [ #     {\"firstname\": \"Jennifer\", \"lastname\": \"Walters\"}, #     {\"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}, #     {\"firstname\": \"Carol\",\"lastname\":\"Danvers\"}, #     {\"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}, #     {\"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}, # ] # for i, customer in enumerate(customers): #     db.create(index=\"customers\", document=customer, id=i)  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, verbose=True)  question = \"What are the first names of all the customers?\" chain.run(question)   We can customize the prompt.  from langchain.prompts.prompt import PromptTemplate  PROMPT_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.  Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question.  Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json.  Use the following format:  Question: Question here ESQuery: Elasticsearch Query formatted as json \"\"\"  PROMPT = PromptTemplate.from_template(     PROMPT_TEMPLATE, ) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, query_prompt=PROMPT)  Previous Using local models Next Interacting with APIs Use case Overview Quickstart Go deeper Case 1: Text-to-SQL query Go deeper Case 2: Text-to-SQL query and execution Go deeper Case 3: SQL agents Agent task example #1 - Running queries Agent task example #2 - Describing a Table Extending the SQL Toolkit Go deeper Elastic Search Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/how_to",
    "title": "How to | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to RunnableParallel: Manipulating data RunnablePassthrough: Passing data through RunnableLambda: Run Custom Functions RunnableBranch: Dynamically route logic based on input Bind runtime args Configure chain internals at runtime Create a runnable with the `@chain` decorator Add fallbacks Stream custom generator functions Inspect your runnables Add message history (memory) Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageHow to How to 📄️ RunnableParallel: Manipulating data  manipulating-inputs-output}  📄️ RunnablePassthrough: Passing data through  passing-data-through}  📄️ RunnableLambda: Run Custom Functions  run-custom-functions}  📄️ RunnableBranch: Dynamically route logic based on input  dynamically-route-logic-based-on-input}  📄️ Bind runtime args  Sometimes we want to invoke a Runnable within a Runnable sequence with  📄️ Configure chain internals at runtime  Oftentimes you may want to experiment with, or even expose to the end  📄️ Create a runnable with the `@chain` decorator  You can also turn an arbitrary function into a chain by adding a  📄️ Add fallbacks  There are many possible points of failure in an LLM application, whether  📄️ Stream custom generator functions  You can use generator functions (ie. functions that use the yield  📄️ Inspect your runnables  Once you create a runnable with LCEL, you may often want to inspect it  📄️ Add message history (memory)  The RunnableWithMessageHistory let’s us add message history to certain  Previous Interface Next RunnableParallel: Manipulating data Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/cookbook",
    "title": "Cookbook | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Prompt + LLM RAG Multiple chains Querying a SQL DB Agents Code writing Routing by semantic similarity Adding memory Adding moderation Managing prompt size Using tools Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageCookbook Cookbook  Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.  📄️ Prompt + LLM  The most common and valuable composition is taking:  📄️ RAG  Let’s look at adding in a retrieval step to a prompt and LLM, which adds  📄️ Multiple chains  Runnables can easily be used to string together multiple Chains  📄️ Querying a SQL DB  We can replicate our SQLDatabaseChain with Runnables.  📄️ Agents  You can pass a Runnable into an agent.  📄️ Code writing  Example of how to use LCEL to write Python code.  📄️ Routing by semantic similarity  With LCEL you can easily add [custom routing  📄️ Adding memory  This shows how to add memory to an arbitrary chain. Right now, you can  📄️ Adding moderation  This shows how to add in moderation (or other safeguards) around your  📄️ Managing prompt size  Agents dynamically call tools. The results of those tool calls are added  📄️ Using tools  You can use any Tools with Runnables easily.  Previous Add message history (memory) Next Prompt + LLM Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/get_started/quickstart",
    "title": "Quickstart | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph Get startedQuickstart Quickstart  In this quickstart we'll show you how to:  Get setup with LangChain, LangSmith and LangServe Use the most basic and common components of LangChain: prompt templates, models, and output parsers Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining Build a simple application with LangChain Trace your application with LangSmith Serve your application with LangServe  That's a fair amount to cover! Let's dive in.  Setup​ Jupyter Notebook​  This guide (and most of the other guides in the documentation) use Jupyter notebooks and assume the reader is as well. Jupyter notebooks are perfect for learning how to work with LLM systems because often times things can go wrong (unexpected output, API down, etc) and going through guides in an interactive environment is a great way to better understand them.  You do not NEED to go through the guide in a Jupyter Notebook, but it is recommended. See here for instructions on how to install.  Installation​  To install LangChain run:  Pip Conda pip install langchain   For more details, see our Installation guide.  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  export LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_API_KEY=\"...\"  Building with LangChain​  LangChain enables building application that connect external sources of data and computation to LLMs. In this quickstart, we will walk through a few different ways of doing that. We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes and LLM to determine whether or not it needs to fetch data to answer questions. We will cover these at a high level, but there are lot of details to all of these! We will link to relevant docs.  LLM Chain​  For this getting started guide, we will provide two options: using OpenAI (a popular model available via API) or using a local open source model.  OpenAI Local  First we'll need to import the LangChain x OpenAI integration package.  pip install langchain-openai   Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:  export OPENAI_API_KEY=\"...\"   We can then initialize the model:  from langchain_openai import ChatOpenAI  llm = ChatOpenAI()   If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:  from langchain_openai import ChatOpenAI  llm = ChatOpenAI(openai_api_key=\"...\")   Once you've installed and initialized the LLM of your choice, we can try using it! Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.  llm.invoke(\"how can langsmith help with testing?\")   We can also guide it's response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.  from langchain_core.prompts import ChatPromptTemplate prompt = ChatPromptTemplate.from_messages([     (\"system\", \"You are world class technical documentation writer.\"),     (\"user\", \"{input}\") ])   We can now combine these into a simple LLM chain:  chain = prompt | llm    We can now invoke it and ask the same question. It still won't know the answer, but it should respond in a more proper tone for a technical writer!  chain.invoke({\"input\": \"how can langsmith help with testing?\"})   The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.  from langchain_core.output_parsers import StrOutputParser  output_parser = StrOutputParser()   We can now add this to the previous chain:  chain = prompt | llm | output_parser   We can now invoke it and ask the same question. The answer will now be a string (rather than a ChatMessage).  chain.invoke({\"input\": \"how can langsmith help with testing?\"})  Diving Deeper​  We've now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.  Retrieval Chain​  In order to properly answer the original question (\"how can langsmith help with testing?\"), we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.  In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.  First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. This requires installing BeautifulSoup:  ```shell pip install beautifulsoup4   After that, we can import and use WebBaseLoader.  from langchain_community.document_loaders import WebBaseLoader loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")  docs = loader.load()   Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.  For embedding models, we once again provide examples for accessing via OpenAI or via local models.  OpenAI Local Make sure you have the `langchain_openai` package installed an the appropriate environment variables set (these are the same as needed for the LLM). from langchain_openai import OpenAIEmbeddings  embeddings = OpenAIEmbeddings()   Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity's sake.  First we need to install the required packages for that:  pip install faiss-cpu   Then we can build our index:  from langchain_community.vectorstores import FAISS from langchain.text_splitter import RecursiveCharacterTextSplitter   text_splitter = RecursiveCharacterTextSplitter() documents = text_splitter.split_documents(docs) vector = FAISS.from_documents(documents, embeddings)   Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.  First, let's set up the chain that takes a question and the retrieved documents and generates an answer.  from langchain.chains.combine_documents import create_stuff_documents_chain  prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:  <context> {context} </context>  Question: {input}\"\"\")  document_chain = create_stuff_documents_chain(llm, prompt)   If we wanted to, we could run this ourselves by passing in documents directly:  from langchain_core.documents import Document  document_chain.invoke({     \"input\": \"how can langsmith help with testing?\",     \"context\": [Document(page_content=\"langsmith can let you visualize test results\")] })   However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.  from langchain.chains import create_retrieval_chain  retriever = vector.as_retriever() retrieval_chain = create_retrieval_chain(retriever, document_chain)   We can now invoke this chain. This returns a dictionary - the response from the LLM is in the answer key  response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"}) print(response[\"answer\"])  # LangSmith offers several features that can help with testing:...   This answer should be much more accurate!  Diving Deeper​  We've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.  Conversation Retrieval Chain​  The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?  We can still use the create_retrieval_chain function, but we need to change two things:  The retrieval method should now not just work on the most recent input, but rather should take the whole history into account. The final LLM chain should likewise take the whole history into account  Updating Retrieval  In order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.  from langchain.chains import create_history_aware_retriever from langchain_core.prompts import MessagesPlaceholder  # First we need a prompt that we can pass into an LLM to generate this search query  prompt = ChatPromptTemplate.from_messages([     MessagesPlaceholder(variable_name=\"chat_history\"),     (\"user\", \"{input}\"),     (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\") ]) retriever_chain = create_history_aware_retriever(llm, retriever, prompt)   We can test this out by passing in an instance where the user is asking a follow up question.  from langchain_core.messages import HumanMessage, AIMessage  chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")] retriever_chain.invoke({     \"chat_history\": chat_history,     \"input\": \"Tell me how\" })   You should see that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.  Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind.  prompt = ChatPromptTemplate.from_messages([     (\"system\", \"Answer the user's questions based on the below context:\\n\\n{context}\"),     MessagesPlaceholder(variable_name=\"chat_history\"),     (\"user\", \"{input}\"), ]) document_chain = create_stuff_documents_chain(llm, prompt)  retrieval_chain = create_retrieval_chain(retriever_chain, document_chain)   We can now test this out end-to-end:  chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")] retrieval_chain.invoke({     \"chat_history\": chat_history,     \"input\": \"Tell me how\" })   We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!  Agent​  We've so far create examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.  NOTE: for this example we will only show how to create an agent using OpenAI models, as local models are not reliable enough yet.  One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access two tools:  The retriever we just created. This will let it easily answer questions about LangSmith A search tool. This will let it easily answer questions that require up to date information.  First, let's set up a tool for the retriever we just created:  from langchain.tools.retriever import create_retriever_tool  retriever_tool = create_retriever_tool(     retriever,     \"langsmith_search\",     \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\", )   The search tool that we will use is Tavily. This will require an API key (they have generous free tier). After creating it on their platform, you need to set it as an environment variable:  export TAVILY_API_KEY=...   If you do not want to set up an API key, you can skip creating this tool.  from langchain_community.tools.tavily_search import TavilySearchResults  search = TavilySearchResults()   We can now create a list of the tools we want to work with:  tools = [retriever_tool, search]   Now that we have the tools, we can create an agent to use them. We will go over this pretty quickly - for a deeper dive into what exactly is going on, check out the Agent's Getting Started documentation  Install langchain hub first  pip install langchainhub   Now we can use it to get a predefined prompt  from langchain_openai import ChatOpenAI from langchain import hub from langchain.agents import create_openai_functions_agent from langchain.agents import AgentExecutor  # Get the prompt to use - you can modify this! prompt = hub.pull(\"hwchase17/openai-functions-agent\") llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) agent = create_openai_functions_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)   We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:  agent_executor.invoke({\"input\": \"how can langsmith help with testing?\"})   We can ask it about the weather:  agent_executor.invoke({\"input\": \"what is the weather in SF?\"})   We can have conversations with it:  chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")] agent_executor.invoke({     \"chat_history\": chat_history,     \"input\": \"Tell me how\" })  Diving Deeper​  We've now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.  Serving with LangServe​  Now that we've built an application, we need to serve it. That's where LangServe comes in. LangServe helps developers deploy LangChain chains as a REST API. You do not need to use LangServe to use LangChain, but in this guide we'll show how you can deploy your app with LangServe.  While the first part of this guide was intended to be run in a Jupyter Notebook, we will now move out of that. We will be creating a Python file and then interacting with it from the command line.  Install with:  pip install \"langserve[all]\"  Server​  To create a server for our application we'll make a serve.py file. This will contain our logic for serving our application. It consists of three things:  The definition of our chain that we just built above Our FastAPI app A definition of a route from which to serve the chain, which is done with langserve.add_routes #!/usr/bin/env python from typing import List  from fastapi import FastAPI from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI from langchain_community.document_loaders import WebBaseLoader from langchain_openai import OpenAIEmbeddings from langchain_community.vectorstores import FAISS from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain.tools.retriever import create_retriever_tool from langchain_community.tools.tavily_search import TavilySearchResults from langchain_openai import ChatOpenAI from langchain import hub from langchain.agents import create_openai_functions_agent from langchain.agents import AgentExecutor from langchain.pydantic_v1 import BaseModel, Field from langchain_core.messages import BaseMessage from langserve import add_routes  # 1. Load Retriever loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\") docs = loader.load() text_splitter = RecursiveCharacterTextSplitter() documents = text_splitter.split_documents(docs) embeddings = OpenAIEmbeddings() vector = FAISS.from_documents(documents, embeddings) retriever = vector.as_retriever()  # 2. Create Tools retriever_tool = create_retriever_tool(     retriever,     \"langsmith_search\",     \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\", ) search = TavilySearchResults() tools = [retriever_tool, search]   # 3. Create Agent prompt = hub.pull(\"hwchase17/openai-functions-agent\") llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0) agent = create_openai_functions_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)   # 4. App definition app = FastAPI(   title=\"LangChain Server\",   version=\"1.0\",   description=\"A simple API server using LangChain's Runnable interfaces\", )  # 5. Adding chain route  # We need to add these input/output schemas because the current AgentExecutor # is lacking in schemas.  class Input(BaseModel):     input: str     chat_history: List[BaseMessage] = Field(         ...,         extra={\"widget\": {\"type\": \"chat\", \"input\": \"location\"}},     )   class Output(BaseModel):     output: str  add_routes(     app,     agent_executor.with_types(input_type=Input, output_type=Output),     path=\"/agent\", )  if __name__ == \"__main__\":     import uvicorn      uvicorn.run(app, host=\"localhost\", port=8000)   And that's it! If we execute this file:  python serve.py   we should see our chain being served at localhost:8000.  Playground​  Every LangServe service comes with a simple built-in UI for configuring and invoking the application with streaming output and visibility into intermediate steps. Head to http://localhost:8000/agent/playground/ to try it out! Pass in the same question as before - \"how can langsmith help with testing?\" - and it should respond same as before.  Client​  Now let's set up a client for programmatically interacting with our service. We can easily do this with the [langserve.RemoteRunnable](/docs/langserve#client). Using this, we can interact with the served chain as if it were running client-side.  from langserve import RemoteRunnable  remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\") remote_chain.invoke({\"input\": \"how can langsmith help with testing?\"})   To learn more about the many other features of LangServe head here.  Next steps​  We've touched on how to build an application with LangChain, how to trace it with LangSmith, and how to serve it with LangServe. There are a lot more features in all three of these than we can cover here. To continue on your journey, we recommend you read the following (in order):  All of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom chains. Model IO covers more details of prompts, LLMs, and output parsers. Retrieval covers more details of everything related to retrieval Agents covers details of everything related to agents Explore common end-to-end use cases and template applications Read up on LangSmith, the platform for debugging, testing, monitoring and more Learn more about serving your applications with LangServe Previous Installation Next Security Setup Jupyter Notebook Installation LangSmith Building with LangChain LLM Chain Diving Deeper Retrieval Chain Diving Deeper Conversation Retrieval Chain Agent Diving Deeper Serving with LangServe Server Playground Client Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/community",
    "title": "Welcome Contributors | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Welcome Contributors Contribute Code Testing Contribute Documentation Contribute Integrations FAQ Welcome Contributors Welcome Contributors  Hi there! Thank you for even being interested in contributing to LangChain. As an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.  🗺️ Guidelines​ 👩‍💻 Ways to contribute​  There are many ways to contribute to LangChain. Here are some common ways people contribute:  Documentation: Help improve our docs, including this one! Code: Help us write code, fix bugs, or improve our infrastructure. Integrations: Help us integrate with your favorite vendors and tools. 🚩GitHub Issues​  Our issues page is kept up to date with bugs, improvements, and feature requests.  There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues.  If you start working on an issue, please assign it to yourself.  If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature. If two issues are related, or blocking, please link them rather than combining them.  We will try to keep these issues as up-to-date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please let us know.  🙋Getting Help​  Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please contact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is smooth for future contributors.  In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help - we do not want these to get in the way of getting good code into the codebase.  🌟 Recognition  If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)! If you have a Twitter account you would like us to mention, please let us know in the PR or through another means.  Next Contribute Code 🗺️ Guidelines 👩‍💻 Ways to contribute 🚩GitHub Issues 🙋Getting Help Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/langserve",
    "title": "🦜️🏓 LangServe | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangServe 🦜️🏓 LangServe       🚩 We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.  Overview​  LangServe helps developers deploy LangChain runnables and chains as a REST API.  This library is integrated with FastAPI and uses pydantic for data validation.  In addition, it provides a client that can be used to call into runnables deployed on a server. A javascript client is available in LangChainJS.  Features​ Input and Output schemas automatically inferred from your LangChain object, and enforced on every API call, with rich error messages API docs page with JSONSchema and Swagger (insert example link) Efficient /invoke/, /batch/ and /stream/ endpoints with support for many concurrent requests on a single server /stream_log/ endpoint for streaming all (or some) intermediate steps from your chain/agent Playground page at /playground/ with streaming output and intermediate steps Built-in (optional) tracing to LangSmith, just add your API key (see Instructions) All built with battle-tested open-source Python libraries like FastAPI, Pydantic, uvloop and asyncio. Use the client SDK to call a LangServe server as if it was a Runnable running locally (or call the HTTP API directly) LangServe Hub Limitations​ Client callbacks are not yet supported for events that originate on the server OpenAPI docs will not be generated when using Pydantic V2. Fast API does not support mixing pydantic v1 and v2 namespaces. See section below for more details. Hosted LangServe​  We will be releasing a hosted version of LangServe for one-click deployments of LangChain applications. Sign up here to get on the waitlist.  Security​ Vulnerability in Versions 0.0.13 - 0.0.15 -- playground endpoint allows accessing arbitrary files on server. Resolved in 0.0.16. Installation​  For both client and server:  pip install \"langserve[all]\"   or pip install \"langserve[client]\" for client code, and pip install \"langserve[server]\" for server code.  LangChain CLI 🛠️​  Use the LangChain CLI to bootstrap a LangServe project quickly.  To use the langchain CLI make sure that you have a recent version of langchain-cli installed. You can install it with pip install -U langchain-cli.  langchain app new ../path/to/directory  Examples​  Get your LangServe instance started quickly with LangChain Templates.  For more examples, see the templates index or the examples directory.  Description Links LLMs Minimal example that reserves OpenAI and Anthropic chat models. Uses async, supports batching and streaming. server, client Retriever Simple server that exposes a retriever as a runnable. server, client Conversational Retriever A Conversational Retriever exposed via LangServe server, client Agent Implementation of an Agent based on OpenAI tools server, client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a session_id supplied by client. server, client RunnableWithMessageHistory to implement chat persisted on backend, keyed off a conversation_id supplied by client, and user_id (see Auth for implementing user_id properly). server, client Configurable Runnable to create a retriever that supports run time configuration of the index name. server, client Configurable Runnable that shows configurable fields and configurable alternatives. server, client APIHandler Shows how to use APIHandler instead of add_routes. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server LCEL Example Example that uses LCEL to manipulate a dictionary input. server, client Auth with add_routes: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes: Implement per user logic and auth for endpoints that use per request config modifier. (Note: At the moment, does not integrate with OpenAPI docs.) server, client Auth with APIHandler: Implement per user logic and auth that shows how to search only within user owned documents. server, client Widgets Different widgets that can be used with playground (file upload and chat) server Widgets File upload widget used for LangServe playground. server, client Sample Application​ Server​  Here's a server that deploys an OpenAI chat model, an Anthropic chat model, and a chain that uses the Anthropic model to tell a joke about a topic.  #!/usr/bin/env python from fastapi import FastAPI from langchain.prompts import ChatPromptTemplate from langchain.chat_models import ChatAnthropic, ChatOpenAI from langserve import add_routes  app = FastAPI(     title=\"LangChain Server\",     version=\"1.0\",     description=\"A simple api server using Langchain's Runnable interfaces\", )  add_routes(     app,     ChatOpenAI(),     path=\"/openai\", )  add_routes(     app,     ChatAnthropic(),     path=\"/anthropic\", )  model = ChatAnthropic() prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") add_routes(     app,     prompt | model,     path=\"/joke\", )  if __name__ == \"__main__\":     import uvicorn      uvicorn.run(app, host=\"localhost\", port=8000)  Docs​  If you've deployed the server above, you can view the generated OpenAPI docs using:  ⚠️ If using pydantic v2, docs will not be generated for invoke, batch, stream, stream_log. See Pydantic section below for more details.  curl localhost:8000/docs   make sure to add the /docs suffix.  ⚠️ Index page / is not defined by design, so curl localhost:8000 or visiting the URL will return a 404. If you want content at / define an endpoint @app.get(\"/\").  Client​  Python SDK   from langchain.schema import SystemMessage, HumanMessage from langchain.prompts import ChatPromptTemplate from langchain.schema.runnable import RunnableMap from langserve import RemoteRunnable  openai = RemoteRunnable(\"http://localhost:8000/openai/\") anthropic = RemoteRunnable(\"http://localhost:8000/anthropic/\") joke_chain = RemoteRunnable(\"http://localhost:8000/joke/\")  joke_chain.invoke({\"topic\": \"parrots\"})  # or async await joke_chain.ainvoke({\"topic\": \"parrots\"})  prompt = [     SystemMessage(content='Act like either a cat or a parrot.'),     HumanMessage(content='Hello!') ]  # Supports astream async for msg in anthropic.astream(prompt):     print(msg, end=\"\", flush=True)  prompt = ChatPromptTemplate.from_messages(     [(\"system\", \"Tell me a long story about {topic}\")] )  # Can define custom chains chain = prompt | RunnableMap({     \"openai\": openai,     \"anthropic\": anthropic, })  chain.batch([{\"topic\": \"parrots\"}, {\"topic\": \"cats\"}])   In TypeScript (requires LangChain.js version 0.0.166 or later):  import {RemoteRunnable} from \"langchain/runnables/remote\";  const chain = new RemoteRunnable({     url: `http://localhost:8000/joke/`, }); const result = await chain.invoke({     topic: \"cats\", });   Python using requests:  import requests  response = requests.post(     \"http://localhost:8000/joke/invoke\",     json={'input': {'topic': 'cats'}} ) response.json()   You can also use curl:  curl --location --request POST 'http://localhost:8000/joke/invoke' \\     --header 'Content-Type: application/json' \\     --data-raw '{         \"input\": {             \"topic\": \"cats\"         }     }'  Endpoints​  The following code:  ... add_routes(     app,     runnable,     path=\"/my_runnable\", )   adds of these endpoints to the server:  POST /my_runnable/invoke - invoke the runnable on a single input POST /my_runnable/batch - invoke the runnable on a batch of inputs POST /my_runnable/stream - invoke on a single input and stream the output POST /my_runnable/stream_log - invoke on a single input and stream the output, including output of intermediate steps as it's generated GET /my_runnable/input_schema - json schema for input to the runnable GET /my_runnable/output_schema - json schema for output of the runnable GET /my_runnable/config_schema - json schema for config of the runnable  These endpoints match the LangChain Expression Language interface -- please reference this documentation for more details.  Playground​  You can find a playground page for your runnable at /my_runnable/playground/. This exposes a simple UI to configure and invoke your runnable with streaming output and intermediate steps.  Widgets​  The playground supports widgets and can be used to test your runnable with different inputs. See the widgets section below for more details.  Sharing​  In addition, for configurable runnables, the playground will allow you to configure the runnable and share a link with the configuration:  Legacy Chains​  LangServe works with both Runnables (constructed via LangChain Expression Language) and legacy chains (inheriting from Chain). However, some of the input schemas for legacy chains may be incomplete/incorrect, leading to errors. This can be fixed by updating the input_schema property of those chains in LangChain. If you encounter any errors, please open an issue on THIS repo, and we will work to address it.  Deployment​ Deploy to AWS​  You can deploy to AWS using the AWS Copilot CLI  copilot init --app [application-name] --name [service-name] --type 'Load Balanced Web Service' --dockerfile './Dockerfile' --deploy   Click here to learn more.  Deploy to Azure​  You can deploy to Azure using Azure Container Apps (Serverless):  az containerapp up --name [container-app-name] --source . --resource-group [resource-group-name] --environment  [environment-name] --ingress external --target-port 8001 --env-vars=OPENAI_API_KEY=your_key   You can find more info here  Deploy to GCP​  You can deploy to GCP Cloud Run using the following command:  gcloud run deploy [your-service-name] --source . --port 8001 --allow-unauthenticated --region us-central1 --set-env-vars=OPENAI_API_KEY=your_key  Community Contributed​ Deploy to Railway​  Example Railway Repo  Pydantic​  LangServe provides support for Pydantic 2 with some limitations.  OpenAPI docs will not be generated for invoke/batch/stream/stream_log when using Pydantic V2. Fast API does not support [mixing pydantic v1 and v2 namespaces]. LangChain uses the v1 namespace in Pydantic v2. Please read the following guidelines to ensure compatibility with LangChain  Except for these limitations, we expect the API endpoints, the playground and any other features to work as expected.  Advanced​ Handling Authentication​  If you need to add authentication to your server, please read Fast API's documentation about dependencies and security.  The below examples show how to wire up authentication logic LangServe endpoints using FastAPI primitives.  You are responsible for providing the actual authentication logic, the users table etc.  If you're not sure what you're doing, you could try using an existing solution Auth0.  Using add_routes​  If you're using add_routes, see examples here.  Description Links Auth with add_routes: Simple authentication that can be applied across all endpoints associated with app. (Not useful on its own for implementing per user logic.) server Auth with add_routes: Simple authentication mechanism based on path dependencies. (No useful on its own for implementing per user logic.) server Auth with add_routes: Implement per user logic and auth for endpoints that use per request config modifier. (Note: At the moment, does not integrate with OpenAPI docs.) server, client  Alternatively, you can use FastAPI's middleware.  Using global dependencies and path dependencies has the advantage that auth will be properly supported in the OpenAPI docs page, but these are not sufficient for implement per user logic (e.g., making an application that can search only within user owned documents).  If you need to implement per user logic, you can use the per_req_config_modifier or APIHandler (below) to implement this logic.  Per User  If you need authorization or logic that is user dependent, specify per_req_config_modifier when using add_routes. Use a callable receives the raw Request object and can extract relevant information from it for authentication and authorization purposes.  Using APIHandler​  If you feel comfortable with FastAPI and python, you can use LangServe's APIHandler.  Description Links Auth with APIHandler: Implement per user logic and auth that shows how to search only within user owned documents. server, client APIHandler Shows how to use APIHandler instead of add_routes. This provides more flexibility for developers to define endpoints. Works well with all FastAPI patterns, but takes a bit more effort. server, client  It's a bit more work, but gives you complete control over the endpoint definitions, so you can do whatever custom logic you need for auth.  Files​  LLM applications often deal with files. There are different architectures that can be made to implement file processing; at a high level:  The file may be uploaded to the server via a dedicated endpoint and processed using a separate endpoint The file may be uploaded by either value (bytes of file) or reference (e.g., s3 url to file content) The processing endpoint may be blocking or non-blocking If significant processing is required, the processing may be offloaded to a dedicated process pool  You should determine what is the appropriate architecture for your application.  Currently, to upload files by value to a runnable, use base64 encoding for the file (multipart/form-data is not supported yet).  Here's an example that shows how to use base64 encoding to send a file to a remote runnable.  Remember, you can always upload files by reference (e.g., s3 url) or upload them as multipart/form-data to a dedicated endpoint.  Custom Input and Output Types​  Input and Output types are defined on all runnables.  You can access them via the input_schema and output_schema properties.  LangServe uses these types for validation and documentation.  If you want to override the default inferred types, you can use the with_types method.  Here's a toy example to illustrate the idea:  from typing import Any  from fastapi import FastAPI from langchain.schema.runnable import RunnableLambda  app = FastAPI()   def func(x: Any) -> int:     \"\"\"Mistyped function that should accept an int but accepts anything.\"\"\"     return x + 1   runnable = RunnableLambda(func).with_types(     input_schema=int, )  add_routes(app, runnable)  Custom User Types​  Inherit from CustomUserType if you want the data to de-serialize into a pydantic model rather than the equivalent dict representation.  At the moment, this type only works server side and is used to specify desired decoding behavior. If inheriting from this type the server will keep the decoded type as a pydantic model instead of converting it into a dict.  from fastapi import FastAPI from langchain.schema.runnable import RunnableLambda  from langserve import add_routes from langserve.schema import CustomUserType  app = FastAPI()   class Foo(CustomUserType):     bar: int   def func(foo: Foo) -> int:     \"\"\"Sample function that expects a Foo type which is a pydantic model\"\"\"     assert isinstance(foo, Foo)     return foo.bar   # Note that the input and output type are automatically inferred! # You do not need to specify them. # runnable = RunnableLambda(func).with_types( # <-- Not needed in this case #     input_schema=Foo, #     output_schema=int, # add_routes(app, RunnableLambda(func), path=\"/foo\")  Playground Widgets​  The playground allows you to define custom widgets for your runnable from the backend.  Here are a few examples:  Description Links Widgets Different widgets that can be used with playground (file upload and chat) server, client Widgets File upload widget used for LangServe playground. server, client Schema​ A widget is specified at the field level and shipped as part of the JSON schema of the input type A widget must contain a key called type with the value being one of a well known list of widgets Other widget keys will be associated with values that describe paths in a JSON object type JsonPath = number | string | (number | string)[]; type NameSpacedPath = { title: string; path: JsonPath }; // Using title to mimick json schema, but can use namespace type OneOfPath = { oneOf: JsonPath[] };  type Widget = {     type: string // Some well known type (e.g., base64file, chat etc.)     [key: string]: JsonPath | NameSpacedPath | OneOfPath; };  Available Widgets​  There are only two widgets that the user can specify manually right now:  File Upload Widget Chat History Widget  See below more information about these widgets.  All other widgets on the playground UI are created and managed automatically by the UI based on the config schema of the Runnable. When you create Configurable Runnables, the playground should create appropriate widgets for you to control the behavior.  File Upload Widget​  Allows creation of a file upload input in the UI playground for files that are uploaded as base64 encoded strings. Here's the full example.  Snippet:  try:     from pydantic.v1 import Field except ImportError:     from pydantic import Field  from langserve import CustomUserType   # ATTENTION: Inherit from CustomUserType instead of BaseModel otherwise #            the server will decode it into a dict instead of a pydantic model. class FileProcessingRequest(CustomUserType):     \"\"\"Request including a base64 encoded file.\"\"\"      # The extra field is used to specify a widget for the playground UI.     file: str = Field(..., extra={\"widget\": {\"type\": \"base64file\"}})     num_chars: int = 100    Example widget:  Chat Widget​  Look at widget example.  To define a chat widget, make sure that you pass \"type\": \"chat\".  \"input\" is JSONPath to the field in the Request that has the new input message. \"output\" is JSONPath to the field in the Response that has new output message(s). Don't specify these fields if the entire input or output should be used as they are ( e.g., if the output is a list of chat messages.)  Here's a snippet:   class ChatHistory(CustomUserType):     chat_history: List[Tuple[str, str]] = Field(         ...,         examples=[[(\"human input\", \"ai response\")]],         extra={\"widget\": {\"type\": \"chat\", \"input\": \"question\", \"output\": \"answer\"}},     )     question: str   def _format_to_messages(input: ChatHistory) -> List[BaseMessage]:     \"\"\"Format the input to a list of messages.\"\"\"     history = input.chat_history     user_input = input.question      messages = []      for human, ai in history:         messages.append(HumanMessage(content=human))         messages.append(AIMessage(content=ai))     messages.append(HumanMessage(content=user_input))     return messages   model = ChatOpenAI() chat_model = RunnableParallel({\"answer\": (RunnableLambda(_format_to_messages) | model)}) add_routes(     app,     chat_model.with_types(input_type=ChatHistory),     config_keys=[\"configurable\"],     path=\"/chat\", )   Example widget:  Enabling / Disabling Endpoints (LangServe >=0.0.33)​  You can enable / disable which endpoints are exposed. Use enabled_endpoints if you want to make sure to never get a new endpoint when upgrading langserve to a newer verison.  Enable: The code below will only enable invoke, batch and the corresponding config_hash endpoint variants.  add_routes(app, chain, enabled_endpoints=[\"invoke\", \"batch\", \"config_hashes\"])   Disable: The code below will disable the playground for the chain  add_routes(app, chain, disabled_endpoints=[\"playground\"])  Previous Token counting Next LangSmith Overview Features Limitations Hosted LangServe Security Installation LangChain CLI 🛠️ Examples Sample Application Server Docs Client Endpoints Playground Widgets Sharing Legacy Chains Deployment Deploy to AWS Deploy to Azure Deploy to GCP Community Contributed Pydantic Advanced Handling Authentication Files Custom Input and Output Types Custom User Types Playground Widgets Available Widgets Chat Widget Enabling / Disabling Endpoints (LangServe >=0.0.33) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/why",
    "title": "Why use LCEL | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageWhy use LCEL Why use LCEL  We recommend reading the LCEL Get started section first.  LCEL makes it easy to build complex chains from basic components. It does this by providing: 1. A unified interface: Every LCEL object implements the Runnable interface, which defines a common set of invocation methods (invoke, batch, stream, ainvoke, …). This makes it possible for chains of LCEL objects to also automatically support these invocations. That is, every chain of LCEL objects is itself an LCEL object. 2. Composition primitives: LCEL provides a number of primitives that make it easy to compose chains, parallelize components, add fallbacks, dynamically configure chain internal, and more.  To better understand the value of LCEL, it’s helpful to see it in action and think about how we might recreate similar functionality without it. In this walkthrough we’ll do just that with our basic example from the get started section. We’ll take our simple prompt + model chain, which under the hood already defines a lot of functionality, and see what it would take to recreate all of it.  %pip install –upgrade –quiet langchain-core langchain-openai  from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain_core.output_parsers import StrOutputParser   prompt = ChatPromptTemplate.from_template(\"Tell me a short joke about {topic}\") model = ChatOpenAI(model=\"gpt-3.5-turbo\") output_parser = StrOutputParser()  chain = prompt | model | output_parser  Invoke​  In the simplest case, we just want to pass in a topic string and get back a joke string:  Without LCEL​ from typing import List  import openai   prompt_template = \"Tell me a short joke about {topic}\" client = openai.OpenAI()  def call_chat_model(messages: List[dict]) -> str:     response = client.chat.completions.create(         model=\"gpt-3.5-turbo\",          messages=messages,     )     return response.choices[0].message.content  def invoke_chain(topic: str) -> str:     prompt_value = prompt_template.format(topic=topic)     messages = [{\"role\": \"user\", \"content\": prompt_value}]     return call_chat_model(messages)  invoke_chain(\"ice cream\")  LCEL​ from langchain_core.runnables import RunnablePassthrough   prompt = ChatPromptTemplate.from_template(     \"Tell me a short joke about {topic}\" ) output_parser = StrOutputParser() model = ChatOpenAI(model=\"gpt-3.5-turbo\") chain = (     {\"topic\": RunnablePassthrough()}      | prompt     | model     | output_parser )  chain.invoke(\"ice cream\")  Stream​  If we want to stream results instead, we’ll need to change our function:  Without LCEL​ from typing import Iterator   def stream_chat_model(messages: List[dict]) -> Iterator[str]:     stream = client.chat.completions.create(         model=\"gpt-3.5-turbo\",         messages=messages,         stream=True,     )     for response in stream:         content = response.choices[0].delta.content         if content is not None:             yield content  def stream_chain(topic: str) -> Iterator[str]:     prompt_value = prompt.format(topic=topic)     return stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])   for chunk in stream_chain(\"ice cream\"):     print(chunk, end=\"\", flush=True)  LCEL​ for chunk in chain.stream(\"ice cream\"):     print(chunk, end=\"\", flush=True)  Batch​  If we want to run on a batch of inputs in parallel, we’ll again need a new function:  Without LCEL​ from concurrent.futures import ThreadPoolExecutor   def batch_chain(topics: list) -> list:     with ThreadPoolExecutor(max_workers=5) as executor:         return list(executor.map(invoke_chain, topics))  batch_chain([\"ice cream\", \"spaghetti\", \"dumplings\"])  LCEL​ chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])  Async​  If we need an asynchronous version:  Without LCEL​ async_client = openai.AsyncOpenAI()  async def acall_chat_model(messages: List[dict]) -> str:     response = await async_client.chat.completions.create(         model=\"gpt-3.5-turbo\",          messages=messages,     )     return response.choices[0].message.content  async def ainvoke_chain(topic: str) -> str:     prompt_value = prompt_template.format(topic=topic)     messages = [{\"role\": \"user\", \"content\": prompt_value}]     return await acall_chat_model(messages)  await ainvoke_chain(\"ice cream\")  LCEL​ chain.ainvoke(\"ice cream\")  LLM instead of chat model​  If we want to use a completion endpoint instead of a chat endpoint:  Without LCEL​ def call_llm(prompt_value: str) -> str:     response = client.completions.create(         model=\"gpt-3.5-turbo-instruct\",         prompt=prompt_value,     )     return response.choices[0].text  def invoke_llm_chain(topic: str) -> str:     prompt_value = prompt_template.format(topic=topic)     return call_llm(prompt_value)  invoke_llm_chain(\"ice cream\")  LCEL​ from langchain_openai import OpenAI  llm = OpenAI(model=\"gpt-3.5-turbo-instruct\") llm_chain = (     {\"topic\": RunnablePassthrough()}      | prompt     | llm     | output_parser )  llm_chain.invoke(\"ice cream\")  Different model provider​  If we want to use Anthropic instead of OpenAI:  Without LCEL​ import anthropic  anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\" anthropic_client = anthropic.Anthropic()  def call_anthropic(prompt_value: str) -> str:     response = anthropic_client.completions.create(         model=\"claude-2\",         prompt=prompt_value,         max_tokens_to_sample=256,     )     return response.completion      def invoke_anthropic_chain(topic: str) -> str:     prompt_value = anthropic_template.format(topic=topic)     return call_anthropic(prompt_value)  invoke_anthropic_chain(\"ice cream\")  LCEL​ from langchain_community.chat_models import ChatAnthropic  anthropic = ChatAnthropic(model=\"claude-2\") anthropic_chain = (     {\"topic\": RunnablePassthrough()}      | prompt      | anthropic     | output_parser )  anthropic_chain.invoke(\"ice cream\")  Runtime configurability​  If we wanted to make the choice of chat model or LLM configurable at runtime:  Without LCEL​ def invoke_configurable_chain(     topic: str,      *,      model: str = \"chat_openai\" ) -> str:     if model == \"chat_openai\":         return invoke_chain(topic)     elif model == \"openai\":         return invoke_llm_chain(topic)     elif model == \"anthropic\":         return invoke_anthropic_chain(topic)     else:         raise ValueError(             f\"Received invalid model '{model}'.\"             \" Expected one of chat_openai, openai, anthropic\"         )  def stream_configurable_chain(     topic: str,      *,      model: str = \"chat_openai\" ) -> Iterator[str]:     if model == \"chat_openai\":         return stream_chain(topic)     elif model == \"openai\":         # Note we haven't implemented this yet.         return stream_llm_chain(topic)     elif model == \"anthropic\":         # Note we haven't implemented this yet         return stream_anthropic_chain(topic)     else:         raise ValueError(             f\"Received invalid model '{model}'.\"             \" Expected one of chat_openai, openai, anthropic\"         )  def batch_configurable_chain(     topics: List[str],      *,      model: str = \"chat_openai\" ) -> List[str]:     # You get the idea     ...  async def abatch_configurable_chain(     topics: List[str],      *,      model: str = \"chat_openai\" ) -> List[str]:     ...  invoke_configurable_chain(\"ice cream\", model=\"openai\") stream = stream_configurable_chain(     \"ice_cream\",      model=\"anthropic\" ) for chunk in stream:     print(chunk, end=\"\", flush=True)  # batch_configurable_chain([\"ice cream\", \"spaghetti\", \"dumplings\"]) # await ainvoke_configurable_chain(\"ice cream\")  With LCEL​ from langchain_core.runnables import ConfigurableField   configurable_model = model.configurable_alternatives(     ConfigurableField(id=\"model\"),      default_key=\"chat_openai\",      openai=llm,     anthropic=anthropic, ) configurable_chain = (     {\"topic\": RunnablePassthrough()}      | prompt      | configurable_model      | output_parser )  configurable_chain.invoke(     \"ice cream\",      config={\"model\": \"openai\"} ) stream = configurable_chain.stream(     \"ice cream\",      config={\"model\": \"anthropic\"} ) for chunk in stream:     print(chunk, end=\"\", flush=True)  configurable_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])  # await configurable_chain.ainvoke(\"ice cream\")  Logging​  If we want to log our intermediate results:  Without LCEL​  We’ll print intermediate steps for illustrative purposes  def invoke_anthropic_chain_with_logging(topic: str) -> str:     print(f\"Input: {topic}\")     prompt_value = anthropic_template.format(topic=topic)     print(f\"Formatted prompt: {prompt_value}\")     output = call_anthropic(prompt_value)     print(f\"Output: {output}\")     return output  invoke_anthropic_chain_with_logging(\"ice cream\")  LCEL​  Every component has built-in integrations with LangSmith. If we set the following two environment variables, all chain traces are logged to LangSmith.  import os  os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  anthropic_chain.invoke(\"ice cream\")   Here’s what our LangSmith trace looks like: https://smith.langchain.com/public/e4de52f8-bcd9-4732-b950-deee4b04e313/r  Fallbacks​  If we wanted to add fallback logic, in case one model API is down:  Without LCEL​ def invoke_chain_with_fallback(topic: str) -> str:     try:         return invoke_chain(topic)     except Exception:         return invoke_anthropic_chain(topic)  async def ainvoke_chain_with_fallback(topic: str) -> str:     try:         return await ainvoke_chain(topic)     except Exception:         # Note: we haven't actually implemented this.         return ainvoke_anthropic_chain(topic)  async def batch_chain_with_fallback(topics: List[str]) -> str:     try:         return batch_chain(topics)     except Exception:         # Note: we haven't actually implemented this.         return batch_anthropic_chain(topics)  invoke_chain_with_fallback(\"ice cream\") # await ainvoke_chain_with_fallback(\"ice cream\") batch_chain_with_fallback([\"ice cream\", \"spaghetti\", \"dumplings\"]))  LCEL​ fallback_chain = chain.with_fallbacks([anthropic_chain])  fallback_chain.invoke(\"ice cream\") # await fallback_chain.ainvoke(\"ice cream\") fallback_chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\"])  Full code comparison​  Even in this simple case, our LCEL chain succinctly packs in a lot of functionality. As chains become more complex, this becomes especially valuable.  Without LCEL​ from concurrent.futures import ThreadPoolExecutor from typing import Iterator, List, Tuple  import anthropic import openai   prompt_template = \"Tell me a short joke about {topic}\" anthropic_template = f\"Human:\\n\\n{prompt_template}\\n\\nAssistant:\" client = openai.OpenAI() async_client = openai.AsyncOpenAI() anthropic_client = anthropic.Anthropic()  def call_chat_model(messages: List[dict]) -> str:     response = client.chat.completions.create(         model=\"gpt-3.5-turbo\",          messages=messages,     )     return response.choices[0].message.content  def invoke_chain(topic: str) -> str:     print(f\"Input: {topic}\")     prompt_value = prompt_template.format(topic=topic)     print(f\"Formatted prompt: {prompt_value}\")     messages = [{\"role\": \"user\", \"content\": prompt_value}]     output = call_chat_model(messages)     print(f\"Output: {output}\")     return output  def stream_chat_model(messages: List[dict]) -> Iterator[str]:     stream = client.chat.completions.create(         model=\"gpt-3.5-turbo\",         messages=messages,         stream=True,     )     for response in stream:         content = response.choices[0].delta.content         if content is not None:             yield content  def stream_chain(topic: str) -> Iterator[str]:     print(f\"Input: {topic}\")     prompt_value = prompt.format(topic=topic)     print(f\"Formatted prompt: {prompt_value}\")     stream = stream_chat_model([{\"role\": \"user\", \"content\": prompt_value}])     for chunk in stream:         print(f\"Token: {chunk}\", end=\"\")         yield chunk  def batch_chain(topics: list) -> list:     with ThreadPoolExecutor(max_workers=5) as executor:         return list(executor.map(invoke_chain, topics))  def call_llm(prompt_value: str) -> str:     response = client.completions.create(         model=\"gpt-3.5-turbo-instruct\",         prompt=prompt_value,     )     return response.choices[0].text  def invoke_llm_chain(topic: str) -> str:     print(f\"Input: {topic}\")     prompt_value = promtp_template.format(topic=topic)     print(f\"Formatted prompt: {prompt_value}\")     output = call_llm(prompt_value)     print(f\"Output: {output}\")     return output  def call_anthropic(prompt_value: str) -> str:     response = anthropic_client.completions.create(         model=\"claude-2\",         prompt=prompt_value,         max_tokens_to_sample=256,     )     return response.completion     def invoke_anthropic_chain(topic: str) -> str:     print(f\"Input: {topic}\")     prompt_value = anthropic_template.format(topic=topic)     print(f\"Formatted prompt: {prompt_value}\")     output = call_anthropic(prompt_value)     print(f\"Output: {output}\")     return output  async def ainvoke_anthropic_chain(topic: str) -> str:     ...  def stream_anthropic_chain(topic: str) -> Iterator[str]:     ...  def batch_anthropic_chain(topics: List[str]) -> List[str]:     ...  def invoke_configurable_chain(     topic: str,      *,      model: str = \"chat_openai\" ) -> str:     if model == \"chat_openai\":         return invoke_chain(topic)     elif model == \"openai\":         return invoke_llm_chain(topic)     elif model == \"anthropic\":         return invoke_anthropic_chain(topic)     else:         raise ValueError(             f\"Received invalid model '{model}'.\"             \" Expected one of chat_openai, openai, anthropic\"         )  def stream_configurable_chain(     topic: str,      *,      model: str = \"chat_openai\" ) -> Iterator[str]:     if model == \"chat_openai\":         return stream_chain(topic)     elif model == \"openai\":         # Note we haven't implemented this yet.         return stream_llm_chain(topic)     elif model == \"anthropic\":         # Note we haven't implemented this yet         return stream_anthropic_chain(topic)     else:         raise ValueError(             f\"Received invalid model '{model}'.\"             \" Expected one of chat_openai, openai, anthropic\"         )  def batch_configurable_chain(     topics: List[str],      *,      model: str = \"chat_openai\" ) -> List[str]:     ...  async def abatch_configurable_chain(     topics: List[str],      *,      model: str = \"chat_openai\" ) -> List[str]:     ...  def invoke_chain_with_fallback(topic: str) -> str:     try:         return invoke_chain(topic)     except Exception:         return invoke_anthropic_chain(topic)  async def ainvoke_chain_with_fallback(topic: str) -> str:     try:         return await ainvoke_chain(topic)     except Exception:         return ainvoke_anthropic_chain(topic)  async def batch_chain_with_fallback(topics: List[str]) -> str:     try:         return batch_chain(topics)     except Exception:         return batch_anthropic_chain(topics)  LCEL​ import os  from langchain_community.chat_models import ChatAnthropic from langchain_openai import ChatOpenAI from langchain_openai import OpenAI from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import RunnablePassthrough, ConfigurableField  os.environ[\"LANGCHAIN_API_KEY\"] = \"...\" os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"  prompt = ChatPromptTemplate.from_template(     \"Tell me a short joke about {topic}\" ) chat_openai = ChatOpenAI(model=\"gpt-3.5-turbo\") openai = OpenAI(model=\"gpt-3.5-turbo-instruct\") anthropic = ChatAnthropic(model=\"claude-2\") model = (     chat_openai     .with_fallbacks([anthropic])     .configurable_alternatives(         ConfigurableField(id=\"model\"),         default_key=\"chat_openai\",         openai=openai,         anthropic=anthropic,     ) )  chain = (     {\"topic\": RunnablePassthrough()}      | prompt      | model      | StrOutputParser() )  Next steps​  To continue learning about LCEL, we recommend: - Reading up on the full LCEL Interface, which we’ve only partially covered here. - Exploring the How-to section to learn about additional composition primitives that LCEL provides. - Looking through the Cookbook section to see LCEL in action for common use cases. A good next use case to look at would be Retrieval-augmented generation.  Previous Get started Next Interface Invoke Stream Batch Async LLM instead of chat model Different model provider Runtime configurability Logging Fallbacks Full code comparison Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers",
    "title": "Providers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores Providers Providers 📄️ Anthropic  All functionality related to Anthropic models.  📄️ AWS  The LangChain integrations related to Amazon AWS platform.  📄️ Google  All functionality related to Google Cloud Platform and other Google products.  📄️ Hugging Face  All functionality related to the Hugging Face Platform.  📄️ Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  📄️ OpenAI  All functionality related to OpenAI  🗃️ More  201 items  Next Anthropic Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/interface",
    "title": "Interface | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageInterface Interface  To make it as easy as possible to create custom chains, we’ve implemented a “Runnable” protocol. The Runnable protocol is implemented for most components. This is a standard interface, which makes it easy to define custom chains as well as invoke them in a standard way. The standard interface includes:  stream: stream back chunks of the response invoke: call the chain on an input batch: call the chain on a list of inputs  These also have corresponding async methods:  astream: stream back chunks of the response async ainvoke: call the chain on an input async abatch: call the chain on a list of inputs async astream_log: stream back intermediate steps as they happen, in addition to the final response  The input type and output type varies by component:  Component Input Type Output Type Prompt Dictionary PromptValue ChatModel Single string, list of chat messages or a PromptValue ChatMessage LLM Single string, list of chat messages or a PromptValue String OutputParser The output of an LLM or ChatModel Depends on the parser Retriever Single string List of Documents Tool Single string or dictionary, depending on the tool Depends on the tool  All runnables expose input and output schemas to inspect the inputs and outputs: - input_schema: an input Pydantic model auto-generated from the structure of the Runnable - output_schema: an output Pydantic model auto-generated from the structure of the Runnable  Let’s take a look at these methods. To do so, we’ll create a super simple PromptTemplate + ChatModel chain.  %pip install –upgrade –quiet langchain-core langchain-community langchain-openai  from langchain_core.prompts import ChatPromptTemplate from langchain_openai import ChatOpenAI  model = ChatOpenAI() prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") chain = prompt | model  Input Schema​  A description of the inputs accepted by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.  # The input schema of the chain is the input schema of its first part, the prompt. chain.input_schema.schema()  {'title': 'PromptInput',  'type': 'object',  'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}  prompt.input_schema.schema()  {'title': 'PromptInput',  'type': 'object',  'properties': {'topic': {'title': 'Topic', 'type': 'string'}}}  model.input_schema.schema()  {'title': 'ChatOpenAIInput',  'anyOf': [{'type': 'string'},   {'$ref': '#/definitions/StringPromptValue'},   {'$ref': '#/definitions/ChatPromptValueConcrete'},   {'type': 'array',    'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},      {'$ref': '#/definitions/HumanMessage'},      {'$ref': '#/definitions/ChatMessage'},      {'$ref': '#/definitions/SystemMessage'},      {'$ref': '#/definitions/FunctionMessage'}]}}],  'definitions': {'StringPromptValue': {'title': 'StringPromptValue',    'description': 'String prompt value.',    'type': 'object',    'properties': {'text': {'title': 'Text', 'type': 'string'},     'type': {'title': 'Type',      'default': 'StringPromptValue',      'enum': ['StringPromptValue'],      'type': 'string'}},    'required': ['text']},   'AIMessage': {'title': 'AIMessage',    'description': 'A Message from an AI.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'ai',      'enum': ['ai'],      'type': 'string'},     'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},    'required': ['content']},   'HumanMessage': {'title': 'HumanMessage',    'description': 'A Message from a human.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'human',      'enum': ['human'],      'type': 'string'},     'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},    'required': ['content']},   'ChatMessage': {'title': 'ChatMessage',    'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'chat',      'enum': ['chat'],      'type': 'string'},     'role': {'title': 'Role', 'type': 'string'}},    'required': ['content', 'role']},   'SystemMessage': {'title': 'SystemMessage',    'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'system',      'enum': ['system'],      'type': 'string'}},    'required': ['content']},   'FunctionMessage': {'title': 'FunctionMessage',    'description': 'A Message for passing the result of executing a function back to a model.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'function',      'enum': ['function'],      'type': 'string'},     'name': {'title': 'Name', 'type': 'string'}},    'required': ['content', 'name']},   'ChatPromptValueConcrete': {'title': 'ChatPromptValueConcrete',    'description': 'Chat prompt value which explicitly lists out the message types it accepts.\\nFor use in external schemas.',    'type': 'object',    'properties': {'messages': {'title': 'Messages',      'type': 'array',      'items': {'anyOf': [{'$ref': '#/definitions/AIMessage'},        {'$ref': '#/definitions/HumanMessage'},        {'$ref': '#/definitions/ChatMessage'},        {'$ref': '#/definitions/SystemMessage'},        {'$ref': '#/definitions/FunctionMessage'}]}},     'type': {'title': 'Type',      'default': 'ChatPromptValueConcrete',      'enum': ['ChatPromptValueConcrete'],      'type': 'string'}},    'required': ['messages']}}}  Output Schema​  A description of the outputs produced by a Runnable. This is a Pydantic model dynamically generated from the structure of any Runnable. You can call .schema() on it to obtain a JSONSchema representation.  # The output schema of the chain is the output schema of its last part, in this case a ChatModel, which outputs a ChatMessage chain.output_schema.schema()  {'title': 'ChatOpenAIOutput',  'anyOf': [{'$ref': '#/definitions/HumanMessage'},   {'$ref': '#/definitions/AIMessage'},   {'$ref': '#/definitions/ChatMessage'},   {'$ref': '#/definitions/FunctionMessage'},   {'$ref': '#/definitions/SystemMessage'}],  'definitions': {'HumanMessage': {'title': 'HumanMessage',    'description': 'A Message from a human.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'human',      'enum': ['human'],      'type': 'string'},     'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},    'required': ['content']},   'AIMessage': {'title': 'AIMessage',    'description': 'A Message from an AI.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'ai',      'enum': ['ai'],      'type': 'string'},     'example': {'title': 'Example', 'default': False, 'type': 'boolean'}},    'required': ['content']},   'ChatMessage': {'title': 'ChatMessage',    'description': 'A Message that can be assigned an arbitrary speaker (i.e. role).',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'chat',      'enum': ['chat'],      'type': 'string'},     'role': {'title': 'Role', 'type': 'string'}},    'required': ['content', 'role']},   'FunctionMessage': {'title': 'FunctionMessage',    'description': 'A Message for passing the result of executing a function back to a model.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'function',      'enum': ['function'],      'type': 'string'},     'name': {'title': 'Name', 'type': 'string'}},    'required': ['content', 'name']},   'SystemMessage': {'title': 'SystemMessage',    'description': 'A Message for priming AI behavior, usually passed in as the first of a sequence\\nof input messages.',    'type': 'object',    'properties': {'content': {'title': 'Content', 'type': 'string'},     'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},     'type': {'title': 'Type',      'default': 'system',      'enum': ['system'],      'type': 'string'}},    'required': ['content']}}}  Stream​ for s in chain.stream({\"topic\": \"bears\"}):     print(s.content, end=\"\", flush=True)  Why don't bears wear shoes?  Because they already have bear feet!  Invoke​ chain.invoke({\"topic\": \"bears\"})  AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\")  Batch​ chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])  [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\"),  AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]   You can set the number of concurrent requests by using the max_concurrency parameter  chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}], config={\"max_concurrency\": 5})  [AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\"),  AIMessage(content=\"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\")]  Async Stream​ async for s in chain.astream({\"topic\": \"bears\"}):     print(s.content, end=\"\", flush=True)  Sure, here's a bear-themed joke for you:  Why don't bears wear shoes?  Because they already have bear feet!  Async Invoke​ await chain.ainvoke({\"topic\": \"bears\"})  AIMessage(content=\"Why don't bears wear shoes? \\n\\nBecause they have bear feet!\")  Async Batch​ await chain.abatch([{\"topic\": \"bears\"}])  [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\")]  Async Stream Intermediate Steps​  All runnables also have a method .astream_log() which is used to stream (as they happen) all or part of the intermediate steps of your chain/sequence.  This is useful to show progress to the user, to use intermediate results, or to debug your chain.  You can stream all steps (default) or include/exclude steps by name, tags or metadata.  This method yields JSONPatch ops that when applied in the same order as received build up the RunState.  class LogEntry(TypedDict):     id: str     \"\"\"ID of the sub-run.\"\"\"     name: str     \"\"\"Name of the object being run.\"\"\"     type: str     \"\"\"Type of the object being run, eg. prompt, chain, llm, etc.\"\"\"     tags: List[str]     \"\"\"List of tags for the run.\"\"\"     metadata: Dict[str, Any]     \"\"\"Key-value pairs of metadata for the run.\"\"\"     start_time: str     \"\"\"ISO-8601 timestamp of when the run started.\"\"\"      streamed_output_str: List[str]     \"\"\"List of LLM tokens streamed by this run, if applicable.\"\"\"     final_output: Optional[Any]     \"\"\"Final output of this run.     Only available after the run has finished successfully.\"\"\"     end_time: Optional[str]     \"\"\"ISO-8601 timestamp of when the run ended.     Only available after the run has finished.\"\"\"   class RunState(TypedDict):     id: str     \"\"\"ID of the run.\"\"\"     streamed_output: List[Any]     \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"     final_output: Optional[Any]     \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.     Only available after the run has finished successfully.\"\"\"      logs: Dict[str, LogEntry]     \"\"\"Map of run names to sub-runs. If filters were supplied, this list will     contain only the runs that matched the filters.\"\"\"  Streaming JSONPatch chunks​  This is useful eg. to stream the JSONPatch in an HTTP server, and then apply the ops on the client to rebuild the run state there. See LangServe for tooling to make it easier to build a webserver from any Runnable.  from langchain_community.vectorstores import FAISS from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough from langchain_openai import OpenAIEmbeddings  template = \"\"\"Answer the question based only on the following context: {context}  Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(template)  vectorstore = FAISS.from_texts(     [\"harrison worked at kensho\"], embedding=OpenAIEmbeddings() ) retriever = vectorstore.as_retriever()  retrieval_chain = (     {         \"context\": retriever.with_config(run_name=\"Docs\"),         \"question\": RunnablePassthrough(),     }     | prompt     | model     | StrOutputParser() )  async for chunk in retrieval_chain.astream_log(     \"where did harrison work?\", include_names=[\"Docs\"] ):     print(\"-\" * 40)     print(chunk)  ---------------------------------------- RunLogPatch({'op': 'replace',   'path': '',   'value': {'final_output': None,             'id': 'e2f2cc72-eb63-4d20-8326-237367482efb',             'logs': {},             'streamed_output': []}}) ---------------------------------------- RunLogPatch({'op': 'add',   'path': '/logs/Docs',   'value': {'end_time': None,             'final_output': None,             'id': '8da492cc-4492-4e74-b8b0-9e60e8693390',             'metadata': {},             'name': 'Docs',             'start_time': '2023-10-19T17:50:13.526',             'streamed_output_str': [],             'tags': ['map:key:context', 'FAISS'],             'type': 'retriever'}}) ---------------------------------------- RunLogPatch({'op': 'add',   'path': '/logs/Docs/final_output',   'value': {'documents': [Document(page_content='harrison worked at kensho')]}},  {'op': 'add',   'path': '/logs/Docs/end_time',   'value': '2023-10-19T17:50:13.713'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'H'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'arrison'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' worked'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' at'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ' Kens'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': 'ho'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': '.'}) ---------------------------------------- RunLogPatch({'op': 'add', 'path': '/streamed_output/-', 'value': ''}) ---------------------------------------- RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': {'output': 'Harrison worked at Kensho.'}})  Streaming the incremental RunState​  You can simply pass diff=False to get incremental values of RunState. You get more verbose output with more repetitive parts.  async for chunk in retrieval_chain.astream_log(     \"where did harrison work?\", include_names=[\"Docs\"], diff=False ):     print(\"-\" * 70)     print(chunk)  ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {},  'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': None,                    'final_output': None,                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': []}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison', ' worked']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison', ' worked', ' at']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['', 'H', 'arrison', ' worked', ' at', ' Kens', 'ho', '.']}) ---------------------------------------------------------------------- RunLog({'final_output': None,  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['',                      'H',                      'arrison',                      ' worked',                      ' at',                      ' Kens',                      'ho',                      '.',                      '']}) ---------------------------------------------------------------------- RunLog({'final_output': {'output': 'Harrison worked at Kensho.'},  'id': 'afe66178-d75f-4c2d-b348-b1d144239cd6',  'logs': {'Docs': {'end_time': '2023-10-19T17:52:15.738',                    'final_output': {'documents': [Document(page_content='harrison worked at kensho')]},                    'id': '88d51118-5756-4891-89c5-2f6a5e90cc26',                    'metadata': {},                    'name': 'Docs',                    'start_time': '2023-10-19T17:52:15.438',                    'streamed_output_str': [],                    'tags': ['map:key:context', 'FAISS'],                    'type': 'retriever'}},  'streamed_output': ['',                      'H',                      'arrison',                      ' worked',                      ' at',                      ' Kens',                      'ho',                      '.',                      '']})  Parallelism​  Let’s take a look at how LangChain Expression Language supports parallel requests. For example, when using a RunnableParallel (often written as a dictionary) it executes each element in parallel.  from langchain_core.runnables import RunnableParallel  chain1 = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\") | model chain2 = (     ChatPromptTemplate.from_template(\"write a short (2 line) poem about {topic}\")     | model ) combined = RunnableParallel(joke=chain1, poem=chain2)  %%time chain1.invoke({\"topic\": \"bears\"})  CPU times: user 54.3 ms, sys: 0 ns, total: 54.3 ms Wall time: 2.29 s  AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\")  %%time chain2.invoke({\"topic\": \"bears\"})  CPU times: user 7.8 ms, sys: 0 ns, total: 7.8 ms Wall time: 1.43 s  AIMessage(content=\"In wild embrace,\\nNature's strength roams with grace.\")  %%time combined.invoke({\"topic\": \"bears\"})  CPU times: user 167 ms, sys: 921 µs, total: 168 ms Wall time: 1.56 s  {'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),  'poem': AIMessage(content=\"Fierce and wild, nature's might,\\nBears roam the woods, shadows of the night.\")}  Parallelism on batches​  Parallelism can be combined with other runnables. Let’s try to use parallelism with batches.  %%time chain1.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])  CPU times: user 159 ms, sys: 3.66 ms, total: 163 ms Wall time: 1.34 s  [AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),  AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy don't cats play poker in the wild?\\n\\nBecause there are too many cheetahs!\")]  %%time chain2.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])  CPU times: user 165 ms, sys: 0 ns, total: 165 ms Wall time: 1.73 s  [AIMessage(content=\"Silent giants roam,\\nNature's strength, love's emblem shown.\"),  AIMessage(content='Whiskers aglow, paws tiptoe,\\nGraceful hunters, hearts aglow.')]  %%time combined.batch([{\"topic\": \"bears\"}, {\"topic\": \"cats\"}])  CPU times: user 507 ms, sys: 125 ms, total: 632 ms Wall time: 1.49 s  [{'joke': AIMessage(content=\"Why don't bears wear shoes?\\n\\nBecause they already have bear feet!\"),   'poem': AIMessage(content=\"Majestic bears roam,\\nNature's wild guardians of home.\")},  {'joke': AIMessage(content=\"Sure, here's a cat joke for you:\\n\\nWhy did the cat sit on the computer?\\n\\nBecause it wanted to keep an eye on the mouse!\"),   'poem': AIMessage(content='Whiskers twitch, eyes gleam,\\nGraceful creatures, feline dream.')}]  Previous Why use LCEL Next How to Input Schema Output Schema Stream Invoke Batch Async Stream Async Invoke Async Batch Async Stream Intermediate Steps Streaming JSONPatch chunks Streaming the incremental RunState Parallelism Parallelism on batches Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/expression_language/cookbook/",
    "title": "Cookbook | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Prompt + LLM RAG Multiple chains Querying a SQL DB Agents Code writing Routing by semantic similarity Adding memory Adding moderation Managing prompt size Using tools Modules Model I/O Retrieval Agents Chains More LangServe LangSmith LangGraph LangChain Expression LanguageCookbook Cookbook  Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.  📄️ Prompt + LLM  The most common and valuable composition is taking:  📄️ RAG  Let’s look at adding in a retrieval step to a prompt and LLM, which adds  📄️ Multiple chains  Runnables can easily be used to string together multiple Chains  📄️ Querying a SQL DB  We can replicate our SQLDatabaseChain with Runnables.  📄️ Agents  You can pass a Runnable into an agent.  📄️ Code writing  Example of how to use LCEL to write Python code.  📄️ Routing by semantic similarity  With LCEL you can easily add [custom routing  📄️ Adding memory  This shows how to add memory to an arbitrary chain. Right now, you can  📄️ Adding moderation  This shows how to add in moderation (or other safeguards) around your  📄️ Managing prompt size  Agents dynamically call tools. The results of those tool calls are added  📄️ Using tools  You can use any Tools with Runnables easily.  Previous Add message history (memory) Next Prompt + LLM Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/additional_resources/youtube",
    "title": "YouTube videos | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K YouTube videos  ⛓ icon marks a new addition [last update 2023-09-21]  Official LangChain YouTube channel​ Introduction to LangChain with Harrison Chase, creator of LangChain​ Building the Future with LLMs, LangChain, & Pinecone by Pinecone LangChain and Weaviate with Harrison Chase and Bob van Luijt - Weaviate Podcast #36 by Weaviate • Vector Database LangChain Demo + Q&A with Harrison Chase by Full Stack Deep Learning LangChain Agents: Build Personal Assistants For Your Data (Q&A with Harrison Chase and Mayo Oshin) by Chat with data Videos (sorted by views)​ Using ChatGPT with YOUR OWN Data. This is magical. (LangChain OpenAI API) by TechLead First look - ChatGPT + WolframAlpha (GPT-3.5 and Wolfram|Alpha via LangChain by James Weaver) by Dr Alan D. Thompson LangChain explained - The hottest new Python framework by AssemblyAI Chatbot with INFINITE MEMORY using OpenAI & Pinecone - GPT-3, Embeddings, ADA, Vector DB, Semantic by David Shapiro ~ AI LangChain for LLMs is... basically just an Ansible playbook by David Shapiro ~ AI Build your own LLM Apps with LangChain & GPT-Index by 1littlecoder BabyAGI - New System of Autonomous AI Agents with LangChain by 1littlecoder Run BabyAGI with Langchain Agents (with Python Code) by 1littlecoder How to Use Langchain With Zapier | Write and Send Email with GPT-3 | OpenAI API Tutorial by StarMorph AI Use Your Locally Stored Files To Get Response From GPT - OpenAI | Langchain | Python by Shweta Lodha Langchain JS | How to Use GPT-3, GPT-4 to Reference your own Data | OpenAI Embeddings Intro by StarMorph AI The easiest way to work with large language models | Learn LangChain in 10min by Sophia Yang 4 Autonomous AI Agents: “Westworld” simulation BabyAGI, AutoGPT, Camel, LangChain by Sophia Yang AI CAN SEARCH THE INTERNET? Langchain Agents + OpenAI ChatGPT by tylerwhatsgood Query Your Data with GPT-4 | Embeddings, Vector Databases | Langchain JS Knowledgebase by StarMorph AI Weaviate + LangChain for LLM apps presented by Erika Cardenas by Weaviate • Vector Database Langchain Overview — How to Use Langchain & ChatGPT by Python In Office Langchain Overview - How to Use Langchain & ChatGPT by Python In Office LangChain Tutorials by Edrick: LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF LangChain 101: The Complete Beginner's Guide Custom langchain Agent & Tools with memory. Turn any Python function into langchain tool with Gpt 3 by echohive Building AI LLM Apps with LangChain (and more?) - LIVE STREAM by Nicholas Renotte ChatGPT with any YouTube video using langchain and chromadb by echohive How to Talk to a PDF using LangChain and ChatGPT by Automata Learning Lab Langchain Document Loaders Part 1: Unstructured Files by Merk LangChain - Prompt Templates (what all the best prompt engineers use) by Nick Daigler LangChain. Crear aplicaciones Python impulsadas por GPT by Jesús Conde Easiest Way to Use GPT In Your Products | LangChain Basics Tutorial by Rachel Woods BabyAGI + GPT-4 Langchain Agent with Internet Access by tylerwhatsgood Learning LLM Agents. How does it actually work? LangChain, AutoGPT & OpenAI by Arnoldas Kemeklis Get Started with LangChain in Node.js by Developers Digest LangChain + OpenAI tutorial: Building a Q&A system w/ own text data by Samuel Chan Langchain + Zapier Agent by Merk Connecting the Internet with ChatGPT (LLMs) using Langchain And Answers Your Questions by Kamalraj M M Build More Powerful LLM Applications for Business’s with LangChain (Beginners Guide) by No Code Blackbox LangFlow LLM Agent Demo for 🦜🔗LangChain by Cobus Greyling Chatbot Factory: Streamline Python Chatbot Creation with LLMs and Langchain by Finxter LangChain Tutorial - ChatGPT mit eigenen Daten by Coding Crashkurse Chat with a CSV | LangChain Agents Tutorial (Beginners) by GoDataProf Introdução ao Langchain - #Cortes - Live DataHackers by Prof. João Gabriel Lima LangChain: Level up ChatGPT !? | LangChain Tutorial Part 1 by Code Affinity KI schreibt krasses Youtube Skript 😲😳 | LangChain Tutorial Deutsch by SimpleKI Chat with Audio: Langchain, Chroma DB, OpenAI, and Assembly AI by AI Anytime QA over documents with Auto vector index selection with Langchain router chains by echohive Build your own custom LLM application with Bubble.io & Langchain (No Code & Beginner friendly) by No Code Blackbox Simple App to Question Your Docs: Leveraging Streamlit, Hugging Face Spaces, LangChain, and Claude! by Chris Alexiuk LANGCHAIN AI- ConstitutionalChainAI + Databutton AI ASSISTANT Web App by Avra LANGCHAIN AI AUTONOMOUS AGENT WEB APP - 👶 BABY AGI 🤖 with EMAIL AUTOMATION using DATABUTTON by Avra The Future of Data Analysis: Using A.I. Models in Data Analysis (LangChain) by Absent Data Memory in LangChain | Deep dive (python) by Eden Marco 9 LangChain UseCases | Beginner's Guide | 2023 by Data Science Basics Use Large Language Models in Jupyter Notebook | LangChain | Agents & Indexes by Abhinaw Tiwari How to Talk to Your Langchain Agent | 11 Labs + Whisper by VRSEN LangChain Deep Dive: 5 FUN AI App Ideas To Build Quickly and Easily by James NoCode LangChain 101: Models by Mckay Wrigley LangChain with JavaScript Tutorial #1 | Setup & Using LLMs by Leon van Zyl LangChain Overview & Tutorial for Beginners: Build Powerful AI Apps Quickly & Easily (ZERO CODE) by James NoCode LangChain In Action: Real-World Use Case With Step-by-Step Tutorial by Rabbitmetrics Summarizing and Querying Multiple Papers with LangChain by Automata Learning Lab Using Langchain (and Replit) through Tana, ask Google/Wikipedia/Wolfram Alpha to fill out a table by Stian Håklev Langchain PDF App (GUI) | Create a ChatGPT For Your PDF in Python by Alejandro AO - Software & Ai Auto-GPT with LangChain 🔥 | Create Your Own Personal AI Assistant by Data Science Basics Create Your OWN Slack AI Assistant with Python & LangChain by Dave Ebbelaar How to Create LOCAL Chatbots with GPT4All and LangChain [Full Guide] by Liam Ottley Build a Multilingual PDF Search App with LangChain, Cohere and Bubble by Menlo Park Lab Building a LangChain Agent (code-free!) Using Bubble and Flowise by Menlo Park Lab Build a LangChain-based Semantic PDF Search App with No-Code Tools Bubble and Flowise by Menlo Park Lab LangChain Memory Tutorial | Building a ChatGPT Clone in Python by Alejandro AO - Software & Ai ChatGPT For Your DATA | Chat with Multiple Documents Using LangChain by Data Science Basics Llama Index: Chat with Documentation using URL Loader by Merk Using OpenAI, LangChain, and Gradio to Build Custom GenAI Applications by David Hundley LangChain, Chroma DB, OpenAI Beginner Guide | ChatGPT with your PDF Build AI chatbot with custom knowledge base using OpenAI API and GPT Index by Irina Nik Build Your Own Auto-GPT Apps with LangChain (Python Tutorial) by Dave Ebbelaar Chat with Multiple PDFs | LangChain App Tutorial in Python (Free LLMs and Embeddings) by Alejandro AO - Software & Ai Chat with a CSV | LangChain Agents Tutorial (Beginners) by Alejandro AO - Software & Ai Create Your Own ChatGPT with PDF Data in 5 Minutes (LangChain Tutorial) by Liam Ottley Build a Custom Chatbot with OpenAI: GPT-Index & LangChain | Step-by-Step Tutorial by Fabrikod Flowise is an open-source no-code UI visual tool to build 🦜🔗LangChain applications by Cobus Greyling LangChain & GPT 4 For Data Analysis: The Pandas Dataframe Agent by Rabbitmetrics GirlfriendGPT - AI girlfriend with LangChain by Toolfinder AI How to build with Langchain 10x easier | ⛓️ LangFlow & Flowise by AI Jason Getting Started With LangChain In 20 Minutes- Build Celebrity Search Application by Krish Naik ⛓ Vector Embeddings Tutorial – Code Your Own AI Assistant with GPT-4 API + LangChain + NLP by FreeCodeCamp.org ⛓ Fully LOCAL Llama 2 Q&A with LangChain by 1littlecoder ⛓ Fully LOCAL Llama 2 Langchain on CPU by 1littlecoder ⛓ Build LangChain Audio Apps with Python in 5 Minutes by AssemblyAI ⛓ Voiceflow & Flowise: Want to Beat Competition? New Tutorial with Real AI Chatbot by AI SIMP ⛓ THIS Is How You Build Production-Ready AI Apps (LangSmith Tutorial) by Dave Ebbelaar ⛓ Build POWERFUL LLM Bots EASILY with Your Own Data - Embedchain - Langchain 2.0? (Tutorial) by WorldofAI ⛓ Code Llama powered Gradio App for Coding: Runs on CPU by AI Anytime ⛓ LangChain Complete Course in One Video | Develop LangChain (AI) Based Solutions for Your Business by UBprogrammer ⛓ How to Run LLaMA Locally on CPU or GPU | Python & Langchain & CTransformers Guide by Code With Prince ⛓ PyData Heidelberg #11 - TimeSeries Forecasting & LLM Langchain by PyData ⛓ Prompt Engineering in Web Development | Using LangChain and Templates with OpenAI by Akamai Developer ⛓ Retrieval-Augmented Generation (RAG) using LangChain and Pinecone - The RAG Special Episode by Generative AI and Data Science On AWS ⛓ LLAMA2 70b-chat Multiple Documents Chatbot with Langchain & Streamlit |All OPEN SOURCE|Replicate API by DataInsightEdge ⛓ Chatting with 44K Fashion Products: LangChain Opportunities and Pitfalls by Rabbitmetrics ⛓ Structured Data Extraction from ChatGPT with LangChain by MG ⛓ Chat with Multiple PDFs using Llama 2, Pinecone and LangChain (Free LLMs and Embeddings) by Muhammad Moin ⛓ Integrate Audio into LangChain.js apps in 5 Minutes by AssemblyAI ⛓ ChatGPT for your data with Local LLM by Jacob Jedryszek ⛓ Training Chatgpt with your personal data using langchain step by step in detail by NextGen Machines ⛓ Use ANY language in LangSmith with REST by Nerding I/O ⛓ How to Leverage the Full Potential of LLMs for Your Business with Langchain - Leon Ruddat by PyData ⛓ ChatCSV App: Chat with CSV files using LangChain and Llama 2 by Muhammad Moin Prompt Engineering and LangChain by Venelin Valkov​ Getting Started with LangChain: Load Custom Data, Run OpenAI Models, Embeddings and ChatGPT Loaders, Indexes & Vectorstores in LangChain: Question Answering on PDF files with ChatGPT LangChain Models: ChatGPT, Flan Alpaca, OpenAI Embeddings, Prompt Templates & Streaming LangChain Chains: Use ChatGPT to Build Conversational Agents, Summaries and Q&A on Text With LLMs Analyze Custom CSV Data with GPT-4 using Langchain Build ChatGPT Chatbots with LangChain Memory: Understanding and Implementing Memory in Conversations  ⛓ icon marks a new addition [last update 2023-09-21]  Official LangChain YouTube channel Introduction to LangChain with Harrison Chase, creator of LangChain Videos (sorted by views) Prompt Engineering and LangChain by Venelin Valkov Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/chatbots/",
    "title": "Chatbots | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesChatbots Chatbots  Open In Colab  Use case​  Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about.  Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information.  Overview​  The chat model interface is based around messages rather than raw text. Several components are important to consider for chat:  chat model: See here for a list of chat model integrations and here for documentation on the chat model interface in LangChain. You can use LLMs (see here) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. prompt template: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. memory: See here for in-depth documentation on memory types retriever (optional): See here for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. Quickstart​  Here’s a quick preview of how we can create chatbot interfaces. First let’s install some dependencies and set the required credentials:  %pip install --upgrade --quiet  langchain langchain-openai  # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()   With a plain chat model, we can get chat completions by passing one or more messages to the model.  The chat model will respond with a message.  from langchain.schema import HumanMessage, SystemMessage from langchain_openai import ChatOpenAI  chat = ChatOpenAI() chat(     [         HumanMessage(             content=\"Translate this sentence from English to French: I love programming.\"         )     ] )  AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)   And if we pass in a list of messages:  messages = [     SystemMessage(         content=\"You are a helpful assistant that translates English to French.\"     ),     HumanMessage(content=\"I love programming.\"), ] chat(messages)  AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)   We can then wrap our chat model in a ConversationChain, which has built-in memory for remembering past user inputs and model outputs.  from langchain.chains import ConversationChain  conversation = ConversationChain(llm=chat) conversation.run(\"Translate this sentence from English to French: I love programming.\")  'Je adore la programmation.'  conversation.run(\"Translate it to German.\")  'Ich liebe Programmieren.'  Memory​  As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is ConversationBufferMemory:  This memory allows for storing of messages in a buffer When called in a chain, it returns all of the messages it has stored  LangChain comes with many other types of memory, too. See here for in-depth documentation on memory types.  For now let’s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so:  from langchain.memory import ConversationBufferMemory  memory = ConversationBufferMemory() memory.chat_memory.add_user_message(\"hi!\") memory.chat_memory.add_ai_message(\"whats up?\")   And now we can load from our memory. The key method exposed by all Memory classes is load_memory_variables. This takes in any initial chain input and returns a list of memory variables which are added to the chain input.  Since this simple memory type doesn’t actually take into account the chain input when loading memory, we can pass in an empty input for now:  memory.load_memory_variables({})  {'history': 'Human: hi!\\nAI: whats up?'}   We can also keep a sliding window of the most recent k interactions using ConversationBufferWindowMemory.  from langchain.memory import ConversationBufferWindowMemory  memory = ConversationBufferWindowMemory(k=1) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"}) memory.load_memory_variables({})  {'history': 'Human: not much you\\nAI: not much'}   ConversationSummaryMemory is an extension of this theme.  It creates a summary of the conversation over time.  This memory is most useful for longer conversations where the full message history would consume many tokens.  from langchain.memory import ConversationSummaryMemory from langchain_openai import OpenAI  llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context(     {\"input\": \"im working on better docs for chatbots\"},     {\"output\": \"oh, that sounds like a lot of work\"}, ) memory.save_context(     {\"input\": \"yes, but it's worth the effort\"},     {\"output\": \"agreed, good docs are important!\"}, )  memory.load_memory_variables({})  {'history': '\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.'}   ConversationSummaryBufferMemory extends this a bit further:  It uses token length rather than number of interactions to determine when to flush interactions.  from langchain.memory import ConversationSummaryBufferMemory  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})  Conversation​  We can unpack what goes under the hood with ConversationChain.  We can specify our memory, ConversationSummaryMemory and we can specify the prompt.  from langchain.chains import LLMChain from langchain.prompts import (     ChatPromptTemplate,     HumanMessagePromptTemplate,     MessagesPlaceholder,     SystemMessagePromptTemplate, )  # LLM llm = ChatOpenAI()  # Prompt prompt = ChatPromptTemplate(     messages=[         SystemMessagePromptTemplate.from_template(             \"You are a nice chatbot having a conversation with a human.\"         ),         # The `variable_name` here is what must align with memory         MessagesPlaceholder(variable_name=\"chat_history\"),         HumanMessagePromptTemplate.from_template(\"{question}\"),     ] )  # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)  # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({\"question\": \"hi\"})    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi  > Finished chain.  {'question': 'hi',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)],  'text': 'Hello! How can I assist you today?'}  conversation(     {\"question\": \"Translate this sentence from English to French: I love programming.\"} )    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming.  > Finished chain.  {'question': 'Translate this sentence from English to French: I love programming.',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),   HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),   AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False)],  'text': 'Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"'}  conversation({\"question\": \"Now translate the sentence to German.\"})    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of \"I love programming\" from English to French is \"J'adore programmer.\" Human: Now translate the sentence to German.  > Finished chain.  {'question': 'Now translate the sentence to German.',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),   HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),   AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False),   HumanMessage(content='Now translate the sentence to German.', additional_kwargs={}, example=False),   AIMessage(content='Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"', additional_kwargs={}, example=False)],  'text': 'Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"'}   We can see the chat history preserved in the prompt using the LangSmith trace.  Chat Retrieval​  Now, suppose we want to chat with documents or some other source of knowledge.  This is popular use case, combining chat with document retrieval.  It allows us to chat with specific information that the model was not trained on.  %pip install --upgrade --quiet  tiktoken chromadb   Load a blog post.  from langchain_community.document_loaders import WebBaseLoader  loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\") data = loader.load()   Split and store this in a vector.  from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data)  from langchain_community.vectorstores import Chroma from langchain_openai import OpenAIEmbeddings  vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())   Create our memory, as before, but’s let’s use ConversationSummaryMemory.  memory = ConversationSummaryMemory(     llm=llm, memory_key=\"chat_history\", return_messages=True )  from langchain.chains import ConversationalRetrievalChain from langchain_openai import ChatOpenAI  llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)  qa(\"How do agents use Task decomposition?\")  {'question': 'How do agents use Task decomposition?',  'chat_history': [SystemMessage(content='', additional_kwargs={})],  'answer': 'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to \"Write a story outline.\" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.'}  qa(\"What are the various ways to implement memory to support it?\")  {'question': 'What are the various ways to implement memory to support it?',  'chat_history': [SystemMessage(content='The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.', additional_kwargs={})],  'answer': 'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.'}   Again, we can use the LangSmith trace to explore the prompt structure.  Going deeper​ Agents, such as the conversational retrieval agent, can be used for retrieval when necessary while also holding a conversation. Previous Interacting with APIs Next Extraction Use case Overview Quickstart Memory Conversation Chat Retrieval Going deeper Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/contributing",
    "title": "Welcome Contributors | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Welcome Contributors Contribute Code Testing Contribute Documentation Contribute Integrations FAQ Welcome Contributors Welcome Contributors  Hi there! Thank you for even being interested in contributing to LangChain. As an open-source project in a rapidly developing field, we are extremely open to contributions, whether they involve new features, improved infrastructure, better documentation, or bug fixes.  🗺️ Guidelines​ 👩‍💻 Ways to contribute​  There are many ways to contribute to LangChain. Here are some common ways people contribute:  Documentation: Help improve our docs, including this one! Code: Help us write code, fix bugs, or improve our infrastructure. Integrations: Help us integrate with your favorite vendors and tools. 🚩GitHub Issues​  Our issues page is kept up to date with bugs, improvements, and feature requests.  There is a taxonomy of labels to help with sorting and discovery of issues of interest. Please use these to help organize issues.  If you start working on an issue, please assign it to yourself.  If you are adding an issue, please try to keep it focused on a single, modular bug/improvement/feature. If two issues are related, or blocking, please link them rather than combining them.  We will try to keep these issues as up-to-date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please let us know.  🙋Getting Help​  Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please contact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is smooth for future contributors.  In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help - we do not want these to get in the way of getting good code into the codebase.  🌟 Recognition  If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)! If you have a Twitter account you would like us to mention, please let us know in the PR or through another means.  Next Contribute Code 🗺️ Guidelines 👩‍💻 Ways to contribute 🚩GitHub Issues 🙋Getting Help Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/",
    "title": "Retrieval | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders Text Splitters Text embedding models Vector stores Retrievers Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrieval Retrieval  Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.  LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.  Document loaders  Document loaders load documents from many different sources. LangChain provides over 100 different document loaders as well as integrations with other major providers in the space, like AirByte and Unstructured. LangChain provides integrations to load all types of documents (HTML, PDF, code) from all types of locations (private S3 buckets, public websites).  Text Splitting  A key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps to prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several transformation algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).  Text embedding models  Another key part of retrieval is creating embeddings for documents. Embeddings capture the semantic meaning of the text, allowing you to quickly and efficiently find other pieces of a text that are similar. LangChain provides integrations with over 25 different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain provides a standard interface, allowing you to easily swap between models.  Vector stores  With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with over 50 different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.  Retrievers  Once the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. LangChain supports basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include:  Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context. Self Query Retriever: User questions often contain a reference to something that isn't just semantic but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query. Ensemble Retriever: Sometimes you may want to retrieve documents from multiple different sources, or using multiple different algorithms. The ensemble retriever allows you to easily do this. And more!  Indexing  The LangChain Indexing API syncs your data from any source into a vector store, helping you:  Avoid writing duplicated content into the vector store Avoid re-writing unchanged content Avoid re-computing embeddings over unchanged content  All of which should save you time and money, as well as improve your vector search results.  Previous YAML parser Next Document loaders Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/callbacks",
    "title": "Callbacks | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Argilla Confident Context Infino Label Studio LLMonitor PromptLayer SageMaker Tracking Streamlit Trubrics Chat loaders Adapters Stores ComponentsCallbacks Callbacks 📄️ Argilla  Argilla is an open-source data curation  📄️ Confident  DeepEval package for unit testing LLMs.  📄️ Context  Context provides user analytics for LLM-powered  📄️ Infino  Infino is a scalable telemetry  📄️ Label Studio  Label Studio is an  📄️ LLMonitor  LLMonitor is an open-source observability platform that provides cost and usage analytics, user tracking, tracing and evaluation tools.  📄️ PromptLayer  PromptLayer is a platform  📄️ SageMaker Tracking  Amazon SageMaker is a fully  📄️ Streamlit  Streamlit is a faster way to build and share data apps.  📄️ Trubrics  Trubrics is an LLM user analytics platform  Previous Zep Next Argilla Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/activeloop_deeplake",
    "title": "Activeloop Deep Lake | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Astra DB Atlas AwaDB AZLyrics BagelDB Banana Baseten Beam Beautiful Soup BiliBili NIBittensor Blackboard Brave Search CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse Cloudflare CnosDB Cohere College Confidential Comet Confident AI Confluence Context C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot DingoDB Discord DocArray Doctran Docugami DSPy DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Serper - Google Search API GooseAI GPT4All Gradient Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres HTML to text iFixit IMSDb Infinity Infino Jaguar Javelin AI Gateway Jina Johnsnowlabs Konko Label Studio LanceDB LangChain Decorators ✨ Lantern Llama.cpp LLMonitor Log10 Marqo MediaWikiDump Meilisearch Metal Milvus Minimax MLflow Deployments for LLMs MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck Motörhead MyScale Neo4j NLPCloud Notion DB Nuclia NVIDIA Obsidian Ollama OpenLLM OpenSearch OpenWeatherMap Outline Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant RAGatouille Ray Serve Rebuff Reddit Redis Replicate Roam Robocorp Rockset Runhouse RWKV-4 Salute Devices SearchApi SearxNG Search API SemaDB SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly Stack Exchange StarRocks StochasticAI Streamlit Stripe Supabase (Postgres) Nebula Tair Telegram Tencent TensorFlow Datasets Tigris 2Markdown Trello Trubrics TruLens Twitter Typesense Unstructured Upstash Redis USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yandex Yeager.ai YouTube Zep Zilliz Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersMoreActiveloop Deep Lake Activeloop Deep Lake  This page covers how to use the Deep Lake ecosystem within LangChain.  Why Deep Lake?​ More than just a (multi-modal) vector store. You can later use the dataset to fine-tune your own LLM models. Not only stores embeddings, but also the original data with automatic version control. Truly serverless. Doesn't require another service and can be used with major cloud providers (AWS S3, GCS, etc.)  Activeloop Deep Lake supports SelfQuery Retrieval: Activeloop Deep Lake Self Query Retrieval  More Resources​ Ultimate Guide to LangChain & Deep Lake: Build ChatGPT to Answer Questions on Your Financial Data Twitter the-algorithm codebase analysis with Deep Lake Here is whitepaper and academic paper for Deep Lake Here is a set of additional resources available for review: Deep Lake, Get started and Tutorials Installation and Setup​ Install the Python package with pip install deeplake Wrappers​ VectorStore​  There exists a wrapper around Deep Lake, a data lake for Deep Learning applications, allowing you to use it as a vector store (for now), whether for semantic search or example selection.  To import this vectorstore:  from langchain_community.vectorstores import DeepLake   For a more detailed walkthrough of the Deep Lake wrapper, see this notebook  Previous OpenAI Next AI21 Labs Why Deep Lake? More Resources Installation and Setup Wrappers VectorStore Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/adapters",
    "title": "Adapters | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters OpenAI Adapter(Old) OpenAI Adapter Stores ComponentsAdapters Adapters 📄️ OpenAI Adapter(Old)  Please ensure OpenAI library is less than 1.0.0; otherwise, refer to  📄️ OpenAI Adapter  Please ensure OpenAI library is version 1.0.0 or higher; otherwise,  Previous WhatsApp Next OpenAI Adapter(Old) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/stores/",
    "title": "Stores | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores In Memory Local Filesystem Astra DB Redis Upstash Redis ComponentsStores Stores  In many different applications, having some sort of key-value storage is helpful. In this section, we will look at a few different ways to store key-value pairs using implementations of the ByteStore interface.  Features (natively supported)​  All ByteStores support the following functions, which are used for modifying multiple key-value pairs at once:  mget(key: Sequence[str]) -> List[Optional[bytes]]: get the contents of multiple keys, returning None if the key does not exist mset(key_value_pairs: Sequence[Tuple[str, bytes]]) -> None: set the contents of multiple keys mdelete(key: Sequence[str]) -> None: delete multiple keys yield_keys(prefix: Optional[str] = None) -> Iterator[str]: yield all keys in the store, optionally filtering by a prefix How to pick one​  ByteStores are designed to be interchangeable. By default, most dependent integrations use the InMemoryByteStore, which is a simple in-memory key-value store.  However, if you start having other requirements, like massive scalability or persistence, you can swap out the ByteStore implementation with one of the other ones documented in this section.  Previous OpenAI Adapter Next Stores Features (natively supported) How to pick one Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/huggingface",
    "title": "Hugging Face | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersHugging Face Hugging Face  All functionality related to the Hugging Face Platform.  LLMs​ Hugging Face Hub​  The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning.  To use, we should have the huggingface_hub python package installed.  pip install huggingface_hub   See a usage example.  from langchain_community.llms import HuggingFaceHub  Hugging Face Local Pipelines​  Hugging Face models can be run locally through the HuggingFacePipeline class.  We need to install transformers python package.  pip install transformers   See a usage example.  from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline  Hugging Face TextGen Inference​  Text Generation Inference is a Rust, Python and gRPC server for text generation inference. Used in production at HuggingFace to power LLMs api-inference widgets.  We need to install text_generation python package.  pip install text_generation   See a usage example.  from langchain_community.llms import HuggingFaceTextGenInference  Document Loaders​ Hugging Face dataset​  Hugging Face Hub is home to over 75,000 datasets in more than 100 languages that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. They used for a diverse range of tasks such as translation, automatic speech recognition, and image classification.  We need to install datasets python package.  pip install datasets   See a usage example.  from langchain_community.document_loaders.hugging_face_dataset import HuggingFaceDatasetLoader  Embedding Models​ Hugging Face Hub​  The Hugging Face Hub is a platform with over 350k models, 75k datasets, and 150k demo apps (Spaces), all open source and publicly available, in an online platform where people can easily collaborate and build ML together. The Hub works as a central place where anyone can explore, experiment, collaborate, and build technology with Machine Learning.  We need to install the sentence_transformers python package.  pip install sentence_transformers  HuggingFaceEmbeddings​  See a usage example.  from langchain_community.embeddings import HuggingFaceEmbeddings  HuggingFaceInstructEmbeddings​  See a usage example.  from langchain_community.embeddings import HuggingFaceInstructEmbeddings  HuggingFaceBgeEmbeddings​  BGE models on the HuggingFace are the best open-source embedding models. BGE model is created by the Beijing Academy of Artificial Intelligence (BAAI). BAAI is a private non-profit organization engaged in AI research and development.  See a usage example.  from langchain_community.embeddings import HuggingFaceBgeEmbeddings  Tools​ Hugging Face Hub Tools​  Hugging Face Tools support text I/O and are loaded using the load_huggingface_tool function.  We need to install several python packages.  pip install transformers huggingface_hub   See a usage example.  from langchain.agents import load_huggingface_tool  Previous Google Next Microsoft LLMs Hugging Face Hub Hugging Face Local Pipelines Hugging Face TextGen Inference Document Loaders Hugging Face dataset Embedding Models Hugging Face Hub Tools Hugging Face Hub Tools Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/vectorstores",
    "title": "Vector stores | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Activeloop Deep Lake Alibaba Cloud OpenSearch AnalyticDB Annoy Astra DB Atlas AwaDB Azure Cosmos DB Azure AI Search BagelDB Baidu Cloud ElasticSearch VectorSearch BigQuery Vector Search Chroma Clarifai ClickHouse DashVector Databricks Vector Search DingoDB DocArray HnswSearch DocArray InMemorySearch Elasticsearch Epsilla Faiss Faiss (Async) Google Vertex AI Vector Search Hippo Hologres Jaguar Vector Database LanceDB Lantern LLMRails Marqo Meilisearch Milvus Momento Vector Index (MVI) MongoDB Atlas MyScale Neo4j Vector Index NucliaDB OpenSearch Postgres Embedding PGVecto.rs PGVector Pinecone Qdrant Redis Rockset ScaNN SemaDB SingleStoreDB scikit-learn SQLite-VSS StarRocks Supabase (Postgres) SurrealDB Tair Tencent Cloud VectorDB Tigris TileDB Timescale Vector (Postgres) Typesense USearch Vald Vearch Vectara Vespa viking DB Weaviate Xata Yellowbrick Zep Zilliz Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsVector stores Vector stores 📄️ Activeloop Deep Lake  Activeloop Deep Lake as a Multi-Modal  📄️ Alibaba Cloud OpenSearch  [Alibaba Cloud  📄️ AnalyticDB  [AnalyticDB for  📄️ Annoy  Annoy  📄️ Astra DB  This page provides a quickstart for using [Astra  📄️ Atlas  Atlas is a platform by Nomic made  📄️ AwaDB  AwaDB is an AI Native database for  📄️ Azure Cosmos DB  [Azure Cosmos DB for MongoDB  📄️ Azure AI Search  [Azure AI  📄️ BagelDB  BagelDB (Open Vector Database for AI), is  📄️ Baidu Cloud ElasticSearch VectorSearch  [Baidu Cloud  📄️ BigQuery Vector Search  BigQueryVectorSearch: BigQuery vector search lets you use  📄️ Chroma  Chroma is a AI-native  📄️ Clarifai  Clarifai is an AI Platform that provides  📄️ ClickHouse  ClickHouse is the fastest and most resource  📄️ DashVector  DashVector is  📄️ Databricks Vector Search  Databricks Vector Search is a serverless similarity search engine that  📄️ DingoDB  DingoDB is a distributed  📄️ DocArray HnswSearch  DocArrayHnswSearch  📄️ DocArray InMemorySearch  DocArrayInMemorySearch  📄️ Elasticsearch  Elasticsearch is a  📄️ Epsilla  Epsilla is an open-source vector database  📄️ Faiss  [Facebook AI Similarity Search  📄️ Faiss (Async)  [Facebook AI Similarity Search  📄️ Google Vertex AI Vector Search  This notebook shows how to use functionality related to the  📄️ Hippo  Transwarp Hippo is an  📄️ Hologres  Hologres  📄️ Jaguar Vector Database  1. It is a distributed vector database  📄️ LanceDB  LanceDB is an open-source database for  📄️ Lantern  Lantern is an open-source  📄️ LLMRails  LLMRails is a API platform for building  📄️ Marqo  This notebook shows how to use functionality related to the Marqo  📄️ Meilisearch  Meilisearch is an open-source,  📄️ Milvus  Milvus is a database that  📄️ Momento Vector Index (MVI)  MVI: the most productive, easiest to use,  📄️ MongoDB Atlas  MongoDB Atlas is a  📄️ MyScale  MyScale is a cloud-based  📄️ Neo4j Vector Index  Neo4j is an open-source graph database with  📄️ NucliaDB  You can use a local NucliaDB instance or use [Nuclia  📄️ OpenSearch  OpenSearch is a scalable, flexible, and  📄️ Postgres Embedding  Postgres Embedding is  📄️ PGVecto.rs  This notebook shows how to use functionality related to the Postgres  📄️ PGVector  PGVector is an open-source  📄️ Pinecone  Pinecone is a vector  📄️ Qdrant  Qdrant (read: quadrant ) is a  📄️ Redis  Redis vector database introduction and langchain integration guide.  📄️ Rockset  Rockset is a real-time search and analytics  📄️ ScaNN  ScaNN (Scalable Nearest Neighbors) is a method for efficient vector  📄️ SemaDB  SemaDB from  📄️ SingleStoreDB  SingleStoreDB is a high-performance  📄️ scikit-learn  scikit-learn is an open-source  📄️ SQLite-VSS  SQLite-VSS is an SQLite  📄️ StarRocks  StarRocks is a High-Performance  📄️ Supabase (Postgres)  Supabase is an open-source Firebase  📄️ SurrealDB  SurrealDB is an end-to-end cloud-native  📄️ Tair  Tair  📄️ Tencent Cloud VectorDB  [Tencent Cloud  📄️ Tigris  Tigris is an open-source Serverless NoSQL  📄️ TileDB  TileDB is a powerful engine  📄️ Timescale Vector (Postgres)  [Timescale  📄️ Typesense  Typesense is an open-source, in-memory search  📄️ USearch  USearch is a Smaller & Faster  📄️ Vald  Vald is a highly scalable distributed  📄️ Vearch  Vearch is the vector search  📄️ Vectara  Vectara is the trusted GenAI platform that  📄️ Vespa  Vespa is a fully featured search engine and  📄️ viking DB  viking DB is a  📄️ Weaviate  Weaviate is an open-source vector database. It  📄️ Xata  Xata is a serverless data platform, based on  📄️ Yellowbrick  Yellowbrick is an  📄️ Zep  Zep is an open-source platform for LLM  📄️ Zilliz  Zilliz Cloud is a fully managed  Previous YandexGPT Next Activeloop Deep Lake Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/openai",
    "title": "OpenAI | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersOpenAI OpenAI  All functionality related to OpenAI  OpenAI is American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft.  The OpenAI API is powered by a diverse set of models with different capabilities and price points.  ChatGPT is the Artificial Intelligence (AI) chatbot developed by OpenAI.  Installation and Setup​  Install the integration package with  pip install langchain-openai   Get an OpenAI api key and set it as an environment variable (OPENAI_API_KEY)  LLM​  See a usage example.  from langchain_openai import OpenAI   If you are using a model hosted on Azure, you should use different wrapper for that:  from langchain_openai import AzureOpenAI   For a more detailed walkthrough of the Azure wrapper, see here  Chat model​  See a usage example.  from langchain_openai import ChatOpenAI   If you are using a model hosted on Azure, you should use different wrapper for that:  from langchain_openai import AzureChatOpenAI   For a more detailed walkthrough of the Azure wrapper, see here  Text Embedding Model​  See a usage example  from langchain_openai import OpenAIEmbeddings  Tokenizer​  There are several places you can use the tiktoken tokenizer. By default, it is used to count tokens for OpenAI LLMs.  You can also use it to count tokens when splitting documents with  from langchain.text_splitter import CharacterTextSplitter CharacterTextSplitter.from_tiktoken_encoder(...)   For a more detailed walkthrough of this, see this notebook  Document Loader​  See a usage example.  from langchain_community.document_loaders.chatgpt import ChatGPTLoader  Retriever​  See a usage example.  from langchain.retrievers import ChatGPTPluginRetriever  Chain​  See a usage example.  from langchain.chains import OpenAIModerationChain  Adapter​  See a usage example.  from langchain.adapters import openai as lc_openai  Tools​ Dall-E Image Generator​  OpenAI Dall-E are text-to-image models developed by OpenAI using deep learning methodologies to generate digital images from natural language descriptions, called \"prompts\".  See a usage example.  from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper  Previous Microsoft Next Activeloop Deep Lake Installation and Setup LLM Chat model Text Embedding Model Tokenizer Document Loader Retriever Chain Adapter Tools Dall-E Image Generator Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/chat_loaders",
    "title": "Chat loaders | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Discord Facebook Messenger GMail iMessage LangSmith Chat Datasets LangSmith LLM Runs Slack Telegram Twitter (via Apify) WeChat WhatsApp Adapters Stores ComponentsChat loaders Chat loaders 📄️ Discord  This notebook shows how to create your own chat loader that works on  📄️ Facebook Messenger  This notebook shows how to load data from Facebook in a format you can  📄️ GMail  This loader goes over how to load data from GMail. There are many ways  📄️ iMessage  This notebook shows how to use the iMessage chat loader. This class  📄️ LangSmith Chat Datasets  This notebook demonstrates an easy way to load a LangSmith chat dataset  📄️ LangSmith LLM Runs  This notebook demonstrates how to directly load data from LangSmith’s  📄️ Slack  This notebook shows how to use the Slack chat loader. This class helps  📄️ Telegram  This notebook shows how to use the Telegram chat loader. This class  📄️ Twitter (via Apify)  This notebook shows how to load chat messages from Twitter to fine-tune  📄️ WeChat  There is not yet a straightforward way to export personal WeChat  📄️ WhatsApp  This notebook shows how to use the WhatsApp chat loader. This class  Previous Trubrics Next Discord Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/toolkits",
    "title": "Agents and toolkits | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits AINetwork Airbyte Question Answering Amadeus Azure Cognitive Services ClickUp CSV Document Comparison Github Gitlab Gmail Jira JSON MultiOn NASA Office365 OpenAPI Natural Language APIs Pandas Dataframe PlayWright Browser PowerBI Dataset Python Robocorp Slack Spark Dataframe Spark SQL SQL Database Steam Game Recommendation & Game Details Xorbits Memory Callbacks Chat loaders Adapters Stores ComponentsAgents and toolkits Agents and toolkits 📄️ AINetwork  AI Network is a layer 1  📄️ Airbyte Question Answering  This notebook shows how to do question answering over structured data,  📄️ Amadeus  This notebook walks you through connecting LangChain to the Amadeus  📄️ Azure Cognitive Services  This toolkit is used to interact with the Azure Cognitive Services API  📄️ ClickUp  ClickUp is an all-in-one productivity platform  📄️ CSV  This notebook shows how to use agents to interact with data in CSV  📄️ Document Comparison  This notebook shows how to use an agent to compare two documents.  📄️ Github  The Github toolkit contains tools that enable an LLM agent to interact  📄️ Gitlab  The Gitlab toolkit contains tools that enable an LLM agent to interact  📄️ Gmail  This notebook walks through connecting a LangChain email to the  📄️ Jira  This notebook goes over how to use the Jira toolkit.  📄️ JSON  This notebook showcases an agent interacting with large JSON/dict  📄️ MultiOn  This notebook walks you through connecting LangChain to the MultiOn  📄️ NASA  This notebook shows how to use agents to interact with the NASA toolkit.  📄️ Office365  Microsoft 365 is a product family of  📄️ OpenAPI  We can construct agents to consume arbitrary APIs, here APIs conformant  📄️ Natural Language APIs  Natural Language API Toolkits (NLAToolkits) permit LangChain Agents  📄️ Pandas Dataframe  This notebook shows how to use agents to interact with a  📄️ PlayWright Browser  This toolkit is used to interact with the browser. While other tools  📄️ PowerBI Dataset  This notebook showcases an agent interacting with a Power BI Dataset.  📄️ Python  This notebook showcases an agent designed to write and execute Python  📄️ Robocorp  This notebook covers how to get started with [Robocorp Action  📄️ Slack  This notebook walks through connecting LangChain to your Slack  📄️ Spark Dataframe  This notebook shows how to use agents to interact with a  📄️ Spark SQL  This notebook shows how to use agents to interact with Spark SQL.  📄️ SQL Database  This notebook showcases an agent designed to interact with a SQL  📄️ Steam Game Recommendation & Game Details  Steam (Wikipedia)) is  📄️ Xorbits  This notebook shows how to use agents to interact with [Xorbits  Previous Zapier Natural Language Actions Next AINetwork Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/google",
    "title": "Google | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersGoogle Google  All functionality related to Google Cloud Platform and other Google products.  Chat models​ Google AI​  Access GoogleAI Gemini models such as gemini-pro and gemini-pro-vision through the ChatGoogleGenerativeAI class.  pip install -U langchain-google-genai   Configure your API key.  export GOOGLE_API_KEY=your-api-key  from langchain_google_genai import ChatGoogleGenerativeAI  llm = ChatGoogleGenerativeAI(model=\"gemini-pro\") llm.invoke(\"Sing a ballad of LangChain.\")   Gemini vision model supports image inputs when providing a single chat message. Example:  from langchain_core.messages import HumanMessage from langchain_google_genai import ChatGoogleGenerativeAI  llm = ChatGoogleGenerativeAI(model=\"gemini-pro-vision\") # example message = HumanMessage(     content=[         {             \"type\": \"text\",             \"text\": \"What's in this image?\",         },  # You can optionally provide text parts         {\"type\": \"image_url\", \"image_url\": \"https://picsum.photos/seed/picsum/200/300\"},     ] ) llm.invoke([message])   The value of image_url can be any of the following:  A public image URL A gcs file (e.g., \"gcs://path/to/file.png\") A local file path A base64 encoded image (e.g., data:image/png;base64,abcd124) A PIL image Vertex AI​  Access PaLM chat models like chat-bison and codechat-bison via Google Cloud.  We need to install langchain-google-vertexai python package.  pip install langchain-google-vertexai   See a usage example.  from langchain_google_vertexai import ChatVertexAI  Document Loaders​ Google BigQuery​  Google BigQuery is a serverless and cost-effective enterprise data warehouse that works across clouds and scales with your data. BigQuery is a part of the Google Cloud Platform.  We need to install google-cloud-bigquery python package.  pip install google-cloud-bigquery   See a usage example.  from langchain_community.document_loaders import BigQueryLoader  LLMs​ Vertex AI​  Access to Gemini and PaLM LLMs (like text-bison and code-bison) via Google Vertex AI.  We need to install langchain-google-vertexai python package.  pip install langchain-google-vertexai   See a usage example.  from langchain_google_vertexai import VertexAI  Model Garden​  Access PaLM and hundreds of OSS models via Vertex AI Model Garden.  We need to install langchain-google-vertexai python package.  pip install langchain-google-vertexai   See a usage example.  from langchain_google_vertexai import VertexAIModelGarden  Google Cloud Storage​  Google Cloud Storage is a managed service for storing unstructured data.  We need to install google-cloud-storage python package.  pip install google-cloud-storage   There are two loaders for the Google Cloud Storage: the Directory and the File loaders.  See a usage example.  from langchain_community.document_loaders import GCSDirectoryLoader   See a usage example.  from langchain_community.document_loaders import GCSFileLoader  Google Drive​  Google Drive is a file storage and synchronization service developed by Google.  Currently, only Google Docs are supported.  We need to install several python packages.  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib   See a usage example and authorization instructions.  from langchain_community.document_loaders import GoogleDriveLoader  Speech-to-Text​  Google Cloud Speech-to-Text is an audio transcription API powered by Google's speech recognition models.  This document loader transcribes audio files and outputs the text results as Documents.  First, we need to install the python package.  pip install google-cloud-speech   See a usage example and authorization instructions.  from langchain_community.document_loaders import GoogleSpeechToTextLoader  Vector Stores​ Google Vertex AI Vector Search​  Google Vertex AI Vector Search, formerly known as Vertex AI Matching Engine, provides the industry's leading high-scale low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.  We need to install several python packages.  pip install tensorflow langchain-google-vertexai tensorflow-hub tensorflow-text   See a usage example.  from langchain_community.vectorstores import MatchingEngine  Google BigQuery Vector Search​  Google BigQuery, BigQuery is a serverless and cost-effective enterprise data warehouse in Google Cloud.  Google BigQuery Vector Search BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.  It can calculate Euclidean or Cosine distance. With LangChain, we default to use Euclidean distance.  We need to install several python packages.  pip install google-cloud-bigquery   See a usage example.  from langchain.vectorstores import BigQueryVectorSearch  Google ScaNN​  Google ScaNN (Scalable Nearest Neighbors) is a python package.  ScaNN is a method for efficient vector similarity search at scale.  ScaNN includes search space pruning and quantization for Maximum Inner Product Search and also supports other distance functions such as Euclidean distance. The implementation is optimized for x86 processors with AVX2 support. See its Google Research github for more details.  We need to install scann python package.  pip install scann   See a usage example.  from langchain_community.vectorstores import ScaNN  Retrievers​ Google Drive​  We need to install several python packages.  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib   See a usage example and authorization instructions.  from langchain_googledrive.retrievers import GoogleDriveRetriever  Vertex AI Search​  Google Cloud Vertex AI Search allows developers to quickly build generative AI powered search engines for customers and employees.  We need to install the google-cloud-discoveryengine python package.  pip install google-cloud-discoveryengine   See a usage example.  from langchain.retrievers import GoogleVertexAISearchRetriever  Document AI Warehouse​  Google Cloud Document AI Warehouse allows enterprises to search, store, govern, and manage documents and their AI-extracted data and metadata in a single platform.  from langchain.retrievers import GoogleDocumentAIWarehouseRetriever docai_wh_retriever = GoogleDocumentAIWarehouseRetriever(     project_number=... ) query = ... documents = docai_wh_retriever.get_relevant_documents(     query, user_ldap=... )  Tools​ Google Cloud Text-to-Speech​  Google Cloud Text-to-Speech enables developers to synthesize natural-sounding speech with 100+ voices, available in multiple languages and variants. It applies DeepMind’s groundbreaking research in WaveNet and Google’s powerful neural networks to deliver the highest fidelity possible.  We need to install a python package.  pip install google-cloud-text-to-speech   See a usage example and authorization instructions.  from langchain.tools import GoogleCloudTextToSpeechTool  Google Drive​  We need to install several python packages.  pip install google-api-python-client google-auth-httplib2 google-auth-oauthlib   See a usage example and authorization instructions.  from langchain_community.utilities.google_drive import GoogleDriveAPIWrapper from langchain_community.tools.google_drive.tool import GoogleDriveSearchTool  Google Places​  We need to install a python package.  pip install googlemaps   See a usage example and authorization instructions.  from langchain.tools import GooglePlacesTool  Google Search​ Set up a Custom Search Engine, following these instructions Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables GOOGLE_API_KEY and GOOGLE_CSE_ID respectively. from langchain_community.utilities import GoogleSearchAPIWrapper   For a more detailed walkthrough of this wrapper, see this notebook.  We can easily load this wrapper as a Tool (to use with an Agent). We can do this with:  from langchain.agents import load_tools tools = load_tools([\"google-search\"])  Google Finance​  We need to install a python package.  pip install google-search-results   See a usage example and authorization instructions.  from langchain_community.tools.google_finance import GoogleFinanceQueryRun from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper  Google Jobs​  We need to install a python package.  pip install google-search-results   See a usage example and authorization instructions.  from langchain_community.tools.google_jobs import GoogleJobsQueryRun from langchain_community.utilities.google_finance import GoogleFinanceAPIWrapper  Google Lens​  See a usage example and authorization instructions.  from langchain_community.tools.google_lens import GoogleLensQueryRun from langchain_community.utilities.google_lens import GoogleLensAPIWrapper  Google Scholar​  We need to install a python package.  pip install google-search-results   See a usage example and authorization instructions.  from langchain_community.tools.google_scholar import GoogleScholarQueryRun from langchain_community.utilities.google_scholar import GoogleScholarAPIWrapper  Google Trends​  We need to install a python package.  pip install google-search-results   See a usage example and authorization instructions.  from langchain_community.tools.google_trends import GoogleTrendsQueryRun from langchain_community.utilities.google_trends import GoogleTrendsAPIWrapper  Document Transformers​ Google Document AI​  Document AI is a Google Cloud Platform service that transforms unstructured data from documents into structured data, making it easier to understand, analyze, and consume.  We need to set up a GCS bucket and create your own OCR processor The GCS_OUTPUT_PATH should be a path to a folder on GCS (starting with gs://) and a processor name should look like projects/PROJECT_NUMBER/locations/LOCATION/processors/PROCESSOR_ID. We can get it either programmatically or copy from the Prediction endpoint section of the Processor details tab in the Google Cloud Console.  pip install google-cloud-documentai pip install google-cloud-documentai-toolbox   See a usage example.  from langchain_community.document_loaders.blob_loaders import Blob from langchain_community.document_loaders.parsers import DocAIParser  Google Translate​  Google Translate is a multilingual neural machine translation service developed by Google to translate text, documents and websites from one language into another.  The GoogleTranslateTransformer allows you to translate text and HTML with the Google Cloud Translation API.  To use it, you should have the google-cloud-translate python package installed, and a Google Cloud project with the Translation API enabled. This transformer uses the Advanced edition (v3).  First, we need to install the python package.  pip install google-cloud-translate   See a usage example and authorization instructions.  from langchain_community.document_transformers import GoogleTranslateTransformer  Toolkits​ GMail​  Gmail is a free email service provided by Google. This toolkit works with emails through the Gmail API.  We need to install several python packages.  pip install google-api-python-client google-auth-oauthlib google-auth-httplib2   See a usage example and authorization instructions.  from langchain_community.agent_toolkits import GmailToolkit  Memory​ Cloud Firestore​  Cloud Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development.  First, we need to install the python package.  pip install firebase-admin   See a usage example and authorization instructions.  from langchain_community.chat_message_histories.firestore import  FirestoreChatMessageHistory  Chat Loaders​ GMail​  Gmail is a free email service provided by Google. This loader works with emails through the Gmail API.  We need to install several python packages.  pip install google-api-python-client google-auth-oauthlib google-auth-httplib2   See a usage example and authorization instructions.  from langchain_community.chat_loaders.gmail import GMailLoader  3rd Party Integrations​ SearchApi​  SearchApi provides a 3rd-party API to access Google search results, YouTube search & transcripts, and other Google-related engines.  See usage examples and authorization instructions.  from langchain_community.utilities import SearchApiAPIWrapper  SerpAPI​  SerpApi provides a 3rd-party API to access Google search results.  See a usage example and authorization instructions.  from langchain_community.utilities import SerpAPIWrapper  Serper.dev​  See a usage example and authorization instructions.  from langchain_community.utilities import GoogleSerperAPIWrapper  YouTube​  YouTube Search package searches YouTube videos avoiding using their heavily rate-limited API.  It uses the form on the YouTube homepage and scrapes the resulting page.  We need to install a python package.  pip install youtube_search   See a usage example.  from langchain.tools import YouTubeSearchTool  YouTube audio​  YouTube is an online video sharing and social media platform created by Google.  Use YoutubeAudioLoader to fetch / download the audio files.  Then, use OpenAIWhisperParser to transcribe them to text.  We need to install several python packages.  pip install yt_dlp pydub librosa   See a usage example and authorization instructions.  from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader from langchain_community.document_loaders.parsers import OpenAIWhisperParser, OpenAIWhisperParserLocal  YouTube transcripts​  YouTube is an online video sharing and social media platform created by Google.  We need to install youtube-transcript-api python package.  pip install youtube-transcript-api   See a usage example.  from langchain_community.document_loaders import YoutubeLoader  Previous AWS Next Hugging Face Chat models Google AI Vertex AI Document Loaders Google BigQuery LLMs Vertex AI Model Garden Google Cloud Storage Google Drive Speech-to-Text Vector Stores Google Vertex AI Vector Search Google BigQuery Vector Search Google ScaNN Retrievers Google Drive Vertex AI Search Document AI Warehouse Tools Google Cloud Text-to-Speech Google Drive Google Places Google Search Google Finance Google Jobs Google Lens Google Scholar Google Trends Document Transformers Google Document AI Google Translate Toolkits GMail Memory Cloud Firestore Chat Loaders GMail 3rd Party Integrations SearchApi SerpAPI Serper.dev YouTube YouTube audio YouTube transcripts Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/aws",
    "title": "AWS | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersAWS AWS  The LangChain integrations related to Amazon AWS platform.  LLMs​ Bedrock​  Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon via a single API, along with a broad set of capabilities you need to build generative AI applications with security, privacy, and responsible AI. Using Amazon Bedrock, you can easily experiment with and evaluate top FMs for your use case, privately customize them with your data using techniques such as fine-tuning and Retrieval Augmented Generation (RAG), and build agents that execute tasks using your enterprise systems and data sources. Since Amazon Bedrock is serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy generative AI capabilities into your applications using the AWS services you are already familiar with.  See a usage example.  from langchain_community.llms.bedrock import Bedrock  Amazon API Gateway​  Amazon API Gateway is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" for applications to access data, business logic, or functionality from your backend services. Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication applications. API Gateway supports containerized and serverless workloads, as well as web applications.  API Gateway handles all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, CORS support, authorization and access control, throttling, monitoring, and API version management. API Gateway has no minimum fees or startup costs. You pay for the API calls you receive and the amount of data transferred out and, with the API Gateway tiered pricing model, you can reduce your cost as your API usage scales.  See a usage example.  from langchain_community.llms import AmazonAPIGateway  SageMaker Endpoint​  Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.  We use SageMaker to host our model and expose it as the SageMaker Endpoint.  See a usage example.  from langchain_community.llms import SagemakerEndpoint from langchain_community.llms.sagemaker_endpoint import LLMContentHandler  Chat models​ Bedrock Chat​  See a usage example.  from langchain_community.chat_models import BedrockChat  Text Embedding Models​ Bedrock​  See a usage example.  from langchain_community.embeddings import BedrockEmbeddings  SageMaker Endpoint​  See a usage example.  from langchain_community.embeddings import SagemakerEndpointEmbeddings from langchain_community.llms.sagemaker_endpoint import ContentHandlerBase  Chains​ Amazon Comprehend Moderation Chain​  Amazon Comprehend is a natural-language processing (NLP) service that uses machine learning to uncover valuable insights and connections in text.  We need to install the boto3 and nltk libraries.  pip install boto3 nltk   See a usage example.  from langchain_experimental.comprehend_moderation import AmazonComprehendModerationChain  Document loaders​ AWS S3 Directory and File​  Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory AWS S3 Buckets  See a usage example for S3DirectoryLoader.  See a usage example for S3FileLoader.  from langchain_community.document_loaders import S3DirectoryLoader, S3FileLoader  Amazon Textract​  Amazon Textract is a machine learning (ML) service that automatically extracts text, handwriting, and data from scanned documents.  See a usage example.  from langchain_community.document_loaders import AmazonTextractPDFLoader  Memory​ AWS DynamoDB​  AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.  We have to configure the AWS CLI.  We need to install the boto3 library.  pip install boto3   See a usage example.  from langchain.memory import DynamoDBChatMessageHistory  Retrievers​ Amazon Kendra​  Amazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.  With Kendra, we can search across a wide range of content types, including documents, FAQs, knowledge bases, manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and contextual meanings to provide highly relevant search results.  We need to install the boto3 library.  pip install boto3   See a usage example.  from langchain.retrievers import AmazonKendraRetriever  Amazon Bedrock (Knowledge Bases)​  Knowledge bases for Amazon Bedrock is an Amazon Web Services (AWS) offering which lets you quickly build RAG applications by using your private data to customize foundation model response.  We need to install the boto3 library.  pip install boto3   See a usage example.  from langchain.retrievers import AmazonKnowledgeBasesRetriever  Vector stores​ Amazon OpenSearch Service​  Amazon OpenSearch Service performs interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. Amazon OpenSearch Service offers the latest versions of OpenSearch, support for many versions of Elasticsearch, as well as visualization capabilities powered by OpenSearch Dashboards and Kibana.  We need to install several python libraries.  pip install boto3 requests requests-aws4auth   See a usage example.  from langchain_community.vectorstores import OpenSearchVectorSearch  Tools​ AWS Lambda​  Amazon AWS Lambda is a serverless computing service provided by Amazon Web Services (AWS). It helps developers to build and run applications and services without provisioning or managing servers. This serverless architecture enables you to focus on writing and deploying code, while AWS automatically takes care of scaling, patching, and managing the infrastructure required to run your applications.  We need to install boto3 python library.  pip install boto3   See a usage example.  Callbacks​ SageMaker Tracking​  Amazon SageMaker is a fully managed service that is used to quickly and easily build, train and deploy machine learning (ML) models.  Amazon SageMaker Experiments is a capability of Amazon SageMaker that lets you organize, track, compare and evaluate ML experiments and model versions.  We need to install several python libraries.  pip install google-search-results sagemaker   See a usage example.  from langchain.callbacks import SageMakerCallbackHandler  Previous Anthropic Next Google LLMs Bedrock Amazon API Gateway SageMaker Endpoint Chat models Bedrock Chat Text Embedding Models Bedrock SageMaker Endpoint Chains Amazon Comprehend Moderation Chain Document loaders AWS S3 Directory and File Amazon Textract Memory AWS DynamoDB Retrievers Amazon Kendra Amazon Bedrock (Knowledge Bases) Vector stores Amazon OpenSearch Service Tools AWS Lambda Callbacks SageMaker Tracking Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/components",
    "title": "Components | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores Components Components 🗃️ LLMs  78 items  🗃️ Chat models  38 items  🗃️ Document loaders  141 items  🗃️ Document transformers  9 items  🗃️ Text embedding models  50 items  🗃️ Vector stores  72 items  🗃️ Retrievers  39 items  🗃️ Tools  56 items  🗃️ Agents and toolkits  28 items  🗃️ Memory  20 items  🗃️ Callbacks  10 items  🗃️ Chat loaders  11 items  🗃️ Adapters  2 items  🗃️ Stores  6 items  Previous Zilliz Next LLMs Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/chat/",
    "title": "Chat models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Alibaba Cloud PAI EAS Anthropic Anthropic Functions Anyscale Azure OpenAI Azure ML Endpoint Baichuan Chat Baidu Qianfan Bedrock Chat Cohere Ernie Bot Chat EverlyAI Fireworks GigaChat Google AI Google Cloud Vertex AI GPTRouter Hugging Face Chat Wrapper JinaChat Konko LiteLLM Llama 2 Chat Llama API LlamaEdge MiniMax MistralAI NVIDIA AI Foundation Endpoints Ollama Ollama Functions OpenAI PromptLayer ChatOpenAI Tencent Hunyuan Tongyi Qwen vLLM Chat Volc Enging Maas YandexGPT ZHIPU AI Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsChat models Chat models Features (natively supported)​  All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all ChatModels basic support for async, streaming and batch, which by default is implemented as below:  Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the ChatModel is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations. Batch support defaults to calling the underlying ChatModel in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig.  Each ChatModel integration can optionally provide native implementations to truly enable async or streaming. The table shows, for each integration, which features have been implemented with native support.  Model Invoke Async invoke Stream Async stream AzureChatOpenAI ✅ ✅ ✅ ✅ BedrockChat ✅ ❌ ✅ ❌ ChatAnthropic ✅ ✅ ✅ ✅ ChatAnyscale ✅ ✅ ✅ ✅ ChatBaichuan ✅ ❌ ✅ ❌ ChatCohere ✅ ✅ ✅ ✅ ChatDatabricks ✅ ❌ ❌ ❌ ChatEverlyAI ✅ ✅ ✅ ✅ ChatFireworks ✅ ✅ ✅ ✅ ChatGooglePalm ✅ ✅ ❌ ❌ ChatHuggingFace ✅ ✅ ❌ ❌ ChatHunyuan ✅ ❌ ✅ ❌ ChatJavelinAIGateway ✅ ✅ ❌ ❌ ChatKonko ✅ ❌ ❌ ❌ ChatLiteLLM ✅ ✅ ✅ ✅ ChatMLflowAIGateway ✅ ❌ ❌ ❌ ChatMlflow ✅ ❌ ❌ ❌ ChatOllama ✅ ✅ ✅ ✅ ChatOpenAI ✅ ✅ ✅ ✅ ChatTongyi ✅ ✅ ✅ ✅ ChatVertexAI ✅ ✅ ✅ ❌ ChatYandexGPT ✅ ✅ ❌ ❌ ChatZhipuAI ✅ ✅ ✅ ❌ ErnieBotChat ✅ ❌ ❌ ❌ GPTRouter ✅ ✅ ✅ ✅ GigaChat ✅ ✅ ✅ ✅ JinaChat ✅ ✅ ✅ ✅ LlamaEdgeChatService ✅ ❌ ✅ ❌ MiniMaxChat ✅ ✅ ❌ ❌ PaiEasChatEndpoint ✅ ❌ ❌ ✅ PromptLayerChatOpenAI ✅ ❌ ❌ ❌ QianfanChatEndpoint ✅ ✅ ✅ ✅ VolcEngineMaasChat ✅ ❌ ✅ ❌ Previous YandexGPT Next Chat models Features (natively supported) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/text_embedding",
    "title": "Text embedding models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Aleph Alpha AwaDB Azure OpenAI Baidu Qianfan Bedrock BGE on Hugging Face Bookend AI Clarifai Cloudflare Workers AI Cohere DashScope DeepInfra EDEN AI Elasticsearch Embaas ERNIE Fake Embeddings FastEmbed by Qdrant Google Generative AI Embeddings Google Vertex AI PaLM GPT4All Gradient Hugging Face Infinity Instruct Embeddings on Hugging Face Jina John Snow Labs Llama-cpp LLMRails LocalAI MiniMax MistralAI ModelScope MosaicML NLP Cloud NVIDIA AI Foundation Endpoints Ollama OpenClip OpenAI SageMaker Self Hosted Sentence Transformers on Hugging Face SpaCy TensorFlow Hub Text Embeddings Inference Together AI Volc Engine Voyage AI Xorbits inference (Xinference) YandexGPT Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsText embedding models Text embedding models 📄️ Aleph Alpha  There are two possible ways to use Aleph Alpha’s semantic embeddings. If  📄️ AwaDB  AwaDB is an AI Native database for  📄️ Azure OpenAI  Let’s load the Azure OpenAI Embedding class with environment variables  📄️ Baidu Qianfan  Baidu AI Cloud Qianfan Platform is a one-stop large model development  📄️ Bedrock  Amazon Bedrock is a fully managed  📄️ BGE on Hugging Face  [BGE models on the  📄️ Bookend AI  Let’s load the Bookend AI Embeddings class.  📄️ Clarifai  Clarifai is an AI Platform that provides  📄️ Cloudflare Workers AI  [Cloudflare,  📄️ Cohere  Let’s load the Cohere Embedding class.  📄️ DashScope  Let’s load the DashScope Embedding class.  📄️ DeepInfra  DeepInfra is a serverless  📄️ EDEN AI  Eden AI is revolutionizing the AI landscape by uniting the best AI  📄️ Elasticsearch  Walkthrough of how to generate embeddings using a hosted embedding model  📄️ Embaas  embaas is a fully managed NLP API service that  📄️ ERNIE  [ERNIE  📄️ Fake Embeddings  LangChain also provides a fake embedding class. You can use this to test  📄️ FastEmbed by Qdrant  FastEmbed from  📄️ Google Generative AI Embeddings  Connect to Google’s generative AI embeddings service using the  📄️ Google Vertex AI PaLM  [Vertex AI PaLM  📄️ GPT4All  GPT4All is a free-to-use, locally  📄️ Gradient  Gradient allows to create Embeddings as well fine tune and get  📄️ Hugging Face  Let’s load the Hugging Face Embedding class.  📄️ Infinity  Infinity allows to create Embeddings using a MIT-licensed Embedding  📄️ Instruct Embeddings on Hugging Face  [Hugging Face  📄️ Jina  Let’s load the Jina Embedding class.  📄️ John Snow Labs  John Snow Labs NLP & LLM ecosystem  📄️ Llama-cpp  This notebook goes over how to use Llama-cpp embeddings within LangChain  📄️ LLMRails  Let’s load the LLMRails Embeddings class.  📄️ LocalAI  Let’s load the LocalAI Embedding class. In order to use the LocalAI  📄️ MiniMax  MiniMax  📄️ MistralAI  This notebook explains how to use MistralAIEmbeddings, which is included  📄️ ModelScope  ModelScope is big repository of the  📄️ MosaicML  MosaicML offers  📄️ NLP Cloud  NLP Cloud is an artificial  📄️ NVIDIA AI Foundation Endpoints  [NVIDIA AI Foundation  📄️ Ollama  Let’s load the Ollama Embeddings class.  📄️ OpenClip  OpenClip is an  📄️ OpenAI  Let’s load the OpenAI Embedding class.  📄️ SageMaker  Let’s load the SageMaker Endpoints Embeddings class. The class can be  📄️ Self Hosted  Let’s load the SelfHostedEmbeddings,  📄️ Sentence Transformers on Hugging Face  [Hugging Face  📄️ SpaCy  spaCy is an open-source software library for  📄️ TensorFlow Hub  TensorFlow Hub is a repository of  📄️ Text Embeddings Inference  Text Embeddings Inference (TEI) is a toolkit for deploying and serving  📄️ Together AI  This notebook covers how to get started with open source embedding  📄️ Volc Engine  This notebook provides you with a guide on how to load the Volcano  📄️ Voyage AI  Voyage AI provides cutting-edge  📄️ Xorbits inference (Xinference)  This notebook goes over how to use Xinference embeddings within  📄️ YandexGPT  This notebook goes over how to use Langchain with  Previous OpenAI metadata tagger Next Aleph Alpha Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/anthropic",
    "title": "Anthropic | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersAnthropic Anthropic  All functionality related to Anthropic models.  Anthropic is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain.  Prompting Overview​  Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.  Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (Human:) and an AI assistant (Assistant:). Prompts sent via the API must contain \\n\\nHuman: and \\n\\nAssistant: as the signals of who's speaking. The final turn must always be \\n\\nAssistant: - the input string cannot have \\n\\nHuman: as the final role.  Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain ChatModel or LLM. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The Anthropic wrapper however is deprecated, as all functionality can be achieved in a more generic way using ChatAnthropic.  Prompting Best Practices​  Anthropic models have several prompting best practices compared to OpenAI models.  No System Messages  Anthropic models are not trained on the concept of a \"system message\". We have worked with the Anthropic team to handle them somewhat appropriately (a Human message with an admin tag) but this is largely a hack and it is recommended that you do not use system messages.  AI Messages Can Continue  A completion from Claude is a continuation of the last text in the string which allows you further control over Claude's output. For example, putting words in Claude's mouth in a prompt like this:  \\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: What do you call a bear with no teeth?  This will return a completion like this A gummy bear! instead of a whole new assistant message with a different random bear joke.  ChatAnthropic​  ChatAnthropic is a subclass of LangChain's ChatModel, meaning it works best with ChatPromptTemplate. You can import this wrapper with the following code:  from langchain_community.chat_models import ChatAnthropic model = ChatAnthropic()   When working with ChatModels, it is preferred that you design your prompts as ChatPromptTemplates. Here is an example below of doing that:  from langchain_core.prompts import ChatPromptTemplate  prompt = ChatPromptTemplate.from_messages([     (\"system\", \"You are a helpful chatbot\"),     (\"human\", \"Tell me a joke about {topic}\"), ])   You can then use this in a chain as follows:  chain = prompt | model chain.invoke({\"topic\": \"bears\"})   How is the prompt actually being formatted under the hood? We can see that by running the following code  prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value)   This produces the following formatted string:  '\\n\\nYou are a helpful chatbot\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:'   We can see that under the hood LangChain is not appending any prefix/suffix to SystemMessage's. This is because Anthropic has no concept of SystemMessage. Anthropic requires all prompts to end with assistant messages. This means if the last message is not an assistant message, the suffix Assistant: will automatically be inserted.  If you decide instead to use a normal PromptTemplate (one that just works on a single string) let's take a look at what happens:  from langchain.prompts import PromptTemplate  prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value)   This produces the following formatted string:  '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant:'   We can see that it automatically adds the Human and Assistant tags. What is happening under the hood? First: the string gets converted to a single human message. This happens generically (because we are using a subclass of ChatModel). Then, similarly to the above example, an empty Assistant message is getting appended. This is Anthropic specific.  [Deprecated] Anthropic​  This Anthropic wrapper is subclassed from LLM. We can import it with:  from langchain_community.llms import Anthropic model = Anthropic()   This model class is designed to work with normal PromptTemplates. An example of that is below:  prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\") chain = prompt | model chain.invoke({\"topic\": \"bears\"})   Let's see what is going on with the prompt templating under the hood!  prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value)   This outputs the following  '\\n\\nHuman: Tell me a joke about bears\\n\\nAssistant: Sure, here you go:\\n'   Notice that it adds the Human tag at the start of the string, and then finishes it with \\n\\nAssistant: Sure, here you go:. The extra Sure, here you go was added on purpose by the Anthropic team.  What happens if we have those symbols in the prompt directly?  prompt = PromptTemplate.from_template(\"Human: Tell me a joke about {topic}\") prompt_value = prompt.format_prompt(topic=\"bears\") model.convert_prompt(prompt_value)   This outputs:  '\\n\\nHuman: Tell me a joke about bears'   We can see that we detect that the user is trying to use the special tokens, and so we don't do any formatting.  ChatAnthropicMessages (Beta)​  ChatAnthropicMessages uses the beta release of Anthropic's new Messages API.  You can use it from the langchain-anthropic package, which you can install with pip install langchain-anthropic.  For more information, see the ChatAnthropicMessages docs  Previous Providers Next AWS Prompting Overview Prompting Best Practices ChatAnthropic Deprecated Anthropic ChatAnthropicMessages (Beta) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/document_loaders",
    "title": "Document loaders | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders acreom Airbyte CDK Airbyte Gong Airbyte Hubspot Airbyte JSON Airbyte Salesforce Airbyte Shopify Airbyte Stripe Airbyte Typeform Airbyte Zendesk Support Airtable Alibaba Cloud MaxCompute Amazon Textract Apify Dataset ArcGIS Arxiv AssemblyAI Audio Transcripts AstraDB Async Chromium AsyncHtml AWS S3 Directory AWS S3 File AZLyrics Azure AI Data Azure Blob Storage Container Azure Blob Storage File Azure AI Document Intelligence BibTeX BiliBili Blackboard Blockchain Brave Search Browserless ChatGPT Data College Confidential Concurrent Loader Confluence CoNLL-U Copy Paste Couchbase CSV Cube Semantic Layer Datadog Logs Diffbot Discord Docugami Docusaurus Dropbox DuckDB Email EPub Etherscan EverNote Facebook Chat Fauna Figma Geopandas Git GitBook GitHub Google BigQuery Google Cloud Storage Directory Google Cloud Storage File Google Drive Google Speech-to-Text Audio Transcripts Grobid Gutenberg Hacker News Huawei OBS Directory Huawei OBS File HuggingFace dataset iFixit Images Image captions IMSDb Iugu Joplin Jupyter Notebook lakeFS LarkSuite (FeiShu) Mastodon MediaWiki Dump Merge Documents Loader mhtml Microsoft Excel Microsoft OneDrive Microsoft OneNote Microsoft PowerPoint Microsoft SharePoint Microsoft Word Modern Treasury MongoDB News URL Notion DB 1/2 Notion DB 2/2 Nuclia Obsidian Open Document Format (ODT) Open City Data Org-mode Pandas DataFrame Polars DataFrame Psychic PubMed PySpark Quip ReadTheDocs Documentation Recursive URL Reddit Roam Rockset rspace RSS Feeds RST Sitemap Slack Snowflake Source Code Spreedly Stripe Subtitle SurrealDB Telegram Tencent COS Directory Tencent COS File TensorFlow Datasets 2Markdown TOML Trello TSV Twitter Unstructured File URL Weather WebBaseLoader WhatsApp Chat Wikipedia XML Xorbits Pandas DataFrame YouTube audio YouTube transcripts Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsDocument loaders Document loaders 📄️ acreom  acreom is a dev-first knowledge base with tasks  📄️ Airbyte CDK  Airbyte is a data integration  📄️ Airbyte Gong  Airbyte is a data integration  📄️ Airbyte Hubspot  Airbyte is a data integration  📄️ Airbyte JSON  Airbyte is a data integration  📄️ Airbyte Salesforce  Airbyte is a data integration  📄️ Airbyte Shopify  Airbyte is a data integration  📄️ Airbyte Stripe  Airbyte is a data integration  📄️ Airbyte Typeform  Airbyte is a data integration  📄️ Airbyte Zendesk Support  Airbyte is a data integration  📄️ Airtable  - Get your API key  📄️ Alibaba Cloud MaxCompute  [Alibaba Cloud  📄️ Amazon Textract  [Amazon  📄️ Apify Dataset  Apify Dataset is a  📄️ ArcGIS  This notebook demonstrates the use of the  📄️ Arxiv  arXiv is an open-access archive for 2 million  📄️ AssemblyAI Audio Transcripts  The AssemblyAIAudioTranscriptLoader allows to transcribe audio files  📄️ AstraDB  DataStax Astra DB  📄️ Async Chromium  Chromium is one of the browsers supported by Playwright, a library used  📄️ AsyncHtml  AsyncHtmlLoader loads raw HTML from a list of URLs concurrently.  📄️ AWS S3 Directory  [Amazon Simple Storage Service (Amazon  📄️ AWS S3 File  [Amazon Simple Storage Service (Amazon  📄️ AZLyrics  AZLyrics is a large, legal, every day  📄️ Azure AI Data  Azure AI Studio provides the capability to  📄️ Azure Blob Storage Container  [Azure Blob  📄️ Azure Blob Storage File  [Azure  📄️ Azure AI Document Intelligence  Azure AI Document Intelligence  📄️ BibTeX  BibTeX is a file format and reference management system commonly used  📄️ BiliBili  Bilibili is one of the most beloved  📄️ Blackboard  Blackboard Learn  📄️ Blockchain  Overview  📄️ Brave Search  Brave Search is a search  📄️ Browserless  Browserless is a service that allows you to run headless Chrome  📄️ ChatGPT Data  ChatGPT is an artificial intelligence (AI)  📄️ College Confidential  College Confidential gives  📄️ Concurrent Loader  Works just like the GenericLoader but concurrently for those who choose  📄️ Confluence  Confluence is a wiki  📄️ CoNLL-U  CoNLL-U is revised  📄️ Copy Paste  This notebook covers how to load a document object from something you  📄️ Couchbase  Couchbase is an award-winning distributed NoSQL  📄️ CSV  A [comma-separated values  📄️ Cube Semantic Layer  This notebook demonstrates the process of retrieving Cube’s data model  📄️ Datadog Logs  Datadog is a monitoring and analytics  📄️ Diffbot  Unlike traditional web scraping tools,  📄️ Discord  Discord is a VoIP and instant messaging social  📄️ Docugami  This notebook covers how to load documents from Docugami. It provides  📄️ Docusaurus  Docusaurus is a static-site generator which  📄️ Dropbox  Dropbox is a file hosting  📄️ DuckDB  DuckDB is an in-process SQL OLAP database  📄️ Email  This notebook shows how to load email (.eml) or Microsoft Outlook  📄️ EPub  EPUB is an e-book file format  📄️ Etherscan  Etherscan is the leading blockchain  📄️ EverNote  EverNote is intended for archiving and  📄️ Facebook Chat  Messenger) is an  📄️ Fauna  Fauna is a Document Database.  📄️ Figma  Figma is a collaborative web application for  📄️ Geopandas  Geopandas is an  📄️ Git  Git is a distributed version  📄️ GitBook  GitBook is a modern documentation  📄️ GitHub  This notebooks shows how you can load issues and pull requests (PRs) for  📄️ Google BigQuery  Google BigQuery is a serverless  📄️ Google Cloud Storage Directory  [Google Cloud  📄️ Google Cloud Storage File  [Google Cloud  📄️ Google Drive  Google Drive is a file  📄️ Google Speech-to-Text Audio Transcripts  The GoogleSpeechToTextLoader allows to transcribe audio files with the  📄️ Grobid  GROBID is a machine learning library for extracting, parsing, and  📄️ Gutenberg  Project Gutenberg is an online  📄️ Hacker News  Hacker News (sometimes  📄️ Huawei OBS Directory  The following code demonstrates how to load objects from the Huawei OBS  📄️ Huawei OBS File  The following code demonstrates how to load an object from the Huawei  📄️ HuggingFace dataset  The Hugging Face Hub is home  📄️ iFixit  iFixit is the largest, open repair community  📄️ Images  This covers how to load images such as JPG or PNG into a document  📄️ Image captions  By default, the loader utilizes the pre-trained [Salesforce BLIP image  📄️ IMSDb  IMSDb is the Internet Movie Script Database.  📄️ Iugu  Iugu is a Brazilian services and software as  📄️ Joplin  Joplin is an open-source note-taking app.  📄️ Jupyter Notebook  [Jupyter  📄️ lakeFS  lakeFS provides scalable version control  📄️ LarkSuite (FeiShu)  LarkSuite is an enterprise collaboration  📄️ Mastodon  Mastodon is a federated social media and  📄️ MediaWiki Dump  [MediaWiki XML  📄️ Merge Documents Loader  Merge the documents returned from a set of specified data loaders.  📄️ mhtml  MHTML is a is used both for emails but also for archived webpages.  📄️ Microsoft Excel  The UnstructuredExcelLoader is used to load Microsoft Excel files.  📄️ Microsoft OneDrive  Microsoft OneDrive (formerly  📄️ Microsoft OneNote  This notebook covers how to load documents from OneNote.  📄️ Microsoft PowerPoint  [Microsoft  📄️ Microsoft SharePoint  Microsoft SharePoint is a  📄️ Microsoft Word  Microsoft Word  📄️ Modern Treasury  Modern Treasury simplifies complex  📄️ MongoDB  MongoDB is a NoSQL , document-oriented  📄️ News URL  This covers how to load HTML news articles from a list of URLs into a  📄️ Notion DB 1/2  Notion is a collaboration platform with  📄️ Notion DB 2/2  Notion is a collaboration platform with  📄️ Nuclia  Nuclia automatically indexes your unstructured  📄️ Obsidian  Obsidian is a powerful and extensible  📄️ Open Document Format (ODT)  The [Open Document Format for Office Applications  📄️ Open City Data  Socrata  📄️ Org-mode  A Org Mode document is a  📄️ Pandas DataFrame  This notebook goes over how to load data from a  📄️ Polars DataFrame  This notebook goes over how to load data from a  📄️ Psychic  This notebook covers how to load documents from Psychic. See  📄️ PubMed  PubMed® by  📄️ PySpark  This notebook goes over how to load data from a  📄️ Quip  Quip is a collaborative productivity software  📄️ ReadTheDocs Documentation  Read the Docs is an open-sourced free  📄️ Recursive URL  We may want to process load all URLs under a root directory.  📄️ Reddit  Reddit is an American social news  📄️ Roam  ROAM is a note-taking tool for networked  📄️ Rockset  Rockset is a real-time analytics database which enables queries on  📄️ rspace  This notebook shows how to use the RSpace document loader to import  📄️ RSS Feeds  This covers how to load HTML news articles from a list of RSS feed URLs  📄️ RST  A [reStructured Text  📄️ Sitemap  Extends from the WebBaseLoader, SitemapLoader loads a sitemap from a  📄️ Slack  Slack is an instant messaging program.  📄️ Snowflake  This notebooks goes over how to load documents from Snowflake  📄️ Source Code  This notebook covers how to load source code files using a special  📄️ Spreedly  Spreedly is a service that allows you to  📄️ Stripe  Stripe is an Irish-American financial  📄️ Subtitle  [The SubRip file  📄️ SurrealDB  SurrealDB is an end-to-end cloud-native  📄️ Telegram  Telegram Messenger is a globally  📄️ Tencent COS Directory  [Tencent Cloud Object Storage  📄️ Tencent COS File  [Tencent Cloud Object Storage  📄️ TensorFlow Datasets  TensorFlow Datasets is a  📄️ 2Markdown  2markdown service transforms website content  📄️ TOML  TOML is a file format for  📄️ Trello  Trello is a web-based  📄️ TSV  A [tab-separated values  📄️ Twitter  Twitter is an online social media and social  📄️ Unstructured File  This notebook covers how to use Unstructured package to load files of  📄️ URL  This covers how to load HTML documents from a list of URLs into a  📄️ Weather  OpenWeatherMap is an open-source  📄️ WebBaseLoader  This covers how to use WebBaseLoader to load all text from HTML  📄️ WhatsApp Chat  WhatsApp (also called  📄️ Wikipedia  Wikipedia is a multilingual free online  📄️ XML  The UnstructuredXMLLoader is used to load XML files. The loader  📄️ Xorbits Pandas DataFrame  This notebook goes over how to load data from a  📄️ YouTube audio  Building chat or QA applications on YouTube videos is a topic of high  📄️ YouTube transcripts  YouTube is an online video sharing and  Previous ZHIPU AI Next acreom Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/tools",
    "title": "Tools | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Alpha Vantage Apify ArXiv AWS Lambda Shell (bash) Bearly Code Interpreter Bing Search Brave Search ChatGPT Plugins Dall-E Image Generator DataForSEO DuckDuckGo Search E2B Data Analysis Eden AI Eleven Labs Text2Speech File System Golden Query Google Cloud Text-to-Speech Google Drive Google Finance Google Jobs Google Lens Google Places Google Scholar Google Search Google Serper Google Trends Gradio GraphQL HuggingFace Hub Tools Human as a tool IFTTT WebHooks Lemon Agent Memorize Metaphor Search Nuclia Understanding OpenWeatherMap Polygon Stock Market API PubMed Python REPL Reddit Search Requests SceneXplain Search Tools SearchApi SearxNG Search Semantic Scholar API Tool SerpAPI StackExchange Tavily Search Twilio Wikipedia Wolfram Alpha Yahoo Finance News YouTube Zapier Natural Language Actions Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsTools Tools 📄️ Alpha Vantage  Alpha Vantage Alpha Vantage provides  📄️ Apify  This notebook shows how to use the [Apify  📄️ ArXiv  This notebook goes over how to use the arxiv tool with an agent.  📄️ AWS Lambda  Amazon AWS Lambda is a  📄️ Shell (bash)  Giving agents access to the shell is powerful (though risky outside a  📄️ Bearly Code Interpreter  Bearly Code Interpreter allows for remote execution of code. This  📄️ Bing Search  Microsoft Bing, commonly referred to as  📄️ Brave Search  This notebook goes over how to use the Brave Search tool.  📄️ ChatGPT Plugins  This example shows how to use ChatGPT Plugins within LangChain  📄️ Dall-E Image Generator  OpenAI Dall-E are text-to-image models  📄️ DataForSEO  DataForSeo provides comprehensive SEO and  📄️ DuckDuckGo Search  This notebook goes over how to use the duck-duck-go search component.  📄️ E2B Data Analysis  E2B’s cloud environments are great runtime sandboxes  📄️ Eden AI  This Jupyter Notebook demonstrates how to use Eden AI tools with an  📄️ Eleven Labs Text2Speech  This notebook shows how to interact with the ElevenLabs API to achieve  📄️ File System  LangChain provides tools for interacting with a local file system out of  📄️ Golden Query  Golden provides a set of natural language APIs  📄️ Google Cloud Text-to-Speech  Google Cloud Text-to-Speech  📄️ Google Drive  This notebook walks through connecting a LangChain to the  📄️ Google Finance  This notebook goes over how to use the Google Finance Tool to get  📄️ Google Jobs  This notebook goes over how to use the Google Jobs Tool to fetch current  📄️ Google Lens  This notebook goes over how to use the Google Lens Tool to fetch  📄️ Google Places  This notebook goes through how to use Google Places API  📄️ Google Scholar  This notebook goes through how to use Google Scholar Tool  📄️ Google Search  This notebook goes over how to use the google search component.  📄️ Google Serper  This notebook goes over how to use the Google Serper component to  📄️ Google Trends  This notebook goes over how to use the Google Trends Tool to fetch  📄️ Gradio  There are many 1000s of Gradio apps on Hugging Face Spaces. This  📄️ GraphQL  GraphQL is a query language for APIs and a  📄️ HuggingFace Hub Tools  [Huggingface  📄️ Human as a tool  Human are AGI so they can certainly be used as a tool to help out AI  📄️ IFTTT WebHooks  This notebook shows how to use IFTTT Webhooks.  📄️ Lemon Agent  Lemon Agent helps you  📄️ Memorize  Fine-tuning LLM itself to memorize information using unsupervised  📄️ Metaphor Search  Metaphor is a search engine fully designed for use by LLMs. Search for  📄️ Nuclia Understanding  Nuclia automatically indexes your unstructured  📄️ OpenWeatherMap  This notebook goes over how to use the OpenWeatherMap component to  📄️ Polygon Stock Market API  Polygon The Polygon.io Stocks API provides REST  📄️ PubMed  PubMed® comprises more than 35  📄️ Python REPL  Sometimes, for complex calculations, rather than have an LLM generate  📄️ Reddit Search  In this notebook, we learn how the Reddit search tool works.  📄️ Requests  The web contains a lot of information that LLMs do not have access to.  📄️ SceneXplain  SceneXplain is an ImageCaptioning service  📄️ Search Tools  This notebook shows off usage of various search tools.  📄️ SearchApi  This notebook shows examples of how to use SearchApi to search the web.  📄️ SearxNG Search  This notebook goes over how to use a self hosted SearxNG search API to  📄️ Semantic Scholar API Tool  This notebook demos how to use the semantic scholar tool with an agent.  📄️ SerpAPI  This notebook goes over how to use the SerpAPI component to search the  📄️ StackExchange  This notebook goes over how to use the stack exchange component.  📄️ Tavily Search  Tavily’s Search API is a search engine built  📄️ Twilio  This notebook goes over how to use the Twilio  📄️ Wikipedia  Wikipedia is a multilingual free online  📄️ Wolfram Alpha  This notebook goes over how to use the wolfram alpha component.  📄️ Yahoo Finance News  This notebook goes over how to use the yahoofinancenews tool with an  📄️ YouTube  YouTube Search package  📄️ Zapier Natural Language Actions  Deprecated This API will be sunset on 2023-11-17:  Previous Zep Next Alpha Vantage Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/document_transformers",
    "title": "Document transformers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Beautiful Soup Google Cloud Document AI Doctran: extract properties Doctran: interrogate documents Doctran: language translation Google Translate HTML to text Nuclia OpenAI metadata tagger Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsDocument transformers Document transformers 📄️ Beautiful Soup  Beautiful Soup is a  📄️ Google Cloud Document AI  Document AI is a document understanding platform from Google Cloud to  📄️ Doctran: extract properties  We can extract useful features of documents using the  📄️ Doctran: interrogate documents  Documents used in a vector store knowledge base are typically stored in  📄️ Doctran: language translation  Comparing documents through embeddings has the benefit of working across  📄️ Google Translate  Google Translate is a multilingual  📄️ HTML to text  html2text is a Python package  📄️ Nuclia  Nuclia automatically indexes your unstructured  📄️ OpenAI metadata tagger  It can often be useful to tag ingested documents with structured  Previous YouTube transcripts Next Beautiful Soup Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/retrievers",
    "title": "Retrievers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Activeloop Deep Memory Amazon Kendra Arcee Retriever Arxiv Azure Cognitive Search Bedrock (Knowledge Bases) BM25 Chaindesk ChatGPT Plugin Cohere Reranker Cohere RAG retriever DocArray ElasticSearch BM25 Embedchain Fleet AI Libraries Context Google Drive Google Vertex AI Search Jaguar Vector Database Kay.ai kNN LOTR (Merger Retriever) Metal Outline Pinecone Hybrid Search PubMed Qdrant Sparse Vector Retriever RAGatouille RePhraseQuery SEC filing Self-querying retriever SingleStoreDB SVM Tavily Search API TF-IDF Vespa Weaviate Hybrid Search Wikipedia Using the You.com Retriever Zep Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsRetrievers Retrievers 📄️ Activeloop Deep Memory  [Activeloop Deep  📄️ Amazon Kendra  [Amazon  📄️ Arcee Retriever  This notebook demonstrates how to use the ArceeRetriever class to  📄️ Arxiv  arXiv is an open-access archive for 2 million  📄️ Azure Cognitive Search  [Azure Cognitive  📄️ Bedrock (Knowledge Bases)  [Knowledge bases for Amazon  📄️ BM25  BM25 also known as the Okapi  📄️ Chaindesk  Chaindesk platform brings  📄️ ChatGPT Plugin  [OpenAI  📄️ Cohere Reranker  Cohere is a Canadian startup that provides  📄️ Cohere RAG retriever  This notebook covers how to get started with Cohere RAG retriever. This  📄️ DocArray  DocArray is a versatile,  📄️ ElasticSearch BM25  Elasticsearch is a  📄️ Embedchain  Embedchain is a RAG framework to create data pipelines. It loads,  📄️ Fleet AI Libraries Context  The Fleet AI team is on a mission to embed the world’s most important  📄️ Google Drive  This notebook covers how to retrieve documents from Google Drive.  📄️ Google Vertex AI Search  Vertex AI Search (formerly  📄️ Jaguar Vector Database  1. It is a distributed vector database  📄️ Kay.ai  Data API built for RAG 🕵️ We are curating the world’s largest datasets  📄️ kNN  In statistics, the [k-nearest neighbors algorithm  📄️ LOTR (Merger Retriever)  Lord of the Retrievers, also known as MergerRetriever, takes a list  📄️ Metal  Metal is a managed service  📄️ Outline  Outline is an open-source collaborative  📄️ Pinecone Hybrid Search  Pinecone is a vector  📄️ PubMed  PubMed® by  📄️ Qdrant Sparse Vector Retriever  Qdrant is an open-source, high-performance  📄️ RAGatouille  This page covers how to use  📄️ RePhraseQuery  RePhraseQuery is a simple retriever that applies an LLM between the  📄️ SEC filing  The SEC filing is a financial statement or other formal document  🗃️ Self-querying retriever  16 items  📄️ SingleStoreDB  SingleStoreDB is a high-performance  📄️ SVM  [Support vector machines  📄️ Tavily Search API  Tavily’s Search API is a search engine built  📄️ TF-IDF  TF-IDF  📄️ Vespa  Vespa is a fully featured search engine and  📄️ Weaviate Hybrid Search  Weaviate is an open-source  📄️ Wikipedia  Wikipedia is a multilingual free online  📄️ Using the You.com Retriever  The retriever from You.com is good for retrieving lots of text. We  📄️ Zep  Retriever Example for Zep  Previous Zilliz Next Activeloop Deep Memory Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/memory",
    "title": "Memory | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Astra DB AWS DynamoDB Cassandra Elasticsearch Google Cloud Firestore Momento Cache MongoDB Motörhead Neo4j Postgres Redis Remembrall Rockset SingleStoreDB SQL (SQLAlchemy) SQLite Streamlit Upstash Redis Xata Zep Callbacks Chat loaders Adapters Stores ComponentsMemory Memory 📄️ Astra DB  DataStax [Astra  📄️ AWS DynamoDB  [Amazon AWS  📄️ Cassandra  Apache Cassandra® is a NoSQL,  📄️ Elasticsearch  Elasticsearch is a  📄️ Google Cloud Firestore  Cloud Firestore is a NoSQL  📄️ Momento Cache  Momento Cache is the world’s first  📄️ MongoDB  MongoDB is a source-available cross-platform document-oriented  📄️ Motörhead  Motörhead is a memory server  📄️ Neo4j  Neo4j is an open-source graph  📄️ Postgres  PostgreSQL also known as  📄️ Redis  [Redis (Remote Dictionary  📄️ Remembrall  This page covers how to use the Remembrall ecosystem within LangChain.  📄️ Rockset  Rockset is a real-time analytics  📄️ SingleStoreDB  This notebook goes over how to use SingleStoreDB to store chat message  📄️ SQL (SQLAlchemy)  Structured Query Language (SQL)  📄️ SQLite  SQLite is a database engine  📄️ Streamlit  Streamlit is an open-source Python  📄️ Upstash Redis  Upstash is a provider of the  📄️ Xata  Xata is a serverless data platform, based on  📄️ Zep  Fast, Scalable Building Blocks for LLM Apps  Previous Xorbits Next Astra DB Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/llms/",
    "title": "LLMs | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs AI21 Aleph Alpha Amazon API Gateway Anyscale Aphrodite Engine Arcee Azure ML Azure OpenAI Baidu Qianfan Banana Baseten Beam Bedrock Bittensor CerebriumAI ChatGLM Clarifai Cloudflare Workers AI Cohere C Transformers CTranslate2 Databricks DeepInfra DeepSparse Eden AI Fireworks ForefrontAI GigaChat Google AI Google Cloud Vertex AI GooseAI GPT4All Gradient Hugging Face Hub Hugging Face Local Pipelines Huggingface TextGen Inference Javelin AI Gateway Tutorial JSONFormer KoboldAI API Llama.cpp LLM Caching integrations LM Format Enforcer Manifest Minimax Modal MosaicML NLP Cloud OCI Data Science Model Deployment Endpoint OctoAI Ollama OpaquePrompts OpenAI OpenLLM OpenLM AliCloud PAI EAS Petals PipelineAI Predibase Prediction Guard PromptLayer OpenAI RELLM Replicate Runhouse SageMakerEndpoint StochasticAI Nebula (Symbl.ai) TextGen Titan Takeoff Titan Takeoff Pro Together AI Tongyi Qwen vLLM Volc Engine Maas IBM watsonx.ai Writer Xorbits Inference (Xinference) YandexGPT Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ComponentsLLMs LLMs Features (natively supported)​  All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. ainvoke, batch, abatch, stream, astream. This gives all LLMs basic support for async, streaming and batch, which by default is implemented as below:  Async support defaults to calling the respective sync method in asyncio's default thread pool executor. This lets other async functions in your application make progress while the LLM is being executed, by moving this call to a background thread. Streaming support defaults to returning an Iterator (or AsyncIterator in the case of async streaming) of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations. Batch support defaults to calling the underlying LLM in parallel for each input by making use of a thread pool executor (in the sync batch case) or asyncio.gather (in the async batch case). The concurrency can be controlled with the max_concurrency key in RunnableConfig.  Each LLM integration can optionally provide native implementations for async, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support.  Model Invoke Async invoke Stream Async stream Batch Async batch AI21 ✅ ❌ ❌ ❌ ❌ ❌ AlephAlpha ✅ ❌ ❌ ❌ ❌ ❌ AmazonAPIGateway ✅ ❌ ❌ ❌ ❌ ❌ Anthropic ✅ ✅ ✅ ✅ ❌ ❌ Anyscale ✅ ✅ ✅ ✅ ✅ ✅ Aphrodite ✅ ❌ ❌ ❌ ✅ ❌ Arcee ✅ ❌ ❌ ❌ ❌ ❌ Aviary ✅ ❌ ❌ ❌ ❌ ❌ AzureMLOnlineEndpoint ✅ ❌ ❌ ❌ ❌ ❌ AzureOpenAI ✅ ✅ ✅ ✅ ✅ ✅ Banana ✅ ❌ ❌ ❌ ❌ ❌ Baseten ✅ ❌ ❌ ❌ ❌ ❌ Beam ✅ ❌ ❌ ❌ ❌ ❌ Bedrock ✅ ❌ ✅ ❌ ❌ ❌ CTransformers ✅ ✅ ❌ ❌ ❌ ❌ CTranslate2 ✅ ❌ ❌ ❌ ✅ ❌ CerebriumAI ✅ ❌ ❌ ❌ ❌ ❌ ChatGLM ✅ ❌ ❌ ❌ ❌ ❌ Clarifai ✅ ❌ ❌ ❌ ❌ ❌ Cohere ✅ ✅ ❌ ❌ ❌ ❌ Databricks ✅ ❌ ❌ ❌ ❌ ❌ DeepInfra ✅ ✅ ✅ ✅ ❌ ❌ DeepSparse ✅ ✅ ✅ ✅ ❌ ❌ EdenAI ✅ ✅ ❌ ❌ ❌ ❌ Fireworks ✅ ✅ ✅ ✅ ✅ ✅ ForefrontAI ✅ ❌ ❌ ❌ ❌ ❌ GPT4All ✅ ❌ ❌ ❌ ❌ ❌ GigaChat ✅ ✅ ✅ ✅ ✅ ✅ GooglePalm ✅ ❌ ✅ ❌ ✅ ❌ GooseAI ✅ ❌ ❌ ❌ ❌ ❌ GradientLLM ✅ ✅ ❌ ❌ ✅ ✅ HuggingFaceEndpoint ✅ ❌ ❌ ❌ ❌ ❌ HuggingFaceHub ✅ ❌ ❌ ❌ ❌ ❌ HuggingFacePipeline ✅ ❌ ❌ ❌ ✅ ❌ HuggingFaceTextGenInference ✅ ✅ ✅ ✅ ❌ ❌ HumanInputLLM ✅ ❌ ❌ ❌ ❌ ❌ JavelinAIGateway ✅ ✅ ❌ ❌ ❌ ❌ KoboldApiLLM ✅ ❌ ❌ ❌ ❌ ❌ LlamaCpp ✅ ❌ ✅ ❌ ❌ ❌ ManifestWrapper ✅ ❌ ❌ ❌ ❌ ❌ Minimax ✅ ❌ ❌ ❌ ❌ ❌ MlflowAIGateway ✅ ❌ ❌ ❌ ❌ ❌ Modal ✅ ❌ ❌ ❌ ❌ ❌ MosaicML ✅ ❌ ❌ ❌ ❌ ❌ NIBittensorLLM ✅ ❌ ❌ ❌ ❌ ❌ NLPCloud ✅ ❌ ❌ ❌ ❌ ❌ Nebula ✅ ❌ ❌ ❌ ❌ ❌ OCIModelDeploymentTGI ✅ ❌ ❌ ❌ ❌ ❌ OCIModelDeploymentVLLM ✅ ❌ ❌ ❌ ❌ ❌ OctoAIEndpoint ✅ ❌ ❌ ❌ ❌ ❌ Ollama ✅ ❌ ❌ ❌ ❌ ❌ OpaquePrompts ✅ ❌ ❌ ❌ ❌ ❌ OpenAI ✅ ✅ ✅ ✅ ✅ ✅ OpenLLM ✅ ✅ ❌ ❌ ❌ ❌ OpenLM ✅ ✅ ✅ ✅ ✅ ✅ PaiEasEndpoint ✅ ❌ ✅ ❌ ❌ ❌ Petals ✅ ❌ ❌ ❌ ❌ ❌ PipelineAI ✅ ❌ ❌ ❌ ❌ ❌ Predibase ✅ ❌ ❌ ❌ ❌ ❌ PredictionGuard ✅ ❌ ❌ ❌ ❌ ❌ PromptLayerOpenAI ✅ ❌ ❌ ❌ ❌ ❌ QianfanLLMEndpoint ✅ ✅ ✅ ✅ ❌ ❌ RWKV ✅ ❌ ❌ ❌ ❌ ❌ Replicate ✅ ❌ ✅ ❌ ❌ ❌ SagemakerEndpoint ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedHuggingFaceLLM ✅ ❌ ❌ ❌ ❌ ❌ SelfHostedPipeline ✅ ❌ ❌ ❌ ❌ ❌ StochasticAI ✅ ❌ ❌ ❌ ❌ ❌ TextGen ✅ ❌ ❌ ❌ ❌ ❌ TitanTakeoff ✅ ❌ ✅ ❌ ❌ ❌ TitanTakeoffPro ✅ ❌ ✅ ❌ ❌ ❌ Tongyi ✅ ✅ ✅ ✅ ✅ ✅ VLLM ✅ ❌ ❌ ❌ ✅ ❌ VLLMOpenAI ✅ ✅ ✅ ✅ ✅ ✅ VertexAI ✅ ✅ ✅ ❌ ✅ ✅ VertexAIModelGarden ✅ ✅ ❌ ❌ ✅ ✅ VolcEngineMaasLLM ✅ ❌ ✅ ❌ ❌ ❌ WatsonxLLM ✅ ❌ ✅ ❌ ✅ ❌ Writer ✅ ❌ ❌ ❌ ❌ ❌ Xinference ✅ ❌ ❌ ❌ ❌ ❌ YandexGPT ✅ ✅ ❌ ❌ ❌ ❌ Previous Components Next LLMs Features (natively supported) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/platforms/microsoft",
    "title": "Microsoft | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersMicrosoft Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  Chat Models​ Azure OpenAI​  Microsoft Azure, often referred to as Azure is a cloud computing platform run by Microsoft, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.  Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3, Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.  pip install langchain-openai   Set the environment variables to get access to the Azure OpenAI service.  import os  os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://<your-endpoint.openai.azure.com/\" os.environ[\"AZURE_OPENAI_API_KEY\"] = \"your AzureOpenAI key\"   See a usage example  from langchain_openai import AzureChatOpenAI  Text Embedding Models​ Azure OpenAI​  See a usage example  from langchain_openai import AzureOpenAIEmbeddings  LLMs​ Azure OpenAI​  See a usage example.  from langchain_openai import AzureOpenAI  Document loaders​ Azure AI Data​  Azure AI Studio provides the capability to upload data assets to cloud storage and register existing data assets from the following sources:  Microsoft OneLake Azure Blob Storage Azure Data Lake gen 2  First, you need to install several python packages.  pip install azureml-fsspec, azure-ai-generative   See a usage example.  from langchain.document_loaders import AzureAIDataLoader  Azure AI Document Intelligence​  Azure AI Document Intelligence (formerly known as Azure Form Recognizer) is machine-learning based service that extracts text (including handwriting), tables or key-value-pairs from scanned documents or images.  Document Intelligence supports PDF, JPEG, PNG, BMP, or TIFF.  First, you need to install a python package.  pip install azure-ai-documentintelligence   See a usage example.  from langchain.document_loaders import AzureAIDocumentIntelligenceLoader  Azure Blob Storage​  Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.  Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol, Network File System (NFS) protocol, and Azure Files REST API. Azure Files are based on the Azure Blob Storage.  Azure Blob Storage is designed for:  Serving images or documents directly to a browser. Storing files for distributed access. Streaming video and audio. Writing to log files. Storing data for backup and restore, disaster recovery, and archiving. Storing data for analysis by an on-premises or Azure-hosted service. pip install azure-storage-blob   See a usage example for the Azure Blob Storage.  from langchain_community.document_loaders import AzureBlobStorageContainerLoader   See a usage example for the Azure Files.  from langchain_community.document_loaders import AzureBlobStorageFileLoader  Microsoft OneDrive​  Microsoft OneDrive (formerly SkyDrive) is a file-hosting service operated by Microsoft.  First, you need to install a python package.  pip install o365   See a usage example.  from langchain_community.document_loaders import OneDriveLoader  Microsoft Word​  Microsoft Word is a word processor developed by Microsoft.  See a usage example.  from langchain_community.document_loaders import UnstructuredWordDocumentLoader  Microsoft Excel​  Microsoft Excel is a spreadsheet editor developed by Microsoft for Windows, macOS, Android, iOS and iPadOS. It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software.  The UnstructuredExcelLoader is used to load Microsoft Excel files. The loader works with both .xlsx and .xls files. The page content will be the raw text of the Excel file. If you use the loader in \"elements\" mode, an HTML representation of the Excel file will be available in the document metadata under the text_as_html key.  See a usage example.  from langchain_community.document_loaders import UnstructuredExcelLoader  Microsoft SharePoint​  Microsoft SharePoint is a website-based collaboration system that uses workflow applications, “list” databases, and other web parts and security features to empower business teams to work together developed by Microsoft.  See a usage example.  from langchain_community.document_loaders.sharepoint import SharePointLoader  Microsoft PowerPoint​  Microsoft PowerPoint is a presentation program by Microsoft.  See a usage example.  from langchain_community.document_loaders import UnstructuredPowerPointLoader  Microsoft OneNote​  First, let's install dependencies:  pip install bs4 msal   See a usage example.  from langchain_community.document_loaders.onenote import OneNoteLoader  Vector stores​ Azure Cosmos DB​  Azure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB.  Installation and Setup​  See detail configuration instructions.  We need to install pymongo python package.  pip install pymongo  Deploy Azure Cosmos DB on Microsoft Azure​  Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture.  With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones.  Sign Up for free to get started today.  See a usage example.  from langchain_community.vectorstores import AzureCosmosDBVectorSearch  Retrievers​ Azure Cognitive Search​  Azure Cognitive Search (formerly known as Azure Search) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications.  Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities:  A search engine for full text search over a search index containing user-owned content Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more Programmability through REST APIs and client libraries in Azure SDKs Azure integration at the data layer, machine learning layer, and AI (Cognitive Services)  See set up instructions.  See a usage example.  from langchain.retrievers import AzureCognitiveSearchRetriever  Utilities​ Bing Search API​  Microsoft Bing, commonly referred to as Bing or Bing Search, is a web search engine owned and operated by Microsoft.  See a usage example.  from langchain_community.utilities import BingSearchAPIWrapper  Toolkits​ Azure Cognitive Services​  We need to install several python packages.  pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision   See a usage example.  from langchain_community.agent_toolkits import O365Toolkit  Microsoft Office 365 email and calendar​  We need to install O365 python package.  pip install O365   See a usage example.  from langchain_community.agent_toolkits import O365Toolkit  Microsoft Azure PowerBI​  We need to install azure-identity python package.  pip install azure-identity   See a usage example.  from langchain_community.agent_toolkits import PowerBIToolkit from langchain_community.utilities.powerbi import PowerBIDataset  More​ Microsoft Presidio​  Presidio (Origin from Latin praesidium ‘protection, garrison’) helps to ensure sensitive data is properly managed and governed. It provides fast identification and anonymization modules for private entities in text and images such as credit card numbers, names, locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more.  First, you need to install several python packages and download a SpaCy model.  pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker python -m spacy download en_core_web_lg   See usage examples.  from langchain_experimental.data_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer  Previous Hugging Face Next OpenAI Chat Models Azure OpenAI Text Embedding Models Azure OpenAI LLMs Azure OpenAI Document loaders Azure AI Data Azure AI Document Intelligence Azure Blob Storage Microsoft OneDrive Microsoft Word Microsoft Excel Microsoft SharePoint Microsoft PowerPoint Microsoft OneNote Vector stores Azure Cosmos DB Retrievers Azure Cognitive Search Utilities Bing Search API Toolkits Azure Cognitive Services Microsoft Office 365 email and calendar Microsoft Azure PowerBI More Microsoft Presidio Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/document_loaders/",
    "title": "Document loaders | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders CSV File Directory HTML JSON Markdown PDF Text Splitters Text embedding models Vector stores Retrievers Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrievalDocument loaders Document loaders INFO  Head to Integrations for documentation on built-in document loader integrations with 3rd-party tools.  Use document loaders to load data from a source as Document's. A Document is a piece of text and associated metadata. For example, there are document loaders for loading a simple .txt file, for loading the text contents of any web page, or even for loading a transcript of a YouTube video.  Document loaders provide a \"load\" method for loading data as documents from a configured source. They optionally implement a \"lazy load\" as well for lazily loading data into memory.  Get started​  The simplest loader reads in a file as text and places it all into one document.  from langchain_community.document_loaders import TextLoader  loader = TextLoader(\"./index.md\") loader.load()  [     Document(page_content='---\\nsidebar_position: 0\\n---\\n# Document loaders\\n\\nUse document loaders to load data from a source as `Document`\\'s. A `Document` is a piece of text\\nand associated metadata. For example, there are document loaders for loading a simple `.txt` file, for loading the text\\ncontents of any web page, or even for loading a transcript of a YouTube video.\\n\\nEvery document loader exposes two methods:\\n1. \"Load\": load documents from the configured source\\n2. \"Load and split\": load documents from the configured source and split them using the passed in text splitter\\n\\nThey optionally implement:\\n\\n3. \"Lazy load\": load documents into memory lazily\\n', metadata={'source': '../docs/docs/modules/data_connection/document_loaders/index.md'}) ]  Previous Retrieval Next CSV Get started Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/tool_use/",
    "title": "Tool use | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Quickstart Agents Choosing between multiple tools Prompting for tool use Human-in-the-loop Chains with parallel tool use Tool error handling Code understanding Synthetic data generation Graph querying Use casesTool use Tool use  An exciting use case for LLMs is building natural language interfaces for other “tools”, whether those are APIs, functions, databases, etc. LangChain is great for building such interfaces because it has:  Good model output parsing, which makes it easy to extract JSON, XML, OpenAI function-calls, etc. from model outputs. A large collection of built-in Tools. Provides a lot of flexibility in how you call these tools.  There are two main ways to use tools: chains and agents. Chains lets you create a pre-defined sequence of tool usage(s). Agents let the model use tools in a loop, so that it can decide how many times to use tools.  To get started with both approaches, head to the Quickstart page.  Previous Web scraping Next Quickstart Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/data_generation",
    "title": "Synthetic data generation | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesSynthetic data generation Synthetic data generation  Open In Colab  Use case​  Synthetic data is artificially generated data, rather than data collected from real-world events. It’s used to simulate real data without compromising privacy or encountering real-world limitations.  Benefits of Synthetic Data:  Privacy and Security: No real personal data at risk of breaches. Data Augmentation: Expands datasets for machine learning. Flexibility: Create specific or rare scenarios. Cost-effective: Often cheaper than real-world data collection. Regulatory Compliance: Helps navigate strict data protection laws. Model Robustness: Can lead to better generalizing AI models. Rapid Prototyping: Enables quick testing without real data. Controlled Experimentation: Simulate specific conditions. Access to Data: Alternative when real data isn’t available.  Note: Despite the benefits, synthetic data should be used carefully, as it may not always capture real-world complexities.  Quickstart​  In this notebook, we’ll dive deep into generating synthetic medical billing records using the langchain library. This tool is particularly useful when you want to develop or test algorithms but don’t want to use real patient data due to privacy concerns or data availability issues.  Setup​  First, you’ll need to have the langchain library installed, along with its dependencies. Since we’re using the OpenAI generator chain, we’ll install that as well. Since this is an experimental lib, we’ll need to include langchain_experimental in our installs. We’ll then import the necessary modules.  %pip install --upgrade --quiet  langchain langchain_experimental langchain-openai # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()  from langchain.prompts import FewShotPromptTemplate, PromptTemplate from langchain_core.pydantic_v1 import BaseModel from langchain_experimental.tabular_synthetic_data.openai import (     OPENAI_TEMPLATE,     create_openai_data_generator, ) from langchain_experimental.tabular_synthetic_data.prompts import (     SYNTHETIC_FEW_SHOT_PREFIX,     SYNTHETIC_FEW_SHOT_SUFFIX, ) from langchain_openai import ChatOpenAI  1. Define Your Data Model​  Every dataset has a structure or a “schema”. The MedicalBilling class below serves as our schema for the synthetic data. By defining this, we’re informing our synthetic data generator about the shape and nature of data we expect.  class MedicalBilling(BaseModel):     patient_id: int     patient_name: str     diagnosis_code: str     procedure_code: str     total_charge: float     insurance_claim_amount: float   For instance, every record will have a patient_id that’s an integer, a patient_name that’s a string, and so on.  2. Sample Data​  To guide the synthetic data generator, it’s useful to provide it with a few real-world-like examples. These examples serve as a “seed” - they’re representative of the kind of data you want, and the generator will use them to create more data that looks similar.  Here are some fictional medical billing records:  examples = [     {         \"example\": \"\"\"Patient ID: 123456, Patient Name: John Doe, Diagnosis Code:          J20.9, Procedure Code: 99203, Total Charge: $500, Insurance Claim Amount: $350\"\"\"     },     {         \"example\": \"\"\"Patient ID: 789012, Patient Name: Johnson Smith, Diagnosis          Code: M54.5, Procedure Code: 99213, Total Charge: $150, Insurance Claim Amount: $120\"\"\"     },     {         \"example\": \"\"\"Patient ID: 345678, Patient Name: Emily Stone, Diagnosis Code:          E11.9, Procedure Code: 99214, Total Charge: $300, Insurance Claim Amount: $250\"\"\"     }, ]  3. Craft a Prompt Template​  The generator doesn’t magically know how to create our data; we need to guide it. We do this by creating a prompt template. This template helps instruct the underlying language model on how to produce synthetic data in the desired format.  OPENAI_TEMPLATE = PromptTemplate(input_variables=[\"example\"], template=\"{example}\")  prompt_template = FewShotPromptTemplate(     prefix=SYNTHETIC_FEW_SHOT_PREFIX,     examples=examples,     suffix=SYNTHETIC_FEW_SHOT_SUFFIX,     input_variables=[\"subject\", \"extra\"],     example_prompt=OPENAI_TEMPLATE, )   The FewShotPromptTemplate includes:  prefix and suffix: These likely contain guiding context or instructions. examples: The sample data we defined earlier. input_variables: These variables (“subject”, “extra”) are placeholders you can dynamically fill later. For instance, “subject” might be filled with “medical_billing” to guide the model further. example_prompt: This prompt template is the format we want each example row to take in our prompt. 4. Creating the Data Generator​  With the schema and the prompt ready, the next step is to create the data generator. This object knows how to communicate with the underlying language model to get synthetic data.  synthetic_data_generator = create_openai_data_generator(     output_schema=MedicalBilling,     llm=ChatOpenAI(         temperature=1     ),  # You'll need to replace with your actual Language Model instance     prompt=prompt_template, )  5. Generate Synthetic Data​  Finally, let’s get our synthetic data!  synthetic_results = synthetic_data_generator.generate(     subject=\"medical_billing\",     extra=\"the name must be chosen at random. Make it something you wouldn't normally choose.\",     runs=10, )   This command asks the generator to produce 10 synthetic medical billing records. The results are stored in synthetic_results. The output will be a list of the MedicalBilling pydantic models.  Other implementations​ from langchain_experimental.synthetic_data import (     DatasetGenerator,     create_data_generation_chain, ) from langchain_openai import ChatOpenAI  # LLM model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0.7) chain = create_data_generation_chain(model)  chain({\"fields\": [\"blue\", \"yellow\"], \"preferences\": {}})  {'fields': ['blue', 'yellow'],  'preferences': {},  'text': 'The vibrant blue sky contrasted beautifully with the bright yellow sun, creating a stunning display of colors that instantly lifted the spirits of all who gazed upon it.'}  chain(     {         \"fields\": {\"colors\": [\"blue\", \"yellow\"]},         \"preferences\": {\"style\": \"Make it in a style of a weather forecast.\"},     } )  {'fields': {'colors': ['blue', 'yellow']},  'preferences': {'style': 'Make it in a style of a weather forecast.'},  'text': \"Good morning! Today's weather forecast brings a beautiful combination of colors to the sky, with hues of blue and yellow gently blending together like a mesmerizing painting.\"}  chain(     {         \"fields\": {\"actor\": \"Tom Hanks\", \"movies\": [\"Forrest Gump\", \"Green Mile\"]},         \"preferences\": None,     } )  {'fields': {'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']},  'preferences': None,  'text': 'Tom Hanks, the renowned actor known for his incredible versatility and charm, has graced the silver screen in unforgettable movies such as \"Forrest Gump\" and \"Green Mile\".'}  chain(     {         \"fields\": [             {\"actor\": \"Tom Hanks\", \"movies\": [\"Forrest Gump\", \"Green Mile\"]},             {\"actor\": \"Mads Mikkelsen\", \"movies\": [\"Hannibal\", \"Another round\"]},         ],         \"preferences\": {\"minimum_length\": 200, \"style\": \"gossip\"},     } )  {'fields': [{'actor': 'Tom Hanks', 'movies': ['Forrest Gump', 'Green Mile']},   {'actor': 'Mads Mikkelsen', 'movies': ['Hannibal', 'Another round']}],  'preferences': {'minimum_length': 200, 'style': 'gossip'},  'text': 'Did you know that Tom Hanks, the beloved Hollywood actor known for his roles in \"Forrest Gump\" and \"Green Mile\", has shared the screen with the talented Mads Mikkelsen, who gained international acclaim for his performances in \"Hannibal\" and \"Another round\"? These two incredible actors have brought their exceptional skills and captivating charisma to the big screen, delivering unforgettable performances that have enthralled audiences around the world. Whether it\\'s Hanks\\' endearing portrayal of Forrest Gump or Mikkelsen\\'s chilling depiction of Hannibal Lecter, these movies have solidified their places in cinematic history, leaving a lasting impact on viewers and cementing their status as true icons of the silver screen.'}   As we can see created examples are diversified and possess information we wanted them to have. Also, their style reflects the given preferences quite well.  Generating exemplary dataset for extraction benchmarking purposes​ inp = [     {         \"Actor\": \"Tom Hanks\",         \"Film\": [             \"Forrest Gump\",             \"Saving Private Ryan\",             \"The Green Mile\",             \"Toy Story\",             \"Catch Me If You Can\",         ],     },     {         \"Actor\": \"Tom Hardy\",         \"Film\": [             \"Inception\",             \"The Dark Knight Rises\",             \"Mad Max: Fury Road\",             \"The Revenant\",             \"Dunkirk\",         ],     }, ]  generator = DatasetGenerator(model, {\"style\": \"informal\", \"minimal length\": 500}) dataset = generator(inp)  dataset  [{'fields': {'Actor': 'Tom Hanks',    'Film': ['Forrest Gump',     'Saving Private Ryan',     'The Green Mile',     'Toy Story',     'Catch Me If You Can']},   'preferences': {'style': 'informal', 'minimal length': 500},   'text': 'Tom Hanks, the versatile and charismatic actor, has graced the silver screen in numerous iconic films including the heartwarming and inspirational \"Forrest Gump,\" the intense and gripping war drama \"Saving Private Ryan,\" the emotionally charged and thought-provoking \"The Green Mile,\" the beloved animated classic \"Toy Story,\" and the thrilling and captivating true story adaptation \"Catch Me If You Can.\" With his impressive range and genuine talent, Hanks continues to captivate audiences worldwide, leaving an indelible mark on the world of cinema.'},  {'fields': {'Actor': 'Tom Hardy',    'Film': ['Inception',     'The Dark Knight Rises',     'Mad Max: Fury Road',     'The Revenant',     'Dunkirk']},   'preferences': {'style': 'informal', 'minimal length': 500},   'text': 'Tom Hardy, the versatile actor known for his intense performances, has graced the silver screen in numerous iconic films, including \"Inception,\" \"The Dark Knight Rises,\" \"Mad Max: Fury Road,\" \"The Revenant,\" and \"Dunkirk.\" Whether he\\'s delving into the depths of the subconscious mind, donning the mask of the infamous Bane, or navigating the treacherous wasteland as the enigmatic Max Rockatansky, Hardy\\'s commitment to his craft is always evident. From his breathtaking portrayal of the ruthless Eames in \"Inception\" to his captivating transformation into the ferocious Max in \"Mad Max: Fury Road,\" Hardy\\'s dynamic range and magnetic presence captivate audiences and leave an indelible mark on the world of cinema. In his most physically demanding role to date, he endured the harsh conditions of the freezing wilderness as he portrayed the rugged frontiersman John Fitzgerald in \"The Revenant,\" earning him critical acclaim and an Academy Award nomination. In Christopher Nolan\\'s war epic \"Dunkirk,\" Hardy\\'s stoic and heroic portrayal of Royal Air Force pilot Farrier showcases his ability to convey deep emotion through nuanced performances. With his chameleon-like ability to inhabit a wide range of characters and his unwavering commitment to his craft, Tom Hardy has undoubtedly solidified his place as one of the most talented and sought-after actors of his generation.'}]  Extraction from generated examples​  Okay, let’s see if we can now extract output from this generated data and how it compares with our case!  from typing import List  from langchain.chains import create_extraction_chain_pydantic from langchain.output_parsers import PydanticOutputParser from langchain.prompts import PromptTemplate from langchain_openai import OpenAI from pydantic import BaseModel, Field  class Actor(BaseModel):     Actor: str = Field(description=\"name of an actor\")     Film: List[str] = Field(description=\"list of names of films they starred in\")  Parsers​ llm = OpenAI() parser = PydanticOutputParser(pydantic_object=Actor)  prompt = PromptTemplate(     template=\"Extract fields from a given text.\\n{format_instructions}\\n{text}\\n\",     input_variables=[\"text\"],     partial_variables={\"format_instructions\": parser.get_format_instructions()}, )  _input = prompt.format_prompt(text=dataset[0][\"text\"]) output = llm(_input.to_string())  parsed = parser.parse(output) parsed  Actor(Actor='Tom Hanks', Film=['Forrest Gump', 'Saving Private Ryan', 'The Green Mile', 'Toy Story', 'Catch Me If You Can'])  (parsed.Actor == inp[0][\"Actor\"]) & (parsed.Film == inp[0][\"Film\"])  True  Extractors​ extractor = create_extraction_chain_pydantic(pydantic_schema=Actor, llm=model) extracted = extractor.run(dataset[1][\"text\"]) extracted  [Actor(Actor='Tom Hardy', Film=['Inception', 'The Dark Knight Rises', 'Mad Max: Fury Road', 'The Revenant', 'Dunkirk'])]  (extracted[0].Actor == inp[1][\"Actor\"]) & (extracted[0].Film == inp[1][\"Film\"])  True  Previous Code understanding Next Graph querying Use case Quickstart Setup 1. Define Your Data Model 2. Sample Data 3. Craft a Prompt Template 4. Creating the Data Generator 5. Generate Synthetic Data Other implementations Generating exemplary dataset for extraction benchmarking purposes Extraction from generated examples Parsers Extractors Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/extraction",
    "title": "Extraction | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesExtraction Extraction  Open In Collab  Use case​  Getting structured output from raw LLM generations is hard.  For example, suppose you need the model output formatted with a specific schema for:  Extracting a structured row to insert into a database Extracting API parameters Extracting different parts of a user query (e.g., for semantic vs keyword search)  Overview​  There are two primary approaches for this:  Functions: Some LLMs can call functions to extract arbitrary entities from LLM responses.  Parsing: Output parsers are classes that structure LLM responses.  Only some LLMs support functions (e.g., OpenAI), and they are more general than parsers.  Parsers extract precisely what is enumerated in a provided schema (e.g., specific attributes of a person).  Functions can infer things beyond of a provided schema (e.g., attributes about a person that you did not ask for).  Quickstart​  OpenAI functions are one way to get started with extraction.  Define a schema that specifies the properties we want to extract from the LLM output.  Then, we can use create_extraction_chain to extract our desired schema using an OpenAI function call.  pip install langchain langchain-openai   # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()  from langchain.chains import create_extraction_chain from langchain_openai import ChatOpenAI  # Schema schema = {     \"properties\": {         \"name\": {\"type\": \"string\"},         \"height\": {\"type\": \"integer\"},         \"hair_color\": {\"type\": \"string\"},     },     \"required\": [\"name\", \"height\"], }  # Input inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"  # Run chain llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo\") chain = create_extraction_chain(schema, llm) chain.run(inp)  [{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},  {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]  Option 1: OpenAI functions​ Looking under the hood​  Let’s dig into what is happening when we call create_extraction_chain.  The LangSmith trace shows that we call the function information_extraction on the input string, inp.  This information_extraction function is defined here and returns a dict.  We can see the dict in the model output:   {       \"info\": [         {           \"name\": \"Alex\",           \"height\": 5,           \"hair_color\": \"blonde\"         },         {           \"name\": \"Claudia\",           \"height\": 6,           \"hair_color\": \"brunette\"         }       ]     }   The create_extraction_chain then parses the raw LLM output for us using JsonKeyOutputFunctionsParser.  This results in the list of JSON objects returned by the chain above:  [{'name': 'Alex', 'height': 5, 'hair_color': 'blonde'},  {'name': 'Claudia', 'height': 6, 'hair_color': 'brunette'}]  Multiple entity types​  We can extend this further.  Let’s say we want to differentiate between dogs and people.  We can add person_ and dog_ prefixes for each property  schema = {     \"properties\": {         \"person_name\": {\"type\": \"string\"},         \"person_height\": {\"type\": \"integer\"},         \"person_hair_color\": {\"type\": \"string\"},         \"dog_name\": {\"type\": \"string\"},         \"dog_breed\": {\"type\": \"string\"},     },     \"required\": [\"person_name\", \"person_height\"], }  chain = create_extraction_chain(schema, llm)  inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde. Alex's dog Frosty is a labrador and likes to play hide and seek.\"\"\"  chain.run(inp)  [{'person_name': 'Alex',   'person_height': 5,   'person_hair_color': 'blonde',   'dog_name': 'Frosty',   'dog_breed': 'labrador'},  {'person_name': 'Claudia',   'person_height': 6,   'person_hair_color': 'brunette'}]  Unrelated entities​  If we use required: [], we allow the model to return only person attributes or only dog attributes for a single entity (person or dog).  schema = {     \"properties\": {         \"person_name\": {\"type\": \"string\"},         \"person_height\": {\"type\": \"integer\"},         \"person_hair_color\": {\"type\": \"string\"},         \"dog_name\": {\"type\": \"string\"},         \"dog_breed\": {\"type\": \"string\"},     },     \"required\": [], }  chain = create_extraction_chain(schema, llm)  inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde. Willow is a German Shepherd that likes to play with other dogs and can always be found playing with Milo, a border collie that lives close by.\"\"\"  chain.run(inp)  [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},  {'person_name': 'Claudia',   'person_height': 6,   'person_hair_color': 'brunette'},  {'dog_name': 'Willow', 'dog_breed': 'German Shepherd'},  {'dog_name': 'Milo', 'dog_breed': 'border collie'}]  Extra information​  The power of functions (relative to using parsers alone) lies in the ability to perform semantic extraction.  In particular, we can ask for things that are not explicitly enumerated in the schema.  Suppose we want unspecified additional information about dogs.  We can use add a placeholder for unstructured extraction, dog_extra_info.  schema = {     \"properties\": {         \"person_name\": {\"type\": \"string\"},         \"person_height\": {\"type\": \"integer\"},         \"person_hair_color\": {\"type\": \"string\"},         \"dog_name\": {\"type\": \"string\"},         \"dog_breed\": {\"type\": \"string\"},         \"dog_extra_info\": {\"type\": \"string\"},     }, }  chain = create_extraction_chain(schema, llm) chain.run(inp)  [{'person_name': 'Alex', 'person_height': 5, 'person_hair_color': 'blonde'},  {'person_name': 'Claudia',   'person_height': 6,   'person_hair_color': 'brunette'},  {'dog_name': 'Willow',   'dog_breed': 'German Shepherd',   'dog_extra_info': 'likes to play with other dogs'},  {'dog_name': 'Milo',   'dog_breed': 'border collie',   'dog_extra_info': 'lives close by'}]   This gives us additional information about the dogs.  Pydantic​  Pydantic is a data validation and settings management library for Python.  It allows you to create data classes with attributes that are automatically validated when you instantiate an object.  Lets define a class with attributes annotated with types.  from typing import Optional  from langchain.chains import create_extraction_chain_pydantic from langchain_core.pydantic_v1 import BaseModel   # Pydantic data class class Properties(BaseModel):     person_name: str     person_height: int     person_hair_color: str     dog_breed: Optional[str]     dog_name: Optional[str]   # Extraction chain = create_extraction_chain_pydantic(pydantic_schema=Properties, llm=llm)  # Run inp = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\" chain.run(inp)  [Properties(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None),  Properties(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)]   As we can see from the trace, we use the function information_extraction, as above, with the Pydantic schema.  Option 2: Parsing​  Output parsers are classes that help structure language model responses.  As shown above, they are used to parse the output of the OpenAI function calls in create_extraction_chain.  But, they can be used independent of functions.  Pydantic​  Just as a above, let’s parse a generation based on a Pydantic data class.  from typing import Optional, Sequence  from langchain.output_parsers import PydanticOutputParser from langchain.prompts import (     PromptTemplate, ) from langchain_openai import OpenAI from pydantic import BaseModel, Field, validator   class Person(BaseModel):     person_name: str     person_height: int     person_hair_color: str     dog_breed: Optional[str]     dog_name: Optional[str]   class People(BaseModel):     \"\"\"Identifying information about all people in a text.\"\"\"      people: Sequence[Person]   # Run query = \"\"\"Alex is 5 feet tall. Claudia is 1 feet taller Alex and jumps higher than him. Claudia is a brunette and Alex is blonde.\"\"\"  # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser(pydantic_object=People)  # Prompt prompt = PromptTemplate(     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",     input_variables=[\"query\"],     partial_variables={\"format_instructions\": parser.get_format_instructions()}, )  # Run _input = prompt.format_prompt(query=query) model = OpenAI(temperature=0) output = model(_input.to_string()) parser.parse(output)  People(people=[Person(person_name='Alex', person_height=5, person_hair_color='blonde', dog_breed=None, dog_name=None), Person(person_name='Claudia', person_height=6, person_hair_color='brunette', dog_breed=None, dog_name=None)])   We can see from the LangSmith trace that we get the same output as above.  We can see that we provide a two-shot prompt in order to instruct the LLM to output in our desired format.  And, we need to do a bit more work:  Define a class that holds multiple instances of Person Explicitly parse the output of the LLM to the Pydantic class  We can see this for other cases, too.  from langchain.output_parsers import PydanticOutputParser from langchain.prompts import (     PromptTemplate, ) from langchain_openai import OpenAI from pydantic import BaseModel, Field, validator   # Define your desired data structure. class Joke(BaseModel):     setup: str = Field(description=\"question to set up a joke\")     punchline: str = Field(description=\"answer to resolve the joke\")      # You can add custom validation logic easily with Pydantic.     @validator(\"setup\")     def question_ends_with_question_mark(cls, field):         if field[-1] != \"?\":             raise ValueError(\"Badly formed question!\")         return field   # And a query intended to prompt a language model to populate the data structure. joke_query = \"Tell me a joke.\"  # Set up a parser + inject instructions into the prompt template. parser = PydanticOutputParser(pydantic_object=Joke)  # Prompt prompt = PromptTemplate(     template=\"Answer the user query.\\n{format_instructions}\\n{query}\\n\",     input_variables=[\"query\"],     partial_variables={\"format_instructions\": parser.get_format_instructions()}, )  # Run _input = prompt.format_prompt(query=joke_query) model = OpenAI(temperature=0) output = model(_input.to_string()) parser.parse(output)  Joke(setup='Why did the chicken cross the road?', punchline='To get to the other side!')   As we can see, we get an output of the Joke class, which respects our originally desired schema: ‘setup’ and ‘punchline’.  We can look at the LangSmith trace to see exactly what is going on under the hood.  Going deeper​ The output parser documentation includes various parser examples for specific types (e.g., lists, datetime, enum, etc). The experimental Anthropic function calling support provides similar functionality to Anthropic chat models. LlamaCPP natively supports constrained decoding using custom grammars, making it easy to output structured content using local LLMs JSONFormer offers another way for structured decoding of a subset of the JSON Schema. Kor is another library for extraction where schema and examples can be provided to the LLM. Previous Chatbots Next Summarization Use case Overview Quickstart Option 1: OpenAI functions Looking under the hood Multiple entity types Unrelated entities Extra information Pydantic Option 2: Parsing Pydantic Going deeper Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/vectorstores/",
    "title": "Vector stores | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders Text Splitters Text embedding models Vector stores Retrievers Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrievalVector stores Vector stores INFO  Head to Integrations for documentation on built-in integrations with 3rd-party vector stores.  One of the most common ways to store and search over unstructured data is to embed it and store the resulting embedding vectors, and then at query time to embed the unstructured query and retrieve the embedding vectors that are 'most similar' to the embedded query. A vector store takes care of storing embedded data and performing vector search for you.  Get started​  This walkthrough showcases basic functionality related to vector stores. A key part of working with vector stores is creating the vector to put in them, which is usually created via embeddings. Therefore, it is recommended that you familiarize yourself with the text embedding model interfaces before diving into this.  There are many great vector store options, here are a few that are free, open-source, and run entirely on your local machine. Review all integrations for many great hosted offerings.  Chroma FAISS Lance  This walkthrough uses the chroma vector database, which runs on your local machine as a library.  pip install chromadb   We want to use OpenAIEmbeddings so we have to get the OpenAI API Key.  import os import getpass  os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')  from langchain_community.document_loaders import TextLoader from langchain_openai import OpenAIEmbeddings from langchain.text_splitter import CharacterTextSplitter from langchain_community.vectorstores import Chroma  # Load the document, split it into chunks, embed each chunk and load it into the vector store. raw_documents = TextLoader('../../../state_of_the_union.txt').load() text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) documents = text_splitter.split_documents(raw_documents) db = Chroma.from_documents(documents, OpenAIEmbeddings())  Similarity search​ query = \"What did the president say about Ketanji Brown Jackson\" docs = db.similarity_search(query) print(docs[0].page_content)      Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.      Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.      One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.      And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.  Similarity search by vector​  It is also possible to do a search for documents similar to a given embedding vector using similarity_search_by_vector which accepts an embedding vector as a parameter instead of a string.  embedding_vector = OpenAIEmbeddings().embed_query(query) docs = db.similarity_search_by_vector(embedding_vector) print(docs[0].page_content)   The query is the same, and so the result is also the same.      Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.      Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.      One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.      And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.  Asynchronous operations​  Vector stores are usually run as a separate service that requires some IO operations, and therefore they might be called asynchronously. That gives performance benefits as you don't waste time waiting for responses from external services. That might also be important if you work with an asynchronous framework, such as FastAPI.  LangChain supports async operation on vector stores. All the methods might be called using their async counterparts, with the prefix a, meaning async.  Qdrant is a vector store, which supports all the async operations, thus it will be used in this walkthrough.  pip install qdrant-client  from langchain_community.vectorstores import Qdrant  Create a vector store asynchronously​ db = await Qdrant.afrom_documents(documents, embeddings, \"http://localhost:6333\")  Similarity search​ query = \"What did the president say about Ketanji Brown Jackson\" docs = await db.asimilarity_search(query) print(docs[0].page_content)      Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.      Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.      One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.      And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.  Similarity search by vector​ embedding_vector = embeddings.embed_query(query) docs = await db.asimilarity_search_by_vector(embedding_vector)  Maximum marginal relevance search (MMR)​  Maximal marginal relevance optimizes for similarity to query and diversity among selected documents. It is also supported in async API.  query = \"What did the president say about Ketanji Brown Jackson\" found_docs = await qdrant.amax_marginal_relevance_search(query, k=2, fetch_k=10) for i, doc in enumerate(found_docs):     print(f\"{i + 1}.\", doc.page_content, \"\\n\")  1. Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections.  Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.  One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court.  And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.  2. We can’t change how divided we’ve been. But we can change how we move forward—on COVID-19 and other issues we must face together.  I recently visited the New York City Police Department days after the funerals of Officer Wilbert Mora and his partner, Officer Jason Rivera.  They were responding to a 9-1-1 call when a man shot and killed them with a stolen gun.  Officer Mora was 27 years old.  Officer Rivera was 22.  Both Dominican Americans who’d grown up on the same streets they later chose to patrol as police officers.  I spoke with their families and told them that we are forever in debt for their sacrifice, and we will carry on their mission to restore the trust and safety every community deserves.  I’ve worked on these issues a long time.  I know what works: Investing in crime prevention and community police officers who’ll walk the beat, who’ll know the neighborhood, and who can restore trust and safety.  Previous CacheBackedEmbeddings Next Retrievers Get started Similarity search Similarity search by vector Asynchronous operations Create a vector store asynchronously Similarity search Similarity search by vector Maximum marginal relevance search (MMR) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/summarization",
    "title": "Summarization | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesSummarization Summarization  Open In Colab  Use case​  Suppose you have a set of documents (PDFs, Notion pages, customer questions, etc.) and you want to summarize the content.  LLMs are a great tool for this given their proficiency in understanding and synthesizing text.  In this walkthrough we’ll go over how to perform document summarization using LLMs.  Overview​  A central question for building a summarizer is how to pass your documents into the LLM’s context window. Two common approaches for this are:  Stuff: Simply “stuff” all your documents into a single prompt. This is the simplest approach (see here for more on the StuffDocumentsChains, which is used for this method).  Map-reduce: Summarize each document on it’s own in a “map” step and then “reduce” the summaries into a final summary (see here for more on the MapReduceDocumentsChain, which is used for this method).  Quickstart​  To give you a sneak preview, either pipeline can be wrapped in a single object: load_summarize_chain.  Suppose we want to summarize a blog post. We can create this in a few lines of code.  First set environment variables and install packages:  %pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain  # Set env var OPENAI_API_KEY or load from a .env file # import dotenv  # dotenv.load_dotenv()  Requirement already satisfied: openai in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.27.8) Requirement already satisfied: tiktoken in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.0) Requirement already satisfied: chromadb in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.4.4) Requirement already satisfied: langchain in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (0.0.299) Requirement already satisfied: requests>=2.20 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (2.31.0) Requirement already satisfied: tqdm in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (4.64.1) Requirement already satisfied: aiohttp in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from openai) (3.8.5) Requirement already satisfied: regex>=2022.1.18 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from tiktoken) (2023.6.3) Requirement already satisfied: pydantic<2.0,>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.10.12) Requirement already satisfied: chroma-hnswlib==0.7.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.7.2) Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.99.1) Requirement already satisfied: uvicorn[standard]>=0.18.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.23.2) Requirement already satisfied: numpy>=1.21.6 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.24.4) Requirement already satisfied: posthog>=2.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.0.1) Requirement already satisfied: typing-extensions>=4.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (4.7.1) Requirement already satisfied: pulsar-client>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (3.2.0) Requirement already satisfied: onnxruntime>=1.14.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (1.15.1) Requirement already satisfied: tokenizers>=0.13.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.13.3) Requirement already satisfied: pypika>=0.48.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (0.48.9) Collecting tqdm (from openai)   Obtaining dependency information for tqdm from https://files.pythonhosted.org/packages/00/e5/f12a80907d0884e6dff9c16d0c0114d81b8cd07dc3ae54c5e962cc83037e/tqdm-4.66.1-py3-none-any.whl.metadata   Downloading tqdm-4.66.1-py3-none-any.whl.metadata (57 kB)      ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 57.6/57.6 kB 2.7 MB/s eta 0:00:00 Requirement already satisfied: overrides>=7.3.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (7.4.0) Requirement already satisfied: importlib-resources in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from chromadb) (6.0.0) Requirement already satisfied: PyYAML>=5.3 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (6.0.1) Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.0.20) Requirement already satisfied: anyio<4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (3.7.1) Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (4.0.3) Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.5.9) Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (1.33) Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (0.0.42) Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (2.8.5) Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from langchain) (8.2.3) Requirement already satisfied: attrs>=17.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (23.1.0) Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (3.2.0) Requirement already satisfied: multidict<7.0,>=4.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (6.0.4) Requirement already satisfied: yarl<2.0,>=1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.9.2) Requirement already satisfied: frozenlist>=1.1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.4.0) Requirement already satisfied: aiosignal>=1.1.2 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from aiohttp->openai) (1.3.1) Requirement already satisfied: idna>=2.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (3.4) Requirement already satisfied: sniffio>=1.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.3.0) Requirement already satisfied: exceptiongroup in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from anyio<4.0->langchain) (1.1.3) Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.1) Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (1.5.1) Requirement already satisfied: typing-inspect>=0.4.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0) Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb) (0.27.0) Requirement already satisfied: jsonpointer>=1.9 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4) Requirement already satisfied: coloredlogs in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1) Requirement already satisfied: flatbuffers in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26) Requirement already satisfied: packaging in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (23.1) Requirement already satisfied: protobuf in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (4.23.4) Requirement already satisfied: sympy in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12) Requirement already satisfied: six>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.16.0) Requirement already satisfied: monotonic>=1.5 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (1.6) Requirement already satisfied: backoff>=1.10.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.2.1) Requirement already satisfied: python-dateutil>2.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from posthog>=2.4.0->chromadb) (2.8.2) Requirement already satisfied: certifi in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from pulsar-client>=3.1.0->chromadb) (2023.7.22) Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.16) Requirement already satisfied: click>=7.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (8.1.7) Requirement already satisfied: h11>=0.8 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.14.0) Requirement already satisfied: httptools>=0.5.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.0) Requirement already satisfied: python-dotenv>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0) Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.17.0) Requirement already satisfied: watchfiles>=0.13 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0) Requirement already satisfied: websockets>=10.4 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (11.0.3) Requirement already satisfied: zipp>=3.1.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from importlib-resources->chromadb) (3.16.2) Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0) Requirement already satisfied: humanfriendly>=9.1 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0) Requirement already satisfied: mpmath>=0.19 in /Users/bagatur/langchain/.venv/lib/python3.9/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0) Using cached tqdm-4.66.1-py3-none-any.whl (78 kB) Installing collected packages: tqdm   Attempting uninstall: tqdm     Found existing installation: tqdm 4.64.1     Uninstalling tqdm-4.64.1:       Successfully uninstalled tqdm-4.64.1 ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. clarifai 9.8.1 requires tqdm==4.64.1, but you have tqdm 4.66.1 which is incompatible. Successfully installed tqdm-4.66.1   We can use chain_type=\"stuff\", especially if using larger context window models such as:  16k token OpenAI gpt-3.5-turbo-1106 100k token Anthropic Claude-2  We can also supply chain_type=\"map_reduce\" or chain_type=\"refine\" (read more here).  from langchain.chains.summarize import load_summarize_chain from langchain_community.document_loaders import WebBaseLoader from langchain_openai import ChatOpenAI  loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\") docs = loader.load()  llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-1106\") chain = load_summarize_chain(llm, chain_type=\"stuff\")  chain.run(docs)  'The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains. It also highlights the challenges and limitations of using LLMs in agent systems.'  Option 1. Stuff​  When we use load_summarize_chain with chain_type=\"stuff\", we will use the StuffDocumentsChain.  The chain will take a list of documents, inserts them all into a prompt, and passes that prompt to an LLM:  from langchain.chains.combine_documents.stuff import StuffDocumentsChain from langchain.chains.llm import LLMChain from langchain.prompts import PromptTemplate  # Define prompt prompt_template = \"\"\"Write a concise summary of the following: \"{text}\" CONCISE SUMMARY:\"\"\" prompt = PromptTemplate.from_template(prompt_template)  # Define LLM chain llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo-16k\") llm_chain = LLMChain(llm=llm, prompt=prompt)  # Define StuffDocumentsChain stuff_chain = StuffDocumentsChain(llm_chain=llm_chain, document_variable_name=\"text\")  docs = loader.load() print(stuff_chain.run(docs))  The article discusses the concept of building autonomous agents powered by large language models (LLMs). It explores the components of such agents, including planning, memory, and tool use. The article provides case studies and proof-of-concept examples of LLM-powered agents in various domains, such as scientific discovery and generative agents simulation. It also highlights the challenges and limitations of using LLMs in agent systems.   Great! We can see that we reproduce the earlier result using the load_summarize_chain.  Go deeper​ You can easily customize the prompt. You can easily try different LLMs, (e.g., Claude) via the llm parameter. Option 2. Map-Reduce​  Let’s unpack the map reduce approach. For this, we’ll first map each document to an individual summary using an LLMChain. Then we’ll use a ReduceDocumentsChain to combine those summaries into a single global summary.  First, we specify the LLMChain to use for mapping each document to an individual summary:  from langchain.chains import MapReduceDocumentsChain, ReduceDocumentsChain from langchain.text_splitter import CharacterTextSplitter  llm = ChatOpenAI(temperature=0)  # Map map_template = \"\"\"The following is a set of documents {docs} Based on this list of docs, please identify the main themes  Helpful Answer:\"\"\" map_prompt = PromptTemplate.from_template(map_template) map_chain = LLMChain(llm=llm, prompt=map_prompt)   We can also use the Prompt Hub to store and fetch prompts.  This will work with your LangSmith API key.  For example, see the map prompt here.  from langchain import hub  map_prompt = hub.pull(\"rlm/map-prompt\") map_chain = LLMChain(llm=llm, prompt=map_prompt)   The ReduceDocumentsChain handles taking the document mapping results and reducing them into a single output. It wraps a generic CombineDocumentsChain (like StuffDocumentsChain) but adds the ability to collapse documents before passing it to the CombineDocumentsChain if their cumulative size exceeds token_max. In this example, we can actually re-use our chain for combining our docs to also collapse our docs.  So if the cumulative number of tokens in our mapped documents exceeds 4000 tokens, then we’ll recursively pass in the documents in batches of \\< 4000 tokens to our StuffDocumentsChain to create batched summaries. And once those batched summaries are cumulatively less than 4000 tokens, we’ll pass them all one last time to the StuffDocumentsChain to create the final summary.  # Reduce reduce_template = \"\"\"The following is set of summaries: {docs} Take these and distill it into a final, consolidated summary of the main themes.  Helpful Answer:\"\"\" reduce_prompt = PromptTemplate.from_template(reduce_template)  # Note we can also get this from the prompt hub, as noted above reduce_prompt = hub.pull(\"rlm/map-prompt\")  reduce_prompt  ChatPromptTemplate(input_variables=['docs'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['docs'], template='The following is a set of documents:\\n{docs}\\nBased on this list of docs, please identify the main themes \\nHelpful Answer:'))])  # Run chain reduce_chain = LLMChain(llm=llm, prompt=reduce_prompt)  # Takes a list of documents, combines them into a single string, and passes this to an LLMChain combine_documents_chain = StuffDocumentsChain(     llm_chain=reduce_chain, document_variable_name=\"docs\" )  # Combines and iteratively reduces the mapped documents reduce_documents_chain = ReduceDocumentsChain(     # This is final chain that is called.     combine_documents_chain=combine_documents_chain,     # If documents exceed context for `StuffDocumentsChain`     collapse_documents_chain=combine_documents_chain,     # The maximum number of tokens to group documents into.     token_max=4000, )   Combining our map and reduce chains into one:  # Combining documents by mapping a chain over them, then combining results map_reduce_chain = MapReduceDocumentsChain(     # Map chain     llm_chain=map_chain,     # Reduce chain     reduce_documents_chain=reduce_documents_chain,     # The variable name in the llm_chain to put the documents in     document_variable_name=\"docs\",     # Return the results of the map steps in the output     return_intermediate_steps=False, )  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(     chunk_size=1000, chunk_overlap=0 ) split_docs = text_splitter.split_documents(docs)  Created a chunk of size 1003, which is longer than the specified 1000  print(map_reduce_chain.run(split_docs))  Based on the list of documents provided, the main themes can be identified as follows:  1. LLM-powered autonomous agents: The documents discuss the concept of building agents with LLM as their core controller and highlight the potential of LLM beyond generating written content. They explore the capabilities of LLM as a general problem solver.  2. Agent system overview: The documents provide an overview of the components that make up a LLM-powered autonomous agent system, including planning, memory, and tool use. Each component is explained in detail, highlighting its role in enhancing the agent's capabilities.  3. Planning: The documents discuss how the agent breaks down large tasks into smaller subgoals and utilizes self-reflection to improve the quality of its actions and results.  4. Memory: The documents explain the importance of both short-term and long-term memory in an agent system. Short-term memory is utilized for in-context learning, while long-term memory allows the agent to retain and recall information over extended periods.  5. Tool use: The documents highlight the agent's ability to call external APIs for additional information and resources that may be missing from its pre-trained model weights. This includes accessing current information, executing code, and retrieving proprietary information.  6. Case studies and proof-of-concept examples: The documents provide examples of how LLM-powered autonomous agents can be applied in various domains, such as scientific discovery and generative agent simulations. These case studies serve as examples of the capabilities and potential applications of such agents.  7. Challenges: The documents acknowledge the challenges associated with building and utilizing LLM-powered autonomous agents, although specific challenges are not mentioned in the given set of documents.  8. Citation and references: The documents include a citation and reference section, indicating that the information presented is based on existing research and sources.  Overall, the main themes in the provided documents revolve around LLM-powered autonomous agents, their components and capabilities, planning, memory, tool use, case studies, and challenges.  Go deeper​  Customization  As shown above, you can customize the LLMs and prompts for map and reduce stages.  Real-world use-case  See this blog post case-study on analyzing user interactions (questions about LangChain documentation)! The blog post and associated repo also introduce clustering as a means of summarization. This opens up a third path beyond the stuff or map-reduce approaches that is worth considering.  Option 3. Refine​  Refine is similar to map-reduce:  The refine documents chain constructs a response by looping over the input documents and iteratively updating its answer. For each document, it passes all non-document inputs, the current document, and the latest intermediate answer to an LLM chain to get a new answer.  This can be easily run with the chain_type=\"refine\" specified.  chain = load_summarize_chain(llm, chain_type=\"refine\") chain.run(split_docs)  'The article explores the concept of building autonomous agents powered by large language models (LLMs) and their potential as problem solvers. It discusses different approaches to task decomposition, the integration of self-reflection into LLM-based agents, and the use of external classical planners for long-horizon planning. The new context introduces the Chain of Hindsight (CoH) approach and Algorithm Distillation (AD) for training models to produce better outputs. It also discusses different types of memory and the use of external memory for fast retrieval. The article explores the concept of tool use and introduces the MRKL system and experiments on fine-tuning LLMs to use external tools. It introduces HuggingGPT, a framework that uses ChatGPT as a task planner, and discusses the challenges of using LLM-powered agents in real-world scenarios. The article concludes with case studies on scientific discovery agents and the use of LLM-powered agents in anticancer drug discovery. It also introduces the concept of generative agents that combine LLM with memory, planning, and reflection mechanisms. The conversation samples provided discuss the implementation of a game architecture and the challenges in building LLM-centered agents. The article provides references to related research papers and resources for further exploration.'   It’s also possible to supply a prompt and return intermediate steps.  prompt_template = \"\"\"Write a concise summary of the following: {text} CONCISE SUMMARY:\"\"\" prompt = PromptTemplate.from_template(prompt_template)  refine_template = (     \"Your job is to produce a final summary\\n\"     \"We have provided an existing summary up to a certain point: {existing_answer}\\n\"     \"We have the opportunity to refine the existing summary\"     \"(only if needed) with some more context below.\\n\"     \"------------\\n\"     \"{text}\\n\"     \"------------\\n\"     \"Given the new context, refine the original summary in Italian\"     \"If the context isn't useful, return the original summary.\" ) refine_prompt = PromptTemplate.from_template(refine_template) chain = load_summarize_chain(     llm=llm,     chain_type=\"refine\",     question_prompt=prompt,     refine_prompt=refine_prompt,     return_intermediate_steps=True,     input_key=\"input_documents\",     output_key=\"output_text\", ) result = chain({\"input_documents\": split_docs}, return_only_outputs=True)  print(result[\"output_text\"])  Il presente articolo discute il concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. Esplora i diversi componenti di un sistema di agenti alimentato da LLM, tra cui la pianificazione, la memoria e l'uso degli strumenti. Dimostrazioni di concetto come AutoGPT mostrano il potenziale di LLM come risolutore generale di problemi. Approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorarsi iterativamente. Tuttavia, ci sono sfide da affrontare, come la limitata capacità di contesto che limita l'inclusione di informazioni storiche dettagliate e la difficoltà di pianificazione a lungo termine e decomposizione delle attività. Inoltre, l'affidabilità dell'interfaccia di linguaggio naturale tra LLM e componenti esterni come la memoria e gli strumenti è incerta, poiché i LLM possono commettere errori di formattazione e mostrare comportamenti ribelli. Nonostante ciò, il sistema AutoGPT viene menzionato come esempio di dimostrazione di concetto che utilizza LLM come controller principale per agenti autonomi. Questo articolo fa riferimento a diverse fonti che esplorano approcci e applicazioni specifiche di LLM nell'ambito degli agenti autonomi.  print(\"\\n\\n\".join(result[\"intermediate_steps\"][:3]))  This article discusses the concept of building autonomous agents using LLM (large language model) as the core controller. The article explores the different components of an LLM-powered agent system, including planning, memory, and tool use. It also provides examples of proof-of-concept demos and highlights the potential of LLM as a general problem solver.  Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente.  Questo articolo discute del concetto di costruire agenti autonomi utilizzando LLM (large language model) come controller principale. L'articolo esplora i diversi componenti di un sistema di agenti alimentato da LLM, inclusa la pianificazione, la memoria e l'uso degli strumenti. Vengono forniti anche esempi di dimostrazioni di proof-of-concept e si evidenzia il potenziale di LLM come risolutore generale di problemi. Inoltre, vengono presentati approcci come Chain of Thought, Tree of Thoughts, LLM+P, ReAct e Reflexion che consentono agli agenti autonomi di pianificare, riflettere su se stessi e migliorare iterativamente. Il nuovo contesto riguarda l'approccio Chain of Hindsight (CoH) che permette al modello di migliorare autonomamente i propri output attraverso un processo di apprendimento supervisionato. Viene anche presentato l'approccio Algorithm Distillation (AD) che applica lo stesso concetto alle traiettorie di apprendimento per compiti di reinforcement learning.  Splitting and summarizing in a single chain​  For convenience, we can wrap both the text splitting of our long document and summarizing in a single AnalyzeDocumentsChain.  from langchain.chains import AnalyzeDocumentChain  summarize_document_chain = AnalyzeDocumentChain(     combine_docs_chain=chain, text_splitter=text_splitter ) summarize_document_chain.run(docs[0].page_content)  ValueError: `run` not supported when there is not exactly one output key. Got ['output_text', 'intermediate_steps'].  Previous Extraction Next Tagging Use case Overview Quickstart Option 1. Stuff Go deeper Option 2. Map-Reduce Go deeper Option 3. Refine Splitting and summarizing in a single chain Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/qa_structured/sql",
    "title": "SQL | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data SQL Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A over structured dataSQL SQL  Open In Colab  Use case​  Enterprise data is often stored in SQL databases.  LLMs make it possible to interact with SQL databases using natural language.  LangChain offers SQL Chains and Agents to build and run SQL queries based on natural language prompts.  These are compatible with any SQL dialect supported by SQLAlchemy (e.g., MySQL, PostgreSQL, Oracle SQL, Databricks, SQLite).  They enable use cases such as:  Generating queries that will be run based on natural language questions Creating chatbots that can answer questions based on database data Building custom dashboards based on insights a user wants to analyze Overview​  LangChain provides tools to interact with SQL Databases:  Build SQL queries based on natural language user questions Query a SQL database using chains for query creation and execution Interact with a SQL database using agents for robust and flexible querying  Quickstart​  First, get required packages and set environment variables:  %pip install --upgrade --quiet  langchain langchain-experimental langchain-openai  # Set env var OPENAI_API_KEY or load from a .env file # import dotenv  # dotenv.load_dotenv()   The below example will use a SQLite connection with Chinook database.  Follow installation steps to create Chinook.db in the same directory as this notebook:  Save this file to the directory as Chinook_Sqlite.sql Run sqlite3 Chinook.db Run .read Chinook_Sqlite.sql Test SELECT * FROM Artist LIMIT 10;  Now, Chinhook.db is in our directory.  Let’s create a SQLDatabaseChain to create and execute SQL queries.  from langchain_community.utilities import SQLDatabase from langchain_experimental.sql import SQLDatabaseChain from langchain_openai import OpenAI  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)  db_chain.run(\"How many employees are there?\")    > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain.  'There are 8 employees.'   Note that this both creates and executes the query.  In the following sections, we will cover the 3 different use cases mentioned in the overview.  Go deeper​  You can load tabular data from other sources other than SQL Databases. For example: - Loading a CSV file - Loading a Pandas DataFrame Here you can check full list of Document Loaders  Case 1: Text-to-SQL query​ from langchain.chains import create_sql_query_chain from langchain_openai import ChatOpenAI   Let’s create the chain that will build the SQL Query:  chain = create_sql_query_chain(ChatOpenAI(temperature=0), db) response = chain.invoke({\"question\": \"How many employees are there\"}) print(response)  SELECT COUNT(*) FROM Employee   After building the SQL query based on a user question, we can execute the query:  db.run(response)  '[(8,)]'   As we can see, the SQL Query Builder chain only created the query, and we handled the query execution separately.  Go deeper​  Looking under the hood  We can look at the LangSmith trace to unpack this:  Some papers have reported good performance when prompting with:  A CREATE TABLE description for each table, which include column names, their types, etc Followed by three example rows in a SELECT statement  create_sql_query_chain adopts this the best practice (see more in this blog).   Improvements  The query builder can be improved in several ways, such as (but not limited to):  Customizing database description to your specific use case Hardcoding a few examples of questions and their corresponding SQL query in the prompt Using a vector database to include dynamic examples that are relevant to the specific user question  All these examples involve customizing the chain’s prompt.  For example, we can include a few examples in our prompt like so:  from langchain.prompts import PromptTemplate  TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Use the following format:  Question: \"Question here\" SQLQuery: \"SQL Query to run\" SQLResult: \"Result of the SQLQuery\" Answer: \"Final answer here\"  Only use the following tables:  {table_info}.  Some examples of SQL queries that correspond to questions are:  {few_shot_examples}  Question: {input}\"\"\"  CUSTOM_PROMPT = PromptTemplate(     input_variables=[\"input\", \"few_shot_examples\", \"table_info\", \"dialect\"],     template=TEMPLATE, )   We can also access this prompt in the LangChain prompt hub.  This will work with your LangSmith API key.  from langchain import hub  CUSTOM_PROMPT = hub.pull(\"rlm/text-to-sql\")  Case 2: Text-to-SQL query and execution​  We can use SQLDatabaseChain from langchain_experimental to create and run SQL queries.  from langchain_experimental.sql import SQLDatabaseChain from langchain_openai import OpenAI  llm = OpenAI(temperature=0, verbose=True) db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=True)  db_chain.run(\"How many employees are there?\")    > Entering new SQLDatabaseChain chain... How many employees are there? SQLQuery:SELECT COUNT(*) FROM \"Employee\"; SQLResult: [(8,)] Answer:There are 8 employees. > Finished chain.  'There are 8 employees.'   As we can see, we get the same result as the previous case.  Here, the chain also handles the query execution and provides a final answer based on the user question and the query result.  Be careful while using this approach as it is susceptible to SQL Injection:  The chain is executing queries that are created by an LLM, and weren’t validated e.g. records may be created, modified or deleted unintentionally_  This is why we see the SQLDatabaseChain is inside langchain_experimental.  Go deeper​  Looking under the hood  We can use the LangSmith trace to see what is happening under the hood:  As discussed above, first we create the query: text: ' SELECT COUNT(*) FROM \"Employee\";'  Then, it executes the query and passes the results to an LLM for synthesis.  Adding Sample Rows  Providing sample data can help the LLM construct correct queries when the data format is not obvious.  For example, we can tell LLM that artists are saved with their full names by providing two rows from the Track table.  db = SQLDatabase.from_uri(     \"sqlite:///Chinook.db\",     include_tables=[         \"Track\"     ],  # we include only one table to save tokens in the prompt :)     sample_rows_in_table_info=2, )   The sample rows are added to the prompt after each corresponding table’s column information.  We can use db.table_info and check which sample rows are included:  print(db.table_info)   CREATE TABLE \"Track\" (     \"TrackId\" INTEGER NOT NULL,      \"Name\" NVARCHAR(200) NOT NULL,      \"AlbumId\" INTEGER,      \"MediaTypeId\" INTEGER NOT NULL,      \"GenreId\" INTEGER,      \"Composer\" NVARCHAR(220),      \"Milliseconds\" INTEGER NOT NULL,      \"Bytes\" INTEGER,      \"UnitPrice\" NUMERIC(10, 2) NOT NULL,      PRIMARY KEY (\"TrackId\"),      FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"),      FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"),      FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") )  /* 2 rows from Track table: TrackId Name    AlbumId MediaTypeId GenreId Composer    Milliseconds    Bytes   UnitPrice 1   For Those About To Rock (We Salute You) 1   1   1   Angus Young, Malcolm Young, Brian Johnson   343719  11170334    0.99 2   Balls to the Wall   2   2   1   None    342562  5510424 0.99 */  Case 3: SQL agents​  LangChain has an SQL Agent which provides a more flexible way of interacting with SQL Databases than the SQLDatabaseChain.  The main advantages of using the SQL Agent are:  It can answer questions based on the databases’ schema as well as on the databases’ content (like describing a specific table) It can recover from errors by running a generated query, catching the traceback and regenerating it correctly  To initialize the agent, we use create_sql_agent function.  This agent contains the SQLDatabaseToolkit which contains tools to:  Create and execute queries Check query syntax Retrieve table descriptions … and more from langchain.agents import create_sql_agent  # from langchain.agents import AgentExecutor from langchain.agents.agent_types import AgentType from langchain_community.agent_toolkits import SQLDatabaseToolkit  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\")  agent_executor = create_sql_agent(     llm=OpenAI(temperature=0),     toolkit=SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0)),     verbose=True,     agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION, )  Agent task example #1 - Running queries​ agent_executor.run(     \"List the total sales per country. Which country's customers spent the most?\" )    > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input:  Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer Observation:  CREATE TABLE \"Customer\" (     \"CustomerId\" INTEGER NOT NULL,      \"FirstName\" NVARCHAR(40) NOT NULL,      \"LastName\" NVARCHAR(20) NOT NULL,      \"Company\" NVARCHAR(80),      \"Address\" NVARCHAR(70),      \"City\" NVARCHAR(40),      \"State\" NVARCHAR(40),      \"Country\" NVARCHAR(40),      \"PostalCode\" NVARCHAR(10),      \"Phone\" NVARCHAR(24),      \"Fax\" NVARCHAR(24),      \"Email\" NVARCHAR(60) NOT NULL,      \"SupportRepId\" INTEGER,      PRIMARY KEY (\"CustomerId\"),      FOREIGN KEY(\"SupportRepId\") REFERENCES \"Employee\" (\"EmployeeId\") )  /* 3 rows from Customer table: CustomerId  FirstName   LastName    Company Address City    State   Country PostalCode  Phone   Fax Email   SupportRepId 1   Luís    Gonçalves   Embraer - Empresa Brasileira de Aeronáutica S.A.    Av. Brigadeiro Faria Lima, 2170 São José dos Campos SP  Brazil  12227-000   +55 (12) 3923-5555  +55 (12) 3923-5566  luisg@embraer.com.br    3 2   Leonie  Köhler  None    Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   +49 0711 2842222    None    leonekohler@surfeu.de   5 3   François    Tremblay    None    1498 rue Bélanger   Montréal    QC  Canada  H2G 1A7 +1 (514) 721-4711   None    ftremblay@gmail.com 3 */   CREATE TABLE \"Invoice\" (     \"InvoiceId\" INTEGER NOT NULL,      \"CustomerId\" INTEGER NOT NULL,      \"InvoiceDate\" DATETIME NOT NULL,      \"BillingAddress\" NVARCHAR(70),      \"BillingCity\" NVARCHAR(40),      \"BillingState\" NVARCHAR(40),      \"BillingCountry\" NVARCHAR(40),      \"BillingPostalCode\" NVARCHAR(10),      \"Total\" NUMERIC(10, 2) NOT NULL,      PRIMARY KEY (\"InvoiceId\"),      FOREIGN KEY(\"CustomerId\") REFERENCES \"Customer\" (\"CustomerId\") )  /* 3 rows from Invoice table: InvoiceId   CustomerId  InvoiceDate BillingAddress  BillingCity BillingState    BillingCountry  BillingPostalCode   Total 1   2   2009-01-01 00:00:00 Theodor-Heuss-Straße 34 Stuttgart   None    Germany 70174   1.98 2   4   2009-01-02 00:00:00 Ullevålsveien 14    Oslo    None    Norway  0171    3.96 3   8   2009-01-03 00:00:00 Grétrystraat 63 Brussels    None    Belgium 1000    5.94 */ Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10 Observation: [('USA', 523.0600000000003), ('Canada', 303.9599999999999), ('France', 195.09999999999994), ('Brazil', 190.09999999999997), ('Germany', 156.48), ('United Kingdom', 112.85999999999999), ('Czech Republic', 90.24000000000001), ('Portugal', 77.23999999999998), ('India', 75.25999999999999), ('Chile', 46.62)] Thought: I now know the final answer Final Answer: The country with the highest total sales is the USA, with a total of $523.06.  > Finished chain.  'The country with the highest total sales is the USA, with a total of $523.06.'   Looking at the LangSmith trace, we can see:  The agent is using a ReAct style prompt First, it will look at the tables: Action: sql_db_list_tables using tool sql_db_list_tables Given the tables as an observation, it thinks and then determinates the next action: Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: I should query the schema of the Invoice and Customer tables. Action: sql_db_schema Action Input: Invoice, Customer  It then formulates the query using the schema from tool sql_db_schema Thought: I should query the total sales per country. Action: sql_db_query Action Input: SELECT Country, SUM(Total) AS TotalSales FROM Invoice INNER JOIN Customer ON Invoice.CustomerId = Customer.CustomerId GROUP BY Country ORDER BY TotalSales DESC LIMIT 10  It finally executes the generated query using tool sql_db_query  Agent task example #2 - Describing a Table​ agent_executor.run(\"Describe the playlisttrack table\")    > Entering new AgentExecutor chain... Action: sql_db_list_tables Action Input:  Observation: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Thought: The PlaylistTrack table is the most relevant to the question. Action: sql_db_schema Action Input: PlaylistTrack Observation:  CREATE TABLE \"PlaylistTrack\" (     \"PlaylistId\" INTEGER NOT NULL,      \"TrackId\" INTEGER NOT NULL,      PRIMARY KEY (\"PlaylistId\", \"TrackId\"),      FOREIGN KEY(\"TrackId\") REFERENCES \"Track\" (\"TrackId\"),      FOREIGN KEY(\"PlaylistId\") REFERENCES \"Playlist\" (\"PlaylistId\") )  /* 3 rows from PlaylistTrack table: PlaylistId  TrackId 1   3402 1   3389 1   3390 */ Thought: I now know the final answer Final Answer: The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.  > Finished chain.  'The PlaylistTrack table contains two columns, PlaylistId and TrackId, which are both integers and form a primary key. It also has two foreign keys, one to the Track table and one to the Playlist table.'  Extending the SQL Toolkit​  Although the out-of-the-box SQL Toolkit contains the necessary tools to start working on a database, it is often the case that some extra tools may be useful for extending the agent’s capabilities. This is particularly useful when trying to use domain specific knowledge in the solution, in order to improve its overall performance.  Some examples include:  Including dynamic few shot examples Finding misspellings in proper nouns to use as column filters  We can create separate tools which tackle these specific use cases and include them as a complement to the standard SQL Toolkit. Let’s see how to include these two custom tools.  Including dynamic few-shot examples​  In order to include dynamic few-shot examples, we need a custom Retriever Tool that handles the vector database in order to retrieve the examples that are semantically similar to the user’s question.  Let’s start by creating a dictionary with some examples:  few_shots = {     \"List all artists.\": \"SELECT * FROM artists;\",     \"Find all albums for the artist 'AC/DC'.\": \"SELECT * FROM albums WHERE ArtistId = (SELECT ArtistId FROM artists WHERE Name = 'AC/DC');\",     \"List all tracks in the 'Rock' genre.\": \"SELECT * FROM tracks WHERE GenreId = (SELECT GenreId FROM genres WHERE Name = 'Rock');\",     \"Find the total duration of all tracks.\": \"SELECT SUM(Milliseconds) FROM tracks;\",     \"List all customers from Canada.\": \"SELECT * FROM customers WHERE Country = 'Canada';\",     \"How many tracks are there in the album with ID 5?\": \"SELECT COUNT(*) FROM tracks WHERE AlbumId = 5;\",     \"Find the total number of invoices.\": \"SELECT COUNT(*) FROM invoices;\",     \"List all tracks that are longer than 5 minutes.\": \"SELECT * FROM tracks WHERE Milliseconds > 300000;\",     \"Who are the top 5 customers by total purchase?\": \"SELECT CustomerId, SUM(Total) AS TotalPurchase FROM invoices GROUP BY CustomerId ORDER BY TotalPurchase DESC LIMIT 5;\",     \"Which albums are from the year 2000?\": \"SELECT * FROM albums WHERE strftime('%Y', ReleaseDate) = '2000';\",     \"How many employees are there\": 'SELECT COUNT(*) FROM \"employee\"', }   We can then create a retriever using the list of questions, assigning the target SQL query as metadata:  from langchain.schema import Document from langchain_community.vectorstores import FAISS from langchain_openai import OpenAIEmbeddings  embeddings = OpenAIEmbeddings()  few_shot_docs = [     Document(page_content=question, metadata={\"sql_query\": few_shots[question]})     for question in few_shots.keys() ] vector_db = FAISS.from_documents(few_shot_docs, embeddings) retriever = vector_db.as_retriever()   Now we can create our own custom tool and append it as a new tool in the create_sql_agent function:  from langchain_community.agent_toolkits import create_retriever_tool  tool_description = \"\"\" This tool will help you understand similar examples to adapt them to the user question. Input to this tool should be the user question. \"\"\"  retriever_tool = create_retriever_tool(     retriever, name=\"sql_get_similar_examples\", description=tool_description ) custom_tool_list = [retriever_tool]   Now we can create the agent, adjusting the standard SQL Agent suffix to consider our use case. Although the most straightforward way to handle this would be to include it just in the tool description, this is often not enough and we need to specify it in the agent prompt using the suffix argument in the constructor.  from langchain.agents import AgentType, create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit from langchain_community.utilities import SQLDatabase from langchain_openai import ChatOpenAI  db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)  toolkit = SQLDatabaseToolkit(db=db, llm=llm)  custom_suffix = \"\"\" I should first get the similar examples I know. If the examples are enough to construct the query, I can build it. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\"  agent = create_sql_agent(     llm=llm,     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     extra_tools=custom_tool_list,     suffix=custom_suffix, )   Let’s try it out:  agent.run(\"How many employees do we have?\")    > Entering new AgentExecutor chain...  Invoking: `sql_get_similar_examples` with `How many employees do we have?`   [Document(page_content='How many employees are there', metadata={'sql_query': 'SELECT COUNT(*) FROM \"employee\"'}), Document(page_content='Find the total number of invoices.', metadata={'sql_query': 'SELECT COUNT(*) FROM invoices;'})] Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM employee` responded: {content}  SELECT COUNT(*) FROM employee Invoking: `sql_db_query` with `SELECT COUNT(*) FROM employee`   [(8,)]We have 8 employees.  > Finished chain.  'We have 8 employees.'   As we can see, the agent first used the sql_get_similar_examples tool in order to retrieve similar examples. As the question was very similar to other few shot examples, the agent didn’t need to use any other tool from the standard Toolkit, thus saving time and tokens.  Finding and correcting misspellings for proper nouns​  In order to filter columns that contain proper nouns such as addresses, song names or artists, we first need to double-check the spelling in order to filter the data correctly.  We can achieve this by creating a vector store using all the distinct proper nouns that exist in the database. We can then have the agent query that vector store each time the user includes a proper noun in their question, to find the correct spelling for that word. In this way, the agent can make sure it understands which entity the user is referring to before building the target query.  Let’s follow a similar approach to the few shots, but without metadata: just embedding the proper nouns and then querying to get the most similar one to the misspelled user question.  First we need the unique values for each entity we want, for which we define a function that parses the result into a list of elements:  import ast import re   def run_query_save_results(db, query):     res = db.run(query)     res = [el for sub in ast.literal_eval(res) for el in sub if el]     res = [re.sub(r\"\\b\\d+\\b\", \"\", string).strip() for string in res]     return res   artists = run_query_save_results(db, \"SELECT Name FROM Artist\") albums = run_query_save_results(db, \"SELECT Title FROM Album\")   Now we can proceed with creating the custom retriever tool and the final agent:  from langchain_community.agent_toolkits import create_retriever_tool from langchain_community.vectorstores import FAISS from langchain_openai import OpenAIEmbeddings  texts = artists + albums  embeddings = OpenAIEmbeddings() vector_db = FAISS.from_texts(texts, embeddings) retriever = vector_db.as_retriever()  retriever_tool = create_retriever_tool(     retriever,     name=\"name_search\",     description=\"use to learn how a piece of data is actually written, can be from names, surnames addresses etc\", )  custom_tool_list = [retriever_tool]  from langchain.agents import AgentType, create_sql_agent from langchain_community.agent_toolkits import SQLDatabaseToolkit from langchain_community.utilities import SQLDatabase from langchain_openai import ChatOpenAI  # db = SQLDatabase.from_uri(\"sqlite:///Chinook.db\") llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0)  toolkit = SQLDatabaseToolkit(db=db, llm=llm)  custom_suffix = \"\"\" If a user asks for me to filter based on proper nouns, I should first check the spelling using the name_search tool. Otherwise, I can then look at the tables in the database to see what I can query. Then I should query the schema of the most relevant tables \"\"\"  agent = create_sql_agent(     llm=llm,     toolkit=toolkit,     verbose=True,     agent_type=AgentType.OPENAI_FUNCTIONS,     extra_tools=custom_tool_list,     suffix=custom_suffix, )   Let’s try it out:  agent.run(\"How many albums does alis in pains have?\")    > Entering new AgentExecutor chain...  Invoking: `name_search` with `alis in pains`   [Document(page_content='House of Pain', metadata={}), Document(page_content='Alice In Chains', metadata={}), Document(page_content='Aisha Duo', metadata={}), Document(page_content='House Of Pain', metadata={})] Invoking: `sql_db_list_tables` with `` responded: {content}  Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track Invoking: `sql_db_schema` with `Album, Artist` responded: {content}   CREATE TABLE \"Album\" (     \"AlbumId\" INTEGER NOT NULL,      \"Title\" NVARCHAR(160) NOT NULL,      \"ArtistId\" INTEGER NOT NULL,      PRIMARY KEY (\"AlbumId\"),      FOREIGN KEY(\"ArtistId\") REFERENCES \"Artist\" (\"ArtistId\") )  /* 3 rows from Album table: AlbumId Title   ArtistId 1   For Those About To Rock We Salute You   1 2   Balls to the Wall   2 3   Restless and Wild   2 */   CREATE TABLE \"Artist\" (     \"ArtistId\" INTEGER NOT NULL,      \"Name\" NVARCHAR(120),      PRIMARY KEY (\"ArtistId\") )  /* 3 rows from Artist table: ArtistId    Name 1   AC/DC 2   Accept 3   Aerosmith */ Invoking: `sql_db_query_checker` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'` responded: {content}  SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains' Invoking: `sql_db_query` with `SELECT COUNT(*) FROM Album JOIN Artist ON Album.ArtistId = Artist.ArtistId WHERE Artist.Name = 'Alice In Chains'`   [(1,)]Alice In Chains has 1 album in the database.  > Finished chain.  'Alice In Chains has 1 album in the database.'   As we can see, the agent used the name_search tool in order to check how to correctly query the database for this specific artist.  Go deeper​  To learn more about the SQL Agent and how it works we refer to the SQL Agent Toolkit documentation.  You can also check Agents for other document types: - Pandas Agent - CSV Agent  Elastic Search​  Going beyond the above use-case, there are integrations with other databases.  For example, we can interact with Elasticsearch analytics database.  This chain builds search queries via the Elasticsearch DSL API (filters and aggregations).  The Elasticsearch client must have permissions for index listing, mapping description and search queries.  See here for instructions on how to run Elasticsearch locally.  Make sure to install the Elasticsearch Python client before:  pip install elasticsearch  from elasticsearch import Elasticsearch from langchain.chains.elasticsearch_database import ElasticsearchDatabaseChain from langchain_openai import ChatOpenAI  # Initialize Elasticsearch python client. # See https://elasticsearch-py.readthedocs.io/en/v8.8.2/api.html#elasticsearch.Elasticsearch ELASTIC_SEARCH_SERVER = \"https://elastic:pass@localhost:9200\" db = Elasticsearch(ELASTIC_SEARCH_SERVER)   Uncomment the next cell to initially populate your db.  # customers = [ #     {\"firstname\": \"Jennifer\", \"lastname\": \"Walters\"}, #     {\"firstname\": \"Monica\",\"lastname\":\"Rambeau\"}, #     {\"firstname\": \"Carol\",\"lastname\":\"Danvers\"}, #     {\"firstname\": \"Wanda\",\"lastname\":\"Maximoff\"}, #     {\"firstname\": \"Jennifer\",\"lastname\":\"Takeda\"}, # ] # for i, customer in enumerate(customers): #     db.create(index=\"customers\", document=customer, id=i)  llm = ChatOpenAI(model_name=\"gpt-4\", temperature=0) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, verbose=True)  question = \"What are the first names of all the customers?\" chain.run(question)   We can customize the prompt.  from langchain.prompts.prompt import PromptTemplate  PROMPT_TEMPLATE = \"\"\"Given an input question, create a syntactically correct Elasticsearch query to run. Unless the user specifies in their question a specific number of examples they wish to obtain, always limit your query to at most {top_k} results. You can order the results by a relevant column to return the most interesting examples in the database.  Unless told to do not query for all the columns from a specific index, only ask for a the few relevant columns given the question.  Pay attention to use only the column names that you can see in the mapping description. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which index. Return the query as valid json.  Use the following format:  Question: Question here ESQuery: Elasticsearch Query formatted as json \"\"\"  PROMPT = PromptTemplate.from_template(     PROMPT_TEMPLATE, ) chain = ElasticsearchDatabaseChain.from_llm(llm=llm, database=db, query_prompt=PROMPT)  Previous Using local models Next Interacting with APIs Use case Overview Quickstart Go deeper Case 1: Text-to-SQL query Go deeper Case 2: Text-to-SQL query and execution Go deeper Case 3: SQL agents Agent task example #1 - Running queries Agent task example #2 - Describing a Table Extending the SQL Toolkit Go deeper Elastic Search Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/document_transformers/",
    "title": "Text Splitters | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders Text Splitters HTMLHeaderTextSplitter Split by character Split code MarkdownHeaderTextSplitter Recursively split by character Semantic Chunking Split by tokens Text embedding models Vector stores Retrievers Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrievalText Splitters Text Splitters  Once you've loaded documents, you'll often want to transform them to better suit your application. The simplest example is you may want to split a long document into smaller chunks that can fit into your model's context window. LangChain has a number of built-in document transformers that make it easy to split, combine, filter, and otherwise manipulate documents.  When you want to deal with long pieces of text, it is necessary to split up that text into chunks. As simple as this sounds, there is a lot of potential complexity here. Ideally, you want to keep the semantically related pieces of text together. What \"semantically related\" means could depend on the type of text. This notebook showcases several ways to do that.  At a high level, text splitters work as following:  Split the text up into small, semantically meaningful chunks (often sentences). Start combining these small chunks into a larger chunk until you reach a certain size (as measured by some function). Once you reach that size, make that chunk its own piece of text and then start creating a new chunk of text with some overlap (to keep context between chunks).  That means there are two different axes along which you can customize your text splitter:  How the text is split How the chunk size is measured Types of Text Splitters​  LangChain offers many different types of text splitters. Below is a table listing all of them, along with a few characteristics:  Name: Name of the text splitter  Splits On: How this text splitter splits text  Adds Metadata: Whether or not this text splitter adds metadata about where each chunk came from.  Description: Description of the splitter, including recommendation on when to use it.  Name Splits On Adds Metadata Description Recursive A list of user defined characters  Recursively splits text. Splitting text recursively serves the purpose of trying to keep related pieces of text next to each other. This is the recommended way to start splitting text. HTML HTML specific characters ✅ Splits text based on HTML-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the HTML) Markdown Markdown specific characters ✅ Splits text based on Markdown-specific characters. Notably, this adds in relevant information about where that chunk came from (based on the Markdown) Code Code (Python, JS) specific characters  Splits text based on characters specific to coding languages. 15 different languages are available to choose from. Token Tokens  Splits text on tokens. There exist a few different ways to measure tokens. Character A user defined character  Splits text based on a user defined character. One of the simpler methods. [Experimental] Semantic Chunker Sentences  First splits on sentences. Then combines ones next to each other if they are semantically similar enough. Taken from Greg Kamradt Evaluate text splitters​  You can evaluate text splitters with the Chunkviz utility created by Greg Kamradt. Chunkviz is a great tool for visualizing how your text splitter is working. It will show you how your text is being split up and help in tuning up the splitting parameters.  Other Document Transforms​  Text splitting is only one example of transformations that you may want to do on documents before passing them to an LLM. Head to Integrations for documentation on built-in document transformer integrations with 3rd-party tools.  Previous PDF Next HTMLHeaderTextSplitter Types of Text Splitters Evaluate text splitters Other Document Transforms Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/model_io/chat",
    "title": "Chat Models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Concepts Prompts Quickstart LLMs Chat Models Quick Start Caching Streaming Tracking token usage Output Parsers Retrieval Agents Chains More LangServe LangSmith LangGraph ModulesModel I/OChat Models Chat Models  ChatModels are a core component of LangChain. LangChain does not serve its own ChatModels, but rather provides a standard interface for interacting with many different models. To be specific, this interface is one that takes as input a list of messages and returns a message.  There are lots of model providers (OpenAI, Cohere, Hugging Face, etc) - the ChatModel class is designed to provide a standard interface for all of them.  Quick Start​  Check out this quick start to get an overview of working with ChatModels, including all the different methods they expose  Integrations​  For a full list of all LLM integrations that LangChain provides, please go to the Integrations page  How-To Guides​  We have several how-to guides for more advanced usage of LLMs. This includes:  How to cache ChatModel responses How to stream responses from a ChatModel How to track token usage in a ChatModel call Previous Tracking token usage Next Quick Start Quick Start Integrations How-To Guides Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/chat_history",
    "title": "Add chat history | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGAdd chat history Add chat history  In many Q&A applications we want to allow the user to have a back-and-forth conversation, meaning the application needs some sort of “memory” of past questions and answers, and some logic for incorporating those into its current thinking.  In this guide we focus on adding logic for incorporating historical messages, and NOT on chat history management. Chat history management is covered here.  We’ll work off of the Q&A app we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Quickstart. We’ll need to update two things about our existing app:  Prompt: Update our prompt to support historical messages as an input. Contextualizing questions: Add a sub-chain that takes the latest user question and reformulates it in the context of the chat history. This is needed in case the latest question references some context from past messages. For example, if a user asks a follow-up question like “Can you elaborate on the second point?”, this cannot be understood without the context of the previous message. Therefore we can’t effectively perform retrieval with a question like this. Setup​ Dependencies​  We’ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.  We’ll use the following packages:  %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4   We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  import getpass import os  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  # import dotenv  # dotenv.load_dotenv()  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()  Chain without chat history​  Here is the Q&A app we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Quickstart:  import bs4 from langchain import hub from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import Chroma from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Load, chunk and index the contents of the blog. loader = WebBaseLoader(     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),     bs_kwargs=dict(         parse_only=bs4.SoupStrainer(             class_=(\"post-content\", \"post-title\", \"post-header\")         )     ), ) docs = loader.load()  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() prompt = hub.pull(\"rlm/rag-prompt\") llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)   def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | llm     | StrOutputParser() )  rag_chain.invoke(\"What is Task Decomposition?\")  'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.'  Contextualizing the question​  First we’ll need to define a sub-chain that takes historical messages and the latest user question, and reformulates the question if it makes reference to any information in the historical information.  We’ll use a prompt that includes a MessagesPlaceholder variable under the name “chat_history”. This allows us to pass in a list of Messages to the prompt using the “chat_history” input key, and these messages will be inserted after the system message and before the human message containing the latest question.  from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder  contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\ which might reference context in the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is.\"\"\" contextualize_q_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", contextualize_q_system_prompt),         MessagesPlaceholder(variable_name=\"chat_history\"),         (\"human\", \"{question}\"),     ] ) contextualize_q_chain = contextualize_q_prompt | llm | StrOutputParser()   Using this chain we can ask follow-up questions that reference past messages and have them reformulated into standalone questions:  from langchain_core.messages import AIMessage, HumanMessage  contextualize_q_chain.invoke(     {         \"chat_history\": [             HumanMessage(content=\"What does LLM stand for?\"),             AIMessage(content=\"Large language model\"),         ],         \"question\": \"What is meant by large\",     } )  'What is the definition of \"large\" in the context of a language model?'  Chain with chat history​  And now we can build our full QA chain.  Notice we add some routing functionality to only run the “condense question chain” when our chat history isn’t empty. Here we’re taking advantage of the fact that if a function in an LCEL chain returns another chain, that chain will itself be invoked.  qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don't know the answer, just say that you don't know. \\ Use three sentences maximum and keep the answer concise.\\  {context}\"\"\" qa_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", qa_system_prompt),         MessagesPlaceholder(variable_name=\"chat_history\"),         (\"human\", \"{question}\"),     ] )   def contextualized_question(input: dict):     if input.get(\"chat_history\"):         return contextualize_q_chain     else:         return input[\"question\"]   rag_chain = (     RunnablePassthrough.assign(         context=contextualized_question | retriever | format_docs     )     | qa_prompt     | llm )  chat_history = []  question = \"What is Task Decomposition?\" ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg])  second_question = \"What are common ways of doing it?\" rag_chain.invoke({\"question\": second_question, \"chat_history\": chat_history})  AIMessage(content='Common ways of task decomposition include:\\n\\n1. Using Chain of Thought (CoT): CoT is a prompting technique that instructs the model to \"think step by step\" and decompose complex tasks into smaller and simpler steps. This approach utilizes more computation at test-time and sheds light on the model\\'s thinking process.\\n\\n2. Prompting with LLM: Language Model (LLM) can be used to prompt the model with simple instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" This method guides the model to break down the task into manageable steps.\\n\\n3. Task-specific instructions: For certain tasks, task-specific instructions can be provided to guide the model in decomposing the task. For example, for writing a novel, the instruction \"Write a story outline\" can be given to help the model break down the task into smaller components.\\n\\n4. Human inputs: In some cases, human inputs can be used to assist in task decomposition. Humans can provide insights, expertise, and domain knowledge to help break down complex tasks into smaller subtasks.\\n\\nThese approaches aim to simplify complex tasks and enable more effective problem-solving and planning.')   Check out the LangSmith trace  Here we’ve gone over how to add application logic for incorporating historical outputs, but we’re still manually updating the chat history and inserting it into each input. In a real Q&A application we’ll want some way of persisting chat history and some way of automatically inserting and updating it.  For this we can use: - BaseChatMessageHistory: Store chat history. - RunnableWithMessageHistory: Wrapper for an LCEL chain and a BaseChatMessageHistory that handles injecting chat history into inputs and updating it after each invocation.  For a detailed walkthrough of how to use these classes together to create a stateful conversational chain, head to the How to add message history (memory) LCEL page.  Previous Returning sources Next Streaming Setup Dependencies LangSmith Chain without chat history Contextualizing the question Chain with chat history Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/graph/",
    "title": "Graph querying | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Diffbot Graph Transformer ArangoDB QA chain Neo4j DB QA chain FalkorDBQAChain HugeGraph QA Chain KuzuQAChain Memgraph QA chain NebulaGraphQAChain NetworkX GraphSparqlQAChain Neptune Open Cypher QA Chain Use casesGraph querying Graph querying  Graph databases give us a powerful way to represent and query real-world relationships. There are a number of chains that make it easy to use LLMs to interact with various graph DBs.  📄️ Diffbot Graph Transformer  Open In Colab  📄️ ArangoDB QA chain  Open In Colab  📄️ Neo4j DB QA chain  This notebook shows how to use LLMs to provide a natural language  📄️ FalkorDBQAChain  This notebook shows how to use LLMs to provide a natural language  📄️ HugeGraph QA Chain  This notebook shows how to use LLMs to provide a natural language  📄️ KuzuQAChain  This notebook shows how to use LLMs to provide a natural language  📄️ Memgraph QA chain  This notebook shows how to use LLMs to provide a natural language  📄️ NebulaGraphQAChain  This notebook shows how to use LLMs to provide a natural language  📄️ NetworkX  NetworkX is a Python package for the  📄️ GraphSparqlQAChain  Graph databases are an excellent choice for applications based on  📄️ Neptune Open Cypher QA Chain  This QA chain queries Neptune graph database using openCypher and  Previous Synthetic data generation Next Diffbot Graph Transformer Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/tagging",
    "title": "Tagging | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesTagging Tagging  Open In Colab  Use case​  Tagging means labeling a document with classes such as:  sentiment language style (formal, informal etc.) covered topics political tendency  Overview​  Tagging has a few components:  function: Like extraction, tagging uses functions to specify how the model should tag a document schema: defines how we want to tag the document Quickstart​  Let’s see a very straightforward example of how we can use OpenAI functions for tagging in LangChain.  %pip install --upgrade --quiet  langchain langchain-openai  # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()  from langchain.chains import create_tagging_chain, create_tagging_chain_pydantic from langchain_openai import ChatOpenAI   We specify a few properties with their expected type in our schema.  # Schema schema = {     \"properties\": {         \"sentiment\": {\"type\": \"string\"},         \"aggressiveness\": {\"type\": \"integer\"},         \"language\": {\"type\": \"string\"},     } }  # LLM llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\") chain = create_tagging_chain(schema, llm)  inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\" chain.run(inp)  {'sentiment': 'positive', 'language': 'Spanish'}  inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" chain.run(inp)  {'sentiment': 'enojado', 'aggressiveness': 1, 'language': 'es'}   As we can see in the examples, it correctly interprets what we want.  The results vary so that we get, for example, sentiments in different languages (‘positive’, ‘enojado’ etc.).  We will see how to control these results in the next section.  Finer control​  Careful schema definition gives us more control over the model’s output.  Specifically, we can define:  possible values for each property description to make sure that the model understands the property required properties to be returned  Here is an example of how we can use _enum_, _description_, and _required_ to control for each of the previously mentioned aspects:  schema = {     \"properties\": {         \"aggressiveness\": {             \"type\": \"integer\",             \"enum\": [1, 2, 3, 4, 5],             \"description\": \"describes how aggressive the statement is, the higher the number the more aggressive\",         },         \"language\": {             \"type\": \"string\",             \"enum\": [\"spanish\", \"english\", \"french\", \"german\", \"italian\"],         },     },     \"required\": [\"language\", \"sentiment\", \"aggressiveness\"], }  chain = create_tagging_chain(schema, llm)   Now the answers are much better!  inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\" chain.run(inp)  {'aggressiveness': 0, 'language': 'spanish'}  inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" chain.run(inp)  {'aggressiveness': 5, 'language': 'spanish'}  inp = \"Weather is ok here, I can go outside without much more than a coat\" chain.run(inp)  {'aggressiveness': 0, 'language': 'english'}   The LangSmith trace lets us peek under the hood:  As with extraction, we call the information_extraction function here on the input string. This OpenAI function extraction information based upon the provided schema.  Pydantic​  We can also use a Pydantic schema to specify the required properties and types.  We can also send other arguments, such as enum or description, to each field.  This lets us specify our schema in the same manner that we would a new class or function in Python with purely Pythonic types.  from langchain_core.pydantic_v1 import BaseModel, Field  class Tags(BaseModel):     sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\"])     aggressiveness: int = Field(         ...,         description=\"describes how aggressive the statement is, the higher the number the more aggressive\",         enum=[1, 2, 3, 4, 5],     )     language: str = Field(         ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]     )  chain = create_tagging_chain_pydantic(Tags, llm)  inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\" res = chain.run(inp)  res  Tags(sentiment='sad', aggressiveness=5, language='spanish')  Going deeper​ You can use the metadata tagger document transformer to extract metadata from a LangChain Document. This covers the same basic functionality as the tagging chain, only applied to a LangChain Document. Previous Summarization Next Web scraping Use case Overview Quickstart Finer control Pydantic Going deeper Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/retrievers/",
    "title": "Retrievers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders Text Splitters Text embedding models Vector stores Retrievers Vector store-backed retriever MultiQueryRetriever Contextual compression Ensemble Retriever Long-Context Reorder MultiVector Retriever Parent Document Retriever Self-querying Time-weighted vector store retriever Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrievalRetrievers Retrievers  A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Vector stores can be used as the backbone of a retriever, but there are other types of retrievers as well.  Retrievers accept a string query as input and return a list of Document's as output.  Advanced Retrieval Types​  LangChain provides several advanced retrieval types. A full list is below, along with the following information:  Name: Name of the retrieval algorithm.  Index Type: Which index type (if any) this relies on.  Uses an LLM: Whether this retrieval method uses an LLM.  When to Use: Our commentary on when you should considering using this retrieval method.  Description: Description of what this retrieval algorithm is doing.  Name Index Type Uses an LLM When to Use Description Vectorstore Vectorstore No If you are just getting started and looking for something quick and easy. This is the simplest method and the one that is easiest to get started with. It involves creating embeddings for each piece of text. ParentDocument Vectorstore + Document Store No If your pages have lots of smaller pieces of distinct information that are best indexed by themselves, but best retrieved all together. This involves indexing multiple chunks for each document. Then you find the chunks that are most similar in embedding space, but you retrieve the whole parent document and return that (rather than individual chunks). Multi Vector Vectorstore + Document Store Sometimes during indexing If you are able to extract information from documents that you think is more relevant to index than the text itself. This involves creating multiple vectors for each document. Each vector could be created in a myriad of ways - examples include summaries of the text and hypothetical questions. Self Query Vectorstore Yes If users are asking questions that are better answered by fetching documents based on metadata rather than similarity with the text. This uses an LLM to transform user input into two things: (1) a string to look up semantically, (2) a metadata filer to go along with it. This is useful because oftentimes questions are about the METADATA of documents (not the content itself). Contextual Compression Any Sometimes If you are finding that your retrieved documents contain too much irrelevant information and are distracting the LLM. This puts a post-processing step on top of another retriever and extracts only the most relevant information from retrieved documents. This can be done with embeddings or an LLM. Time-Weighted Vectorstore Vectorstore No If you have timestamps associated with your documents, and you want to retrieve the most recent ones This fetches documents based on a combination of semantic similarity (as in normal vector retrieval) and recency (looking at timestamps of indexed documents) Multi-Query Retriever Any Yes If users are asking questions that are complex and require multiple pieces of distinct information to respond This uses an LLM to generate multiple queries from the original one. This is useful when the original query needs pieces of information about multiple topics to be properly answered. By generating multiple queries, we can then fetch documents for each of them. Ensemble Any No If you have multiple retrieval methods and want to try combining them. This fetches documents from multiple retrievers and then combines them. Long-Context Reorder Any No If you are working with a long-context model and noticing that it's not paying attention to information in the middle of retrieved documents. This fetches documents from an underlying retriever, and then reorders them so that the most similar are near the beginning and end. This is useful because it's been shown that for longer context models they sometimes don't pay attention to information in the middle of the context window. Third Party Integrations​  LangChain also integrates with many third-party retrieval services. For a full list of these, check out this list of all integrations.  Using Retrievers in LCEL​  Since retrievers are Runnable's, we can easily compose them with other Runnable objects:  from langchain_openai import ChatOpenAI from langchain_core.prompts import ChatPromptTemplate from langchain.schema import StrOutputParser from langchain_core.runnables import RunnablePassthrough  template = \"\"\"Answer the question based only on the following context:  {context}  Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(template) model = ChatOpenAI()   def format_docs(docs):     return \"\\n\\n\".join([d.page_content for d in docs])   chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser() )  chain.invoke(\"What did the president say about technology?\")   Custom Retriever​  Since the retriever interface is so simple, it's pretty easy to write a custom one.  from langchain_core.retrievers import BaseRetriever from langchain_core.callbacks import CallbackManagerForRetrieverRun from langchain_core.documents import Document from typing import List   class CustomRetriever(BaseRetriever):          def _get_relevant_documents(         self, query: str, *, run_manager: CallbackManagerForRetrieverRun     ) -> List[Document]:         return [Document(page_content=query)]  retriever = CustomRetriever()  retriever.get_relevant_documents(\"bar\")  Previous Vector stores Next Vector store-backed retriever Advanced Retrieval Types Third Party Integrations Using Retrievers in LCEL Custom Retriever Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/chatbots",
    "title": "Chatbots | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesChatbots Chatbots  Open In Colab  Use case​  Chatbots are one of the central LLM use-cases. The core features of chatbots are that they can have long-running conversations and have access to information that users want to know about.  Aside from basic prompting and LLMs, memory and retrieval are the core components of a chatbot. Memory allows a chatbot to remember past interactions, and retrieval provides a chatbot with up-to-date, domain-specific information.  Overview​  The chat model interface is based around messages rather than raw text. Several components are important to consider for chat:  chat model: See here for a list of chat model integrations and here for documentation on the chat model interface in LangChain. You can use LLMs (see here) for chatbots as well, but chat models have a more conversational tone and natively support a message interface. prompt template: Prompt templates make it easy to assemble prompts that combine default messages, user input, chat history, and (optionally) additional retrieved context. memory: See here for in-depth documentation on memory types retriever (optional): See here for in-depth documentation on retrieval systems. These are useful if you want to build a chatbot with domain-specific knowledge. Quickstart​  Here’s a quick preview of how we can create chatbot interfaces. First let’s install some dependencies and set the required credentials:  %pip install --upgrade --quiet  langchain langchain-openai  # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()   With a plain chat model, we can get chat completions by passing one or more messages to the model.  The chat model will respond with a message.  from langchain.schema import HumanMessage, SystemMessage from langchain_openai import ChatOpenAI  chat = ChatOpenAI() chat(     [         HumanMessage(             content=\"Translate this sentence from English to French: I love programming.\"         )     ] )  AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)   And if we pass in a list of messages:  messages = [     SystemMessage(         content=\"You are a helpful assistant that translates English to French.\"     ),     HumanMessage(content=\"I love programming.\"), ] chat(messages)  AIMessage(content=\"J'adore la programmation.\", additional_kwargs={}, example=False)   We can then wrap our chat model in a ConversationChain, which has built-in memory for remembering past user inputs and model outputs.  from langchain.chains import ConversationChain  conversation = ConversationChain(llm=chat) conversation.run(\"Translate this sentence from English to French: I love programming.\")  'Je adore la programmation.'  conversation.run(\"Translate it to German.\")  'Ich liebe Programmieren.'  Memory​  As we mentioned above, the core component of chatbots is the memory system. One of the simplest and most commonly used forms of memory is ConversationBufferMemory:  This memory allows for storing of messages in a buffer When called in a chain, it returns all of the messages it has stored  LangChain comes with many other types of memory, too. See here for in-depth documentation on memory types.  For now let’s take a quick look at ConversationBufferMemory. We can manually add a few chat messages to the memory like so:  from langchain.memory import ConversationBufferMemory  memory = ConversationBufferMemory() memory.chat_memory.add_user_message(\"hi!\") memory.chat_memory.add_ai_message(\"whats up?\")   And now we can load from our memory. The key method exposed by all Memory classes is load_memory_variables. This takes in any initial chain input and returns a list of memory variables which are added to the chain input.  Since this simple memory type doesn’t actually take into account the chain input when loading memory, we can pass in an empty input for now:  memory.load_memory_variables({})  {'history': 'Human: hi!\\nAI: whats up?'}   We can also keep a sliding window of the most recent k interactions using ConversationBufferWindowMemory.  from langchain.memory import ConversationBufferWindowMemory  memory = ConversationBufferWindowMemory(k=1) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"}) memory.load_memory_variables({})  {'history': 'Human: not much you\\nAI: not much'}   ConversationSummaryMemory is an extension of this theme.  It creates a summary of the conversation over time.  This memory is most useful for longer conversations where the full message history would consume many tokens.  from langchain.memory import ConversationSummaryMemory from langchain_openai import OpenAI  llm = OpenAI(temperature=0) memory = ConversationSummaryMemory(llm=llm) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context(     {\"input\": \"im working on better docs for chatbots\"},     {\"output\": \"oh, that sounds like a lot of work\"}, ) memory.save_context(     {\"input\": \"yes, but it's worth the effort\"},     {\"output\": \"agreed, good docs are important!\"}, )  memory.load_memory_variables({})  {'history': '\\nThe human greets the AI, to which the AI responds. The human then mentions they are working on better docs for chatbots, to which the AI responds that it sounds like a lot of work. The human agrees that it is worth the effort, and the AI agrees that good docs are important.'}   ConversationSummaryBufferMemory extends this a bit further:  It uses token length rather than number of interactions to determine when to flush interactions.  from langchain.memory import ConversationSummaryBufferMemory  memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10) memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"}) memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})  Conversation​  We can unpack what goes under the hood with ConversationChain.  We can specify our memory, ConversationSummaryMemory and we can specify the prompt.  from langchain.chains import LLMChain from langchain.prompts import (     ChatPromptTemplate,     HumanMessagePromptTemplate,     MessagesPlaceholder,     SystemMessagePromptTemplate, )  # LLM llm = ChatOpenAI()  # Prompt prompt = ChatPromptTemplate(     messages=[         SystemMessagePromptTemplate.from_template(             \"You are a nice chatbot having a conversation with a human.\"         ),         # The `variable_name` here is what must align with memory         MessagesPlaceholder(variable_name=\"chat_history\"),         HumanMessagePromptTemplate.from_template(\"{question}\"),     ] )  # Notice that we `return_messages=True` to fit into the MessagesPlaceholder # Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True) conversation = LLMChain(llm=llm, prompt=prompt, verbose=True, memory=memory)  # Notice that we just pass in the `question` variables - `chat_history` gets populated by memory conversation({\"question\": \"hi\"})    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi  > Finished chain.  {'question': 'hi',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False)],  'text': 'Hello! How can I assist you today?'}  conversation(     {\"question\": \"Translate this sentence from English to French: I love programming.\"} )    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming.  > Finished chain.  {'question': 'Translate this sentence from English to French: I love programming.',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),   HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),   AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False)],  'text': 'Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"'}  conversation({\"question\": \"Now translate the sentence to German.\"})    > Entering new LLMChain chain... Prompt after formatting: System: You are a nice chatbot having a conversation with a human. Human: hi AI: Hello! How can I assist you today? Human: Translate this sentence from English to French: I love programming. AI: Sure! The translation of \"I love programming\" from English to French is \"J'adore programmer.\" Human: Now translate the sentence to German.  > Finished chain.  {'question': 'Now translate the sentence to German.',  'chat_history': [HumanMessage(content='hi', additional_kwargs={}, example=False),   AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}, example=False),   HumanMessage(content='Translate this sentence from English to French: I love programming.', additional_kwargs={}, example=False),   AIMessage(content='Sure! The translation of \"I love programming\" from English to French is \"J\\'adore programmer.\"', additional_kwargs={}, example=False),   HumanMessage(content='Now translate the sentence to German.', additional_kwargs={}, example=False),   AIMessage(content='Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"', additional_kwargs={}, example=False)],  'text': 'Certainly! The translation of \"I love programming\" from English to German is \"Ich liebe das Programmieren.\"'}   We can see the chat history preserved in the prompt using the LangSmith trace.  Chat Retrieval​  Now, suppose we want to chat with documents or some other source of knowledge.  This is popular use case, combining chat with document retrieval.  It allows us to chat with specific information that the model was not trained on.  %pip install --upgrade --quiet  tiktoken chromadb   Load a blog post.  from langchain_community.document_loaders import WebBaseLoader  loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\") data = loader.load()   Split and store this in a vector.  from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data)  from langchain_community.vectorstores import Chroma from langchain_openai import OpenAIEmbeddings  vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())   Create our memory, as before, but’s let’s use ConversationSummaryMemory.  memory = ConversationSummaryMemory(     llm=llm, memory_key=\"chat_history\", return_messages=True )  from langchain.chains import ConversationalRetrievalChain from langchain_openai import ChatOpenAI  llm = ChatOpenAI() retriever = vectorstore.as_retriever() qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)  qa(\"How do agents use Task decomposition?\")  {'question': 'How do agents use Task decomposition?',  'chat_history': [SystemMessage(content='', additional_kwargs={})],  'answer': 'Agents can use task decomposition in several ways:\\n\\n1. Simple prompting: Agents can use Language Model based prompting to break down tasks into subgoals. For example, by providing prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\", the agent can generate a sequence of smaller steps that lead to the completion of the overall task.\\n\\n2. Task-specific instructions: Agents can be given task-specific instructions to guide their planning process. For example, if the task is to write a novel, the agent can be instructed to \"Write a story outline.\" This provides a high-level structure for the task and helps in breaking it down into smaller components.\\n\\n3. Human inputs: Agents can also take inputs from humans to decompose tasks. This can be done through direct communication or by leveraging human expertise. Humans can provide guidance and insights to help the agent break down complex tasks into manageable subgoals.\\n\\nOverall, task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.'}  qa(\"What are the various ways to implement memory to support it?\")  {'question': 'What are the various ways to implement memory to support it?',  'chat_history': [SystemMessage(content='The human asks how agents use task decomposition. The AI explains that agents can use task decomposition in several ways, including simple prompting, task-specific instructions, and human inputs. Task decomposition allows agents to break down large tasks into smaller, more manageable subgoals, enabling them to plan and execute complex tasks efficiently.', additional_kwargs={})],  'answer': 'There are several ways to implement memory to support task decomposition:\\n\\n1. Long-Term Memory Management: This involves storing and organizing information in a long-term memory system. The agent can retrieve past experiences, knowledge, and learned strategies to guide the task decomposition process.\\n\\n2. Internet Access: The agent can use internet access to search for relevant information and gather resources to aid in task decomposition. This allows the agent to access a vast amount of information and utilize it in the decomposition process.\\n\\n3. GPT-3.5 Powered Agents: The agent can delegate simple tasks to GPT-3.5 powered agents. These agents can perform specific tasks or provide assistance in task decomposition, allowing the main agent to focus on higher-level planning and decision-making.\\n\\n4. File Output: The agent can store the results of task decomposition in files or documents. This allows for easy retrieval and reference during the execution of the task.\\n\\nThese memory resources help the agent in organizing and managing information, making informed decisions, and effectively decomposing complex tasks into smaller, manageable subgoals.'}   Again, we can use the LangSmith trace to explore the prompt structure.  Going deeper​ Agents, such as the conversational retrieval agent, can be used for retrieval when necessary while also holding a conversation. Previous Interacting with APIs Next Extraction Use case Overview Quickstart Memory Conversation Chat Retrieval Going deeper Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/code_understanding",
    "title": "Code understanding | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesCode understanding Code understanding  Open In Collab  Use case​  Source code analysis is one of the most popular LLM applications (e.g., GitHub Co-Pilot, Code Interpreter, Codium, and Codeium) for use-cases such as:  Q&A over the code base to understand how it works Using LLMs for suggesting refactors or improvements Using LLMs for documenting the code  Overview​  The pipeline for QA over code follows the steps we do for document question answering, with some differences:  In particular, we can employ a splitting strategy that does a few things:  Keeps each top-level function and class in the code is loaded into separate documents. Puts remaining into a separate document. Retains metadata about where each split comes from Quickstart​ %pip install --upgrade --quiet  langchain-openai tiktoken chromadb langchain  # Set env var OPENAI_API_KEY or load from a .env file # import dotenv  # dotenv.load_dotenv()   We’ll follow the structure of this notebook and employ context aware code splitting.  Loading​  We will upload all python project files using the langchain_community.document_loaders.TextLoader.  The following script iterates over the files in the LangChain repository and loads every .py file (a.k.a. documents):  # from git import Repo from langchain.text_splitter import Language from langchain_community.document_loaders.generic import GenericLoader from langchain_community.document_loaders.parsers import LanguageParser  # Clone repo_path = \"/Users/rlm/Desktop/test_repo\" # repo = Repo.clone_from(\"https://github.com/langchain-ai/langchain\", to_path=repo_path)   We load the py code using LanguageParser, which will:  Keep top-level functions and classes together (into a single document) Put remaining code into a separate document Retains metadata about where each split comes from # Load loader = GenericLoader.from_filesystem(     repo_path + \"/libs/langchain/langchain\",     glob=\"**/*\",     suffixes=[\".py\"],     exclude=[\"**/non-utf8-encoding.py\"],     parser=LanguageParser(language=Language.PYTHON, parser_threshold=500), ) documents = loader.load() len(documents)  1293  Splitting​  Split the Document into chunks for embedding and vector storage.  We can use RecursiveCharacterTextSplitter w/ language specified.  from langchain.text_splitter import RecursiveCharacterTextSplitter  python_splitter = RecursiveCharacterTextSplitter.from_language(     language=Language.PYTHON, chunk_size=2000, chunk_overlap=200 ) texts = python_splitter.split_documents(documents) len(texts)  3748  RetrievalQA​  We need to store the documents in a way we can semantically search for their content.  The most common approach is to embed the contents of each document then store the embedding and document in a vector store.  When setting up the vectorstore retriever:  We test max marginal relevance for retrieval And 8 documents returned Go deeper​ Browse the > 40 vectorstores integrations here. See further documentation on vectorstores here. Browse the > 30 text embedding integrations here. See further documentation on embedding models here. from langchain_community.vectorstores import Chroma from langchain_openai import OpenAIEmbeddings  db = Chroma.from_documents(texts, OpenAIEmbeddings(disallowed_special=())) retriever = db.as_retriever(     search_type=\"mmr\",  # Also test \"similarity\"     search_kwargs={\"k\": 8}, )  Chat​  Test chat, just as we do for chatbots.  Go deeper​ Browse the > 55 LLM and chat model integrations here. See further documentation on LLMs and chat models here. Use local LLMS: The popularity of PrivateGPT and GPT4All underscore the importance of running LLMs locally. from langchain.chains import ConversationalRetrievalChain from langchain.memory import ConversationSummaryMemory from langchain_openai import ChatOpenAI  llm = ChatOpenAI(model_name=\"gpt-4\") memory = ConversationSummaryMemory(     llm=llm, memory_key=\"chat_history\", return_messages=True ) qa = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)  question = \"How can I initialize a ReAct agent?\" result = qa(question) result[\"answer\"]  'To initialize a ReAct agent, you need to follow these steps:\\n\\n1. Initialize a language model `llm` of type `BaseLanguageModel`.\\n\\n2. Initialize a document store `docstore` of type `Docstore`.\\n\\n3. Create a `DocstoreExplorer` with the initialized `docstore`. The `DocstoreExplorer` is used to search for and look up terms in the document store.\\n\\n4. Create an array of `Tool` objects. The `Tool` objects represent the actions that the agent can perform. In the case of `ReActDocstoreAgent`, the tools must be \"Search\" and \"Lookup\" with their corresponding functions from the `DocstoreExplorer`.\\n\\n5. Initialize the `ReActDocstoreAgent` using the `from_llm_and_tools` method with the `llm` (language model) and `tools` as parameters.\\n\\n6. Initialize the `ReActChain` (which is the `AgentExecutor`) using the `ReActDocstoreAgent` and `tools` as parameters.\\n\\nHere is an example of how to do this:\\n\\n```python\\nfrom langchain.chains import ReActChain, OpenAI\\nfrom langchain.docstore.base import Docstore\\nfrom langchain.docstore.document import Document\\nfrom langchain_core.tools import BaseTool\\n\\n# Initialize the LLM and a docstore\\nllm = OpenAI()\\ndocstore = Docstore()\\n\\ndocstore_explorer = DocstoreExplorer(docstore)\\ntools = [\\n    Tool(\\n        name=\"Search\",\\n        func=docstore_explorer.search,\\n        description=\"Search for a term in the docstore.\",\\n    ),\\n    Tool(\\n        name=\"Lookup\",\\n        func=docstore_explorer.lookup,\\n        description=\"Lookup a term in the docstore.\",\\n    ),\\n]\\nagent = ReActDocstoreAgent.from_llm_and_tools(llm, tools)\\nreact = ReActChain(agent=agent, tools=tools)\\n```\\n\\nKeep in mind that this is a simplified example and you might need to adapt it to your specific needs.'  questions = [     \"What is the class hierarchy?\",     \"What classes are derived from the Chain class?\",     \"What one improvement do you propose in code in relation to the class hierarchy for the Chain class?\", ]  for question in questions:     result = qa(question)     print(f\"-> **Question**: {question} \\n\")     print(f\"**Answer**: {result['answer']} \\n\")  -> **Question**: What is the class hierarchy?   **Answer**: The class hierarchy in object-oriented programming is the structure that forms when classes are derived from other classes. The derived class is a subclass of the base class also known as the superclass. This hierarchy is formed based on the concept of inheritance in object-oriented programming where a subclass inherits the properties and functionalities of the superclass.   In the given context, we have the following examples of class hierarchies:  1. `BaseCallbackHandler --> <name>CallbackHandler` means `BaseCallbackHandler` is a base class and `<name>CallbackHandler` (like `AimCallbackHandler`, `ArgillaCallbackHandler` etc.) are derived classes that inherit from `BaseCallbackHandler`.  2. `BaseLoader --> <name>Loader` means `BaseLoader` is a base class and `<name>Loader` (like `TextLoader`, `UnstructuredFileLoader` etc.) are derived classes that inherit from `BaseLoader`.  3. `ToolMetaclass --> BaseTool --> <name>Tool` means `ToolMetaclass` is a base class, `BaseTool` is a derived class that inherits from `ToolMetaclass`, and `<name>Tool` (like `AIPluginTool`, `BaseGraphQLTool` etc.) are further derived classes that inherit from `BaseTool`.   -> **Question**: What classes are derived from the Chain class?   **Answer**: The classes that are derived from the Chain class are:  1. LLMSummarizationCheckerChain 2. MapReduceChain 3. OpenAIModerationChain 4. NatBotChain 5. QAGenerationChain 6. QAWithSourcesChain 7. RetrievalQAWithSourcesChain 8. VectorDBQAWithSourcesChain 9. RetrievalQA 10. VectorDBQA 11. LLMRouterChain 12. MultiPromptChain 13. MultiRetrievalQAChain 14. MultiRouteChain 15. RouterChain 16. SequentialChain 17. SimpleSequentialChain 18. TransformChain 19. BaseConversationalRetrievalChain 20. ConstitutionalChain   -> **Question**: What one improvement do you propose in code in relation to the class hierarchy for the Chain class?   **Answer**: As an AI model, I don't have personal opinions. However, one suggestion could be to improve the documentation of the Chain class hierarchy. The current comments and docstrings provide some details but it could be helpful to include more explicit explanations about the hierarchy, roles of each subclass, and their relationships with one another. Also, incorporating UML diagrams or other visuals could help developers better understand the structure and interactions of the classes.    The can look at the LangSmith trace to see what is happening under the hood:  In particular, the code well structured and kept together in the retrieval output The retrieved code and chat history are passed to the LLM for answer distillation  Open source LLMs​  We can use Code LLaMA via LLamaCPP or Ollama integration.  Note: be sure to upgrade llama-cpp-python in order to use the new gguf file format.  CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama2/bin/pip install -U llama-cpp-python --no-cache-dir   Check out the latest code-llama models here.  from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain.chains import ConversationalRetrievalChain, LLMChain from langchain.memory import ConversationSummaryMemory from langchain.prompts import PromptTemplate from langchain_community.llms import LlamaCpp  callback_manager = CallbackManager([StreamingStdOutCallbackHandler()]) llm = LlamaCpp(     model_path=\"/Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf\",     n_ctx=5000,     n_gpu_layers=1,     n_batch=512,     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls     callback_manager=callback_manager,     verbose=True, )  llama_model_loader: loaded meta data with 17 key-value pairs and 363 tensors from /Users/rlm/Desktop/Code/llama/code-llama/codellama-13b-instruct.Q4_K_M.gguf (version GGUF V1 (latest)) llama_model_loader: - tensor    0:                token_embd.weight q4_0     [  5120, 32016,     1,     1 ] llama_model_loader: - tensor    1:               output_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor    2:                    output.weight f16      [  5120, 32016,     1,     1 ] llama_model_loader: - tensor    3:              blk.0.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    4:              blk.0.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    5:              blk.0.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    6:         blk.0.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor    7:            blk.0.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor    8:            blk.0.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor    9:              blk.0.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   10:           blk.0.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   11:            blk.0.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   12:              blk.1.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   13:              blk.1.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   14:              blk.1.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   15:         blk.1.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   16:            blk.1.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   17:            blk.1.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   18:              blk.1.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   19:           blk.1.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   20:            blk.1.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   21:              blk.2.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   22:              blk.2.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   23:              blk.2.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   24:         blk.2.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   25:            blk.2.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   26:            blk.2.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   27:              blk.2.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   28:           blk.2.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   29:            blk.2.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   30:              blk.3.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   31:              blk.3.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   32:              blk.3.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   33:         blk.3.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   34:            blk.3.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   35:            blk.3.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   36:              blk.3.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   37:           blk.3.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   38:            blk.3.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   39:              blk.4.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   40:              blk.4.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   41:              blk.4.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   42:         blk.4.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   43:            blk.4.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   44:            blk.4.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   45:              blk.4.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   46:           blk.4.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   47:            blk.4.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   48:              blk.5.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   49:              blk.5.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   50:              blk.5.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   51:         blk.5.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   52:            blk.5.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   53:            blk.5.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   54:              blk.5.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   55:           blk.5.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   56:            blk.5.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   57:              blk.6.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   58:              blk.6.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   59:              blk.6.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   60:         blk.6.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   61:            blk.6.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   62:            blk.6.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   63:              blk.6.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   64:           blk.6.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   65:            blk.6.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   66:              blk.7.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   67:              blk.7.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   68:              blk.7.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   69:         blk.7.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   70:            blk.7.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   71:            blk.7.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   72:              blk.7.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   73:           blk.7.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   74:            blk.7.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   75:              blk.8.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   76:              blk.8.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   77:              blk.8.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   78:         blk.8.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   79:            blk.8.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   80:            blk.8.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   81:              blk.8.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   82:           blk.8.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   83:            blk.8.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   84:              blk.9.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   85:              blk.9.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   86:              blk.9.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   87:         blk.9.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   88:            blk.9.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   89:            blk.9.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   90:              blk.9.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   91:           blk.9.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   92:            blk.9.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor   93:             blk.10.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   94:             blk.10.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   95:             blk.10.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   96:        blk.10.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor   97:           blk.10.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor   98:           blk.10.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor   99:             blk.10.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  100:          blk.10.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  101:           blk.10.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  102:             blk.11.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  103:             blk.11.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  104:             blk.11.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  105:        blk.11.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  106:           blk.11.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  107:           blk.11.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  108:             blk.11.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  109:          blk.11.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  110:           blk.11.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  111:             blk.12.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  112:             blk.12.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  113:             blk.12.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  114:        blk.12.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  115:           blk.12.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  116:           blk.12.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  117:             blk.12.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  118:          blk.12.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  119:           blk.12.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  120:             blk.13.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  121:             blk.13.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  122:             blk.13.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  123:        blk.13.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  124:           blk.13.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  125:           blk.13.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  126:             blk.13.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  127:          blk.13.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  128:           blk.13.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  129:             blk.14.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  130:             blk.14.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  131:             blk.14.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  132:        blk.14.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  133:           blk.14.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  134:           blk.14.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  135:             blk.14.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  136:          blk.14.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  137:           blk.14.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  138:             blk.15.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  139:             blk.15.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  140:             blk.15.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  141:        blk.15.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  142:           blk.15.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  143:           blk.15.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  144:             blk.15.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  145:          blk.15.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  146:           blk.15.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  147:             blk.16.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  148:             blk.16.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  149:             blk.16.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  150:        blk.16.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  151:           blk.16.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  152:           blk.16.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  153:             blk.16.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  154:          blk.16.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  155:           blk.16.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  156:             blk.17.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  157:             blk.17.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  158:             blk.17.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  159:        blk.17.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  160:           blk.17.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  161:           blk.17.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  162:             blk.17.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  163:          blk.17.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  164:           blk.17.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  165:             blk.18.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  166:             blk.18.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  167:             blk.18.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  168:        blk.18.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  169:           blk.18.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  170:           blk.18.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  171:             blk.18.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  172:          blk.18.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  173:           blk.18.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  174:             blk.19.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  175:             blk.19.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  176:             blk.19.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  177:        blk.19.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  178:           blk.19.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  179:           blk.19.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  180:             blk.19.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  181:          blk.19.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  182:           blk.19.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  183:             blk.20.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  184:             blk.20.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  185:             blk.20.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  186:        blk.20.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  187:           blk.20.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  188:           blk.20.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  189:             blk.20.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  190:          blk.20.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  191:           blk.20.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  192:             blk.21.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  193:             blk.21.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  194:             blk.21.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  195:        blk.21.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  196:           blk.21.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  197:           blk.21.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  198:             blk.21.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  199:          blk.21.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  200:           blk.21.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  201:             blk.22.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  202:             blk.22.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  203:             blk.22.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  204:        blk.22.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  205:           blk.22.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  206:           blk.22.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  207:             blk.22.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  208:          blk.22.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  209:           blk.22.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  210:             blk.23.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  211:             blk.23.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  212:             blk.23.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  213:        blk.23.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  214:           blk.23.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  215:           blk.23.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  216:             blk.23.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  217:          blk.23.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  218:           blk.23.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  219:             blk.24.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  220:             blk.24.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  221:             blk.24.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  222:        blk.24.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  223:           blk.24.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  224:           blk.24.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  225:             blk.24.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  226:          blk.24.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  227:           blk.24.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  228:             blk.25.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  229:             blk.25.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  230:             blk.25.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  231:        blk.25.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  232:           blk.25.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  233:           blk.25.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  234:             blk.25.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  235:          blk.25.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  236:           blk.25.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  237:             blk.26.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  238:             blk.26.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  239:             blk.26.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  240:        blk.26.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  241:           blk.26.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  242:           blk.26.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  243:             blk.26.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  244:          blk.26.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  245:           blk.26.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  246:             blk.27.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  247:             blk.27.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  248:             blk.27.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  249:        blk.27.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  250:           blk.27.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  251:           blk.27.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  252:             blk.27.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  253:          blk.27.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  254:           blk.27.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  255:             blk.28.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  256:             blk.28.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  257:             blk.28.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  258:        blk.28.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  259:           blk.28.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  260:           blk.28.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  261:             blk.28.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  262:          blk.28.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  263:           blk.28.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  264:             blk.29.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  265:             blk.29.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  266:             blk.29.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  267:        blk.29.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  268:           blk.29.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  269:           blk.29.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  270:             blk.29.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  271:          blk.29.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  272:           blk.29.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  273:             blk.30.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  274:             blk.30.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  275:             blk.30.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  276:        blk.30.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  277:           blk.30.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  278:           blk.30.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  279:             blk.30.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  280:          blk.30.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  281:           blk.30.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  282:             blk.31.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  283:             blk.31.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  284:             blk.31.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  285:        blk.31.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  286:           blk.31.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  287:           blk.31.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  288:             blk.31.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  289:          blk.31.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  290:           blk.31.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  291:             blk.32.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  292:             blk.32.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  293:             blk.32.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  294:        blk.32.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  295:           blk.32.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  296:           blk.32.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  297:             blk.32.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  298:          blk.32.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  299:           blk.32.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  300:             blk.33.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  301:             blk.33.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  302:             blk.33.attn_v.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  303:        blk.33.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  304:           blk.33.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  305:           blk.33.ffn_down.weight q4_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  306:             blk.33.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  307:          blk.33.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  308:           blk.33.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  309:             blk.34.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  310:             blk.34.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  311:             blk.34.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  312:        blk.34.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  313:           blk.34.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  314:           blk.34.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  315:             blk.34.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  316:          blk.34.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  317:           blk.34.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  318:             blk.35.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  319:             blk.35.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  320:             blk.35.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  321:        blk.35.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  322:           blk.35.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  323:           blk.35.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  324:             blk.35.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  325:          blk.35.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  326:           blk.35.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  327:             blk.36.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  328:             blk.36.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  329:             blk.36.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  330:        blk.36.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  331:           blk.36.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  332:           blk.36.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  333:             blk.36.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  334:          blk.36.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  335:           blk.36.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  336:             blk.37.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  337:             blk.37.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  338:             blk.37.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  339:        blk.37.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  340:           blk.37.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  341:           blk.37.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  342:             blk.37.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  343:          blk.37.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  344:           blk.37.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  345:             blk.38.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  346:             blk.38.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  347:             blk.38.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  348:        blk.38.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  349:           blk.38.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  350:           blk.38.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  351:             blk.38.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  352:          blk.38.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  353:           blk.38.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  354:             blk.39.attn_q.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  355:             blk.39.attn_k.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  356:             blk.39.attn_v.weight q6_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  357:        blk.39.attn_output.weight q4_K     [  5120,  5120,     1,     1 ] llama_model_loader: - tensor  358:           blk.39.ffn_gate.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  359:           blk.39.ffn_down.weight q6_K     [ 13824,  5120,     1,     1 ] llama_model_loader: - tensor  360:             blk.39.ffn_up.weight q4_K     [  5120, 13824,     1,     1 ] llama_model_loader: - tensor  361:          blk.39.attn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - tensor  362:           blk.39.ffn_norm.weight f32      [  5120,     1,     1,     1 ] llama_model_loader: - kv   0:                       general.architecture str      llama_model_loader: - kv   1:                               general.name str      llama_model_loader: - kv   2:                       llama.context_length u32      llama_model_loader: - kv   3:                     llama.embedding_length u32      llama_model_loader: - kv   4:                          llama.block_count u32      llama_model_loader: - kv   5:                  llama.feed_forward_length u32      llama_model_loader: - kv   6:                 llama.rope.dimension_count u32      llama_model_loader: - kv   7:                 llama.attention.head_count u32      llama_model_loader: - kv   8:              llama.attention.head_count_kv u32      llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32      llama_model_loader: - kv  10:                       llama.rope.freq_base f32      llama_model_loader: - kv  11:                          general.file_type u32      llama_model_loader: - kv  12:                       tokenizer.ggml.model str      llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr      llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr      llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr      llama_model_loader: - kv  16:               general.quantization_version u32      llama_model_loader: - type  f32:   81 tensors llama_model_loader: - type  f16:    1 tensors llama_model_loader: - type q4_0:    1 tensors llama_model_loader: - type q4_K:  240 tensors llama_model_loader: - type q6_K:   40 tensors llm_load_print_meta: format         = GGUF V1 (latest) llm_load_print_meta: arch           = llama llm_load_print_meta: vocab type     = SPM llm_load_print_meta: n_vocab        = 32016 llm_load_print_meta: n_merges       = 0 llm_load_print_meta: n_ctx_train    = 16384 llm_load_print_meta: n_ctx          = 5000 llm_load_print_meta: n_embd         = 5120 llm_load_print_meta: n_head         = 40 llm_load_print_meta: n_head_kv      = 40 llm_load_print_meta: n_layer        = 40 llm_load_print_meta: n_rot          = 128 llm_load_print_meta: n_gqa          = 1 llm_load_print_meta: f_norm_eps     = 1.0e-05 llm_load_print_meta: f_norm_rms_eps = 1.0e-05 llm_load_print_meta: n_ff           = 13824 llm_load_print_meta: freq_base      = 1000000.0 llm_load_print_meta: freq_scale     = 1 llm_load_print_meta: model type     = 13B llm_load_print_meta: model ftype    = mostly Q4_K - Medium llm_load_print_meta: model size     = 13.02 B llm_load_print_meta: general.name   = LLaMA llm_load_print_meta: BOS token = 1 '<s>' llm_load_print_meta: EOS token = 2 '</s>' llm_load_print_meta: UNK token = 0 '<unk>' llm_load_print_meta: LF token  = 13 '<0x0A>' llm_load_tensors: ggml ctx size =    0.11 MB llm_load_tensors: mem required  = 7685.49 MB (+ 3906.25 MB per state) ................................................................................................. llama_new_context_with_model: kv self size  = 3906.25 MB ggml_metal_init: allocating ggml_metal_init: loading '/Users/rlm/miniforge3/envs/llama2/lib/python3.9/site-packages/llama_cpp/ggml-metal.metal' ggml_metal_init: loaded kernel_add                            0x12126dd00 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_add_row                        0x12126d610 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul                            0x12126f2a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_row                        0x12126f500 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_scale                          0x12126f760 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_silu                           0x12126fe40 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_relu                           0x1212700a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_gelu                           0x121270300 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_soft_max                       0x121270560 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_diag_mask_inf                  0x1212707c0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_f16                   0x121270a20 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_0                  0x121270c80 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_1                  0x121270ee0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q8_0                  0x121271140 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q2_K                  0x1212713a0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q3_K                  0x121271600 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q4_K                  0x121271860 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q5_K                  0x121271ac0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_get_rows_q6_K                  0x121271d20 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_rms_norm                       0x121271f80 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_norm                           0x1212721e0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_f16_f32                0x121272440 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_0_f32               0x1212726a0 | th_max =  896 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_1_f32               0x121272900 | th_max =  896 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q8_0_f32               0x121272b60 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q2_K_f32               0x121272dc0 | th_max =  640 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q3_K_f32               0x121273020 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q4_K_f32               0x121273280 | th_max =  576 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q5_K_f32               0x1212734e0 | th_max =  576 | th_width =   32 ggml_metal_init: loaded kernel_mul_mat_q6_K_f32               0x121273740 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_f16_f32                 0x1212739a0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                0x121273c00 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                0x121273e60 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                0x1212740c0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                0x121274320 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                0x121274580 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                0x1212747e0 | th_max =  768 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                0x121274a40 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                0x121274ca0 | th_max =  704 | th_width =   32 ggml_metal_init: loaded kernel_rope                           0x121274f00 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_alibi_f32                      0x121275160 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f32_f16                    0x1212753c0 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f32_f32                    0x121275620 | th_max = 1024 | th_width =   32 ggml_metal_init: loaded kernel_cpy_f16_f16                    0x121275880 | th_max = 1024 | th_width =   32 ggml_metal_init: recommendedMaxWorkingSetSize  = 21845.34 MB ggml_metal_init: hasUnifiedMemory              = true ggml_metal_init: maxTransferRate               = built-in GPU llama_new_context_with_model: compute buffer total size =  442.03 MB llama_new_context_with_model: max tensor size =   312.66 MB ggml_metal_add_buffer: allocated 'data            ' buffer, size =  7686.00 MB, (20243.77 / 21845.34) ggml_metal_add_buffer: allocated 'eval            ' buffer, size =     1.42 MB, (20245.19 / 21845.34) ggml_metal_add_buffer: allocated 'kv              ' buffer, size =  3908.25 MB, (24153.44 / 21845.34), warning: current allocated size is greater than the recommended max working set size AVX = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | VSX = 0 |  ggml_metal_add_buffer: allocated 'alloc           ' buffer, size =   440.64 MB, (24594.08 / 21845.34), warning: current allocated size is greater than the recommended max working set size  llm(     \"Question: In bash, how do I list all the text files in the current directory that have been modified in the last month? Answer:\" )  Llama.generate: prefix-match hit  llama_print_timings:        load time =  1074.43 ms llama_print_timings:      sample time =   180.71 ms /   256 runs   (    0.71 ms per token,  1416.67 tokens per second) llama_print_timings: prompt eval time =     0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second) llama_print_timings:        eval time =  9593.04 ms /   256 runs   (   37.47 ms per token,    26.69 tokens per second) llama_print_timings:       total time = 10139.91 ms   You can use the find command with a few options to this task. Here is an example of how you might go about it:  find . -type f -mtime +28 -exec ls {} \\; This command only for plain files (not), and limits the search to files that were more than 28 days ago, then the \"ls\" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find.  You can also use find in with other unix utilities like sort and grep to the list of files before they are:  find . -type f -mtime +28 | sort | grep pattern This will find all plain files that match a given pattern, then sort the listically and filter it for only the matches.  Answer: `find` is pretty with its search. The should work as well:  \\begin{code} ls -l $(find . -mtime +28) \\end{code}  (It's a bad idea to parse output from `ls`, though, as you may  ' You can use the find command with a few options to this task. Here is an example of how you might go about it:\\n\\nfind . -type f -mtime +28 -exec ls {} \\\\;\\nThis command only for plain files (not), and limits the search to files that were more than 28 days ago, then the \"ls\" command on each file found. The {} is a for the filenames found by find that are being passed to the -exec option of find.\\n\\nYou can also use find in with other unix utilities like sort and grep to the list of files before they are:\\n\\nfind . -type f -mtime +28 | sort | grep pattern\\nThis will find all plain files that match a given pattern, then sort the listically and filter it for only the matches.\\n\\nAnswer: `find` is pretty with its search. The should work as well:\\n\\n\\\\begin{code}\\nls -l $(find . -mtime +28)\\n\\\\end{code}\\n\\n(It\\'s a bad idea to parse output from `ls`, though, as you may'  from langchain.chains.question_answering import load_qa_chain  # Prompt template = \"\"\"Use the following pieces of context to answer the question at the end.  If you don't know the answer, just say that you don't know, don't try to make up an answer.  Use three sentences maximum and keep the answer as concise as possible.  {context} Question: {question} Helpful Answer:\"\"\" QA_CHAIN_PROMPT = PromptTemplate(     input_variables=[\"context\", \"question\"],     template=template, )   We can also use the LangChain Prompt Hub to store and fetch prompts.  This will work with your LangSmith API key.  Let’s try with a default RAG prompt, here.  from langchain import hub  QA_CHAIN_PROMPT = hub.pull(\"rlm/rag-prompt-default\")  # Docs question = \"How can I initialize a ReAct agent?\" docs = retriever.get_relevant_documents(question)  # Chain chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_CHAIN_PROMPT)  # Run chain({\"input_documents\": docs, \"question\": question}, return_only_outputs=True)  Llama.generate: prefix-match hit  llama_print_timings:        load time =  1074.43 ms llama_print_timings:      sample time =    65.46 ms /    94 runs   (    0.70 ms per token,  1435.95 tokens per second) llama_print_timings: prompt eval time = 15975.57 ms /  1408 tokens (   11.35 ms per token,    88.13 tokens per second) llama_print_timings:        eval time =  4772.57 ms /    93 runs   (   51.32 ms per token,    19.49 tokens per second) llama_print_timings:       total time = 20959.57 ms   You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool: ```python from langchain.agents.react import ReActAgent from langchain_community.tools.lookup import Lookup from langchain_community.tools.search import Search ReActAgent(Lookup(), Search()) ```  {'output_text': ' You can use the `ReActAgent` class and pass it the desired tools as, for example, you would do like this to create an agent with the `Lookup` and `Search` tool:\\n```python\\nfrom langchain.agents.react import ReActAgent\\nfrom langchain_community.tools.lookup import Lookup\\nfrom langchain_community.tools.search import Search\\nReActAgent(Lookup(), Search())\\n```'}   Here’s the trace RAG, showing the retrieved docs.  Previous Tool error handling Next Synthetic data generation Use case Overview Quickstart Loading Splitting RetrievalQA Chat Open source LLMs Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/apis",
    "title": "Interacting with APIs | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesInteracting with APIs Interacting with APIs  Open In Colab  Use case​  Suppose you want an LLM to interact with external APIs.  This can be very useful for retrieving context for the LLM to utilize.  And, more generally, it allows us to interact with APIs using natural language!  Overview​  There are two primary ways to interface LLMs with external APIs:  Functions: For example, OpenAI functions is one popular means of doing this. LLM-generated interface: Use an LLM with access to API documentation to create an interface.  Quickstart​  Many APIs are already compatible with OpenAI function calling.  For example, Klarna has a YAML file that describes its API and allows OpenAI to interact with it:  https://www.klarna.com/us/shopping/public/openai/v0/api-docs/   Other options include:  Speak for translation XKCD for comics  We can supply the specification to get_openapi_chain directly in order to query the API with OpenAI functions:  pip install langchain langchain-openai   # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()  from langchain.chains.openai_functions.openapi import get_openapi_chain  chain = get_openapi_chain(     \"https://www.klarna.com/us/shopping/public/openai/v0/api-docs/\" ) chain(\"What are some options for a men's large blue button down shirt\")  Attempting to load an OpenAPI 3.0.1 spec.  This may result in degraded performance. Convert your OpenAPI spec to 3.1.* spec for better support.  {'query': \"What are some options for a men's large blue button down shirt\",  'response': {'products': [{'name': 'Cubavera Four Pocket Guayabera Shirt',     'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202055522/Clothing/Cubavera-Four-Pocket-Guayabera-Shirt/?utm_source=openai&ref-site=openai_plugin',     'price': '$13.50',     'attributes': ['Material:Polyester,Cotton',      'Target Group:Man',      'Color:Red,White,Blue,Black',      'Properties:Pockets',      'Pattern:Solid Color',      'Size (Small-Large):S,XL,L,M,XXL']},    {'name': 'Polo Ralph Lauren Plaid Short Sleeve Button-down Oxford Shirt',     'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3207163438/Clothing/Polo-Ralph-Lauren-Plaid-Short-Sleeve-Button-down-Oxford-Shirt/?utm_source=openai&ref-site=openai_plugin',     'price': '$52.20',     'attributes': ['Material:Cotton',      'Target Group:Man',      'Color:Red,Blue,Multicolor',      'Size (Small-Large):S,XL,L,M,XXL']},    {'name': 'Brixton Bowery Flannel Shirt',     'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3202331096/Clothing/Brixton-Bowery-Flannel-Shirt/?utm_source=openai&ref-site=openai_plugin',     'price': '$27.48',     'attributes': ['Material:Cotton',      'Target Group:Man',      'Color:Gray,Blue,Black,Orange',      'Properties:Pockets',      'Pattern:Checkered',      'Size (Small-Large):XL,3XL,4XL,5XL,L,M,XXL']},    {'name': 'Vineyard Vines Gingham On-The-Go brrr Classic Fit Shirt Crystal',     'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201938510/Clothing/Vineyard-Vines-Gingham-On-The-Go-brrr-Classic-Fit-Shirt-Crystal/?utm_source=openai&ref-site=openai_plugin',     'price': '$80.64',     'attributes': ['Material:Cotton',      'Target Group:Man',      'Color:Blue',      'Size (Small-Large):XL,XS,L,M']},    {'name': \"Carhartt Men's Loose Fit Midweight Short Sleeve Plaid Shirt\",     'url': 'https://www.klarna.com/us/shopping/pl/cl10001/3201826024/Clothing/Carhartt-Men-s-Loose-Fit-Midweight-Short-Sleeve-Plaid-Shirt/?utm_source=openai&ref-site=openai_plugin',     'price': '$17.99',     'attributes': ['Material:Cotton',      'Target Group:Man',      'Color:Red,Brown,Blue,Green',      'Properties:Pockets',      'Pattern:Checkered',      'Size (Small-Large):S,XL,L,M']}]}}  Functions​  We can unpack what is happening when we use the functions to call external APIs.  Let’s look at the LangSmith trace:  See here that we call the OpenAI LLM with the provided API spec: https://www.klarna.com/us/shopping/public/openai/v0/api-docs/  The prompt then tells the LLM to use the API spec with input question: Use the provided APIs to respond to this user query: What are some options for a men's large blue button down shirt  The LLM returns the parameters for the function call productsUsingGET, which is specified in the provided API spec: function_call:   name: productsUsingGET   arguments: |-     {       \"params\": {         \"countryCode\": \"US\",         \"q\": \"men's large blue button down shirt\",         \"size\": 5,         \"min_price\": 0,         \"max_price\": 100       }     }   This Dict above split and the API is called here. API Chain​  We can also build our own interface to external APIs using the APIChain and provided API documentation.  from langchain.chains import APIChain from langchain.chains.api import open_meteo_docs from langchain_openai import OpenAI  llm = OpenAI(temperature=0) chain = APIChain.from_llm_and_api_docs(     llm,     open_meteo_docs.OPEN_METEO_DOCS,     verbose=True,     limit_to_domains=[\"https://api.open-meteo.com/\"], ) chain.run(     \"What is the weather like right now in Munich, Germany in degrees Fahrenheit?\" )    > Entering new APIChain chain... https://api.open-meteo.com/v1/forecast?latitude=48.1351&longitude=11.5820&hourly=temperature_2m&temperature_unit=fahrenheit&current_weather=true {\"latitude\":48.14,\"longitude\":11.58,\"generationtime_ms\":0.1710653305053711,\"utc_offset_seconds\":0,\"timezone\":\"GMT\",\"timezone_abbreviation\":\"GMT\",\"elevation\":521.0,\"current_weather_units\":{\"time\":\"iso8601\",\"interval\":\"seconds\",\"temperature\":\"°F\",\"windspeed\":\"km/h\",\"winddirection\":\"°\",\"is_day\":\"\",\"weathercode\":\"wmo code\"},\"current_weather\":{\"time\":\"2023-11-01T21:30\",\"interval\":900,\"temperature\":46.5,\"windspeed\":7.7,\"winddirection\":259,\"is_day\":0,\"weathercode\":3},\"hourly_units\":{\"time\":\"iso8601\",\"temperature_2m\":\"°F\"},\"hourly\":{\"time\":[\"2023-11-01T00:00\",\"2023-11-01T01:00\",\"2023-11-01T02:00\",\"2023-11-01T03:00\",\"2023-11-01T04:00\",\"2023-11-01T05:00\",\"2023-11-01T06:00\",\"2023-11-01T07:00\",\"2023-11-01T08:00\",\"2023-11-01T09:00\",\"2023-11-01T10:00\",\"2023-11-01T11:00\",\"2023-11-01T12:00\",\"2023-11-01T13:00\",\"2023-11-01T14:00\",\"2023-11-01T15:00\",\"2023-11-01T16:00\",\"2023-11-01T17:00\",\"2023-11-01T18:00\",\"2023-11-01T19:00\",\"2023-11-01T20:00\",\"2023-11-01T21:00\",\"2023-11-01T22:00\",\"2023-11-01T23:00\",\"2023-11-02T00:00\",\"2023-11-02T01:00\",\"2023-11-02T02:00\",\"2023-11-02T03:00\",\"2023-11-02T04:00\",\"2023-11-02T05:00\",\"2023-11-02T06:00\",\"2023-11-02T07:00\",\"2023-11-02T08:00\",\"2023-11-02T09:00\",\"2023-11-02T10:00\",\"2023-11-02T11:00\",\"2023-11-02T12:00\",\"2023-11-02T13:00\",\"2023-11-02T14:00\",\"2023-11-02T15:00\",\"2023-11-02T16:00\",\"2023-11-02T17:00\",\"2023-11-02T18:00\",\"2023-11-02T19:00\",\"2023-11-02T20:00\",\"2023-11-02T21:00\",\"2023-11-02T22:00\",\"2023-11-02T23:00\",\"2023-11-03T00:00\",\"2023-11-03T01:00\",\"2023-11-03T02:00\",\"2023-11-03T03:00\",\"2023-11-03T04:00\",\"2023-11-03T05:00\",\"2023-11-03T06:00\",\"2023-11-03T07:00\",\"2023-11-03T08:00\",\"2023-11-03T09:00\",\"2023-11-03T10:00\",\"2023-11-03T11:00\",\"2023-11-03T12:00\",\"2023-11-03T13:00\",\"2023-11-03T14:00\",\"2023-11-03T15:00\",\"2023-11-03T16:00\",\"2023-11-03T17:00\",\"2023-11-03T18:00\",\"2023-11-03T19:00\",\"2023-11-03T20:00\",\"2023-11-03T21:00\",\"2023-11-03T22:00\",\"2023-11-03T23:00\",\"2023-11-04T00:00\",\"2023-11-04T01:00\",\"2023-11-04T02:00\",\"2023-11-04T03:00\",\"2023-11-04T04:00\",\"2023-11-04T05:00\",\"2023-11-04T06:00\",\"2023-11-04T07:00\",\"2023-11-04T08:00\",\"2023-11-04T09:00\",\"2023-11-04T10:00\",\"2023-11-04T11:00\",\"2023-11-04T12:00\",\"2023-11-04T13:00\",\"2023-11-04T14:00\",\"2023-11-04T15:00\",\"2023-11-04T16:00\",\"2023-11-04T17:00\",\"2023-11-04T18:00\",\"2023-11-04T19:00\",\"2023-11-04T20:00\",\"2023-11-04T21:00\",\"2023-11-04T22:00\",\"2023-11-04T23:00\",\"2023-11-05T00:00\",\"2023-11-05T01:00\",\"2023-11-05T02:00\",\"2023-11-05T03:00\",\"2023-11-05T04:00\",\"2023-11-05T05:00\",\"2023-11-05T06:00\",\"2023-11-05T07:00\",\"2023-11-05T08:00\",\"2023-11-05T09:00\",\"2023-11-05T10:00\",\"2023-11-05T11:00\",\"2023-11-05T12:00\",\"2023-11-05T13:00\",\"2023-11-05T14:00\",\"2023-11-05T15:00\",\"2023-11-05T16:00\",\"2023-11-05T17:00\",\"2023-11-05T18:00\",\"2023-11-05T19:00\",\"2023-11-05T20:00\",\"2023-11-05T21:00\",\"2023-11-05T22:00\",\"2023-11-05T23:00\",\"2023-11-06T00:00\",\"2023-11-06T01:00\",\"2023-11-06T02:00\",\"2023-11-06T03:00\",\"2023-11-06T04:00\",\"2023-11-06T05:00\",\"2023-11-06T06:00\",\"2023-11-06T07:00\",\"2023-11-06T08:00\",\"2023-11-06T09:00\",\"2023-11-06T10:00\",\"2023-11-06T11:00\",\"2023-11-06T12:00\",\"2023-11-06T13:00\",\"2023-11-06T14:00\",\"2023-11-06T15:00\",\"2023-11-06T16:00\",\"2023-11-06T17:00\",\"2023-11-06T18:00\",\"2023-11-06T19:00\",\"2023-11-06T20:00\",\"2023-11-06T21:00\",\"2023-11-06T22:00\",\"2023-11-06T23:00\",\"2023-11-07T00:00\",\"2023-11-07T01:00\",\"2023-11-07T02:00\",\"2023-11-07T03:00\",\"2023-11-07T04:00\",\"2023-11-07T05:00\",\"2023-11-07T06:00\",\"2023-11-07T07:00\",\"2023-11-07T08:00\",\"2023-11-07T09:00\",\"2023-11-07T10:00\",\"2023-11-07T11:00\",\"2023-11-07T12:00\",\"2023-11-07T13:00\",\"2023-11-07T14:00\",\"2023-11-07T15:00\",\"2023-11-07T16:00\",\"2023-11-07T17:00\",\"2023-11-07T18:00\",\"2023-11-07T19:00\",\"2023-11-07T20:00\",\"2023-11-07T21:00\",\"2023-11-07T22:00\",\"2023-11-07T23:00\"],\"temperature_2m\":[47.9,46.9,47.1,46.6,45.8,45.2,43.4,43.5,46.8,51.5,55.0,56.3,58.1,57.9,57.0,56.6,54.4,52.1,49.1,48.3,47.7,46.9,46.2,45.8,44.4,42.4,41.7,41.7,42.0,42.7,43.6,44.3,45.9,48.0,49.1,50.7,52.2,52.6,51.9,50.3,48.1,47.4,47.1,46.9,46.2,45.7,45.6,45.6,45.7,45.3,45.1,44.2,43.6,43.2,42.8,41.6,41.0,42.1,42.4,42.3,42.7,43.9,44.2,43.6,41.9,40.4,39.0,40.8,40.2,40.1,39.6,38.8,38.2,36.9,35.8,36.4,37.3,38.5,38.9,39.0,41.8,45.4,48.7,50.8,51.7,52.1,51.3,49.8,48.6,47.8,47.0,46.3,45.9,45.6,45.7,46.1,46.3,46.4,46.3,46.3,45.8,45.4,45.5,47.1,49.3,51.2,52.4,53.1,53.5,53.4,53.0,52.4,51.6,50.5,49.6,49.0,48.6,48.1,47.6,47.0,46.4,46.0,45.5,45.1,44.4,43.7,43.9,45.6,48.1,50.3,51.7,52.8,53.5,52.7,51.5,50.2,48.8,47.4,46.2,45.5,45.0,44.6,44.3,44.2,43.9,43.4,43.0,42.6,42.3,42.0,42.2,43.0,44.3,45.5,46.8,48.1,48.9,49.0,48.7,48.1,47.4,46.5,45.7,45.1,44.5,44.3,44.5,45.1]}}  > Finished chain.  ' The current temperature in Munich, Germany is 46.5°F.'   Note that we supply information about the API:  open_meteo_docs.OPEN_METEO_DOCS[0:500]  'BASE URL: https://api.open-meteo.com/\\n\\nAPI Documentation\\nThe API endpoint /v1/forecast accepts a geographical coordinate, a list of weather variables and responds with a JSON hourly weather forecast for 7 days. Time always starts at 0:00 today and contains 168 hours. All URL parameters are listed below:\\n\\nParameter\\tFormat\\tRequired\\tDefault\\tDescription\\nlatitude, longitude\\tFloating point\\tYes\\t\\tGeographical WGS84 coordinate of the location\\nhourly\\tString array\\tNo\\t\\tA list of weather variables which shou'   Under the hood, we do two things:  api_request_chain: Generate an API URL based on the input question and the api_docs api_answer_chain: generate a final answer based on the API response  We can look at the LangSmith trace to inspect this:  The api_request_chain produces the API url from our question and the API documentation:  Here we make the API request with the API url. The api_answer_chain takes the response from the API and provides us with a natural language response:  Going deeper​  Test with other APIs  import os  os.environ[\"TMDB_BEARER_TOKEN\"] = \"\" from langchain.chains.api import tmdb_docs  headers = {\"Authorization\": f\"Bearer {os.environ['TMDB_BEARER_TOKEN']}\"} chain = APIChain.from_llm_and_api_docs(     llm,     tmdb_docs.TMDB_DOCS,     headers=headers,     verbose=True,     limit_to_domains=[\"https://api.themoviedb.org/\"], ) chain.run(\"Search for 'Avatar'\")  import os  from langchain.chains import APIChain from langchain.chains.api import podcast_docs from langchain_openai import OpenAI  listen_api_key = \"xxx\"  # Get api key here: https://www.listennotes.com/api/pricing/ llm = OpenAI(temperature=0) headers = {\"X-ListenAPI-Key\": listen_api_key} chain = APIChain.from_llm_and_api_docs(     llm,     podcast_docs.PODCAST_DOCS,     headers=headers,     verbose=True,     limit_to_domains=[\"https://listen-api.listennotes.com/\"], ) chain.run(     \"Search for 'silicon valley bank' podcast episodes, audio length is more than 30 minutes, return only 1 results\" )   Web requests  URL requests are such a common use-case that we have the LLMRequestsChain, which makes an HTTP GET request.  from langchain.chains import LLMChain, LLMRequestsChain from langchain.prompts import PromptTemplate from langchain_openai import OpenAI  template = \"\"\"Between >>> and <<< are the raw search result text from google. Extract the answer to the question '{query}' or say \"not found\" if the information is not contained. Use the format Extracted:<answer or \"not found\"> >>> {requests_result} <<< Extracted:\"\"\"  PROMPT = PromptTemplate(     input_variables=[\"query\", \"requests_result\"],     template=template, )  chain = LLMRequestsChain(llm_chain=LLMChain(llm=OpenAI(temperature=0), prompt=PROMPT)) question = \"What are the Three (3) biggest countries, and their respective sizes?\" inputs = {     \"query\": question,     \"url\": \"https://www.google.com/search?q=\" + question.replace(\" \", \"+\"), } chain(inputs)  {'query': 'What are the Three (3) biggest countries, and their respective sizes?',  'url': 'https://www.google.com/search?q=What+are+the+Three+(3)+biggest+countries,+and+their+respective+sizes?',  'output': ' Russia (17,098,242 km²), Canada (9,984,670 km²), China (9,706,961 km²)'}  Previous SQL Next Chatbots Use case Overview Quickstart Functions API Chain Going deeper Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/model_io/llms/",
    "title": "LLMs | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Concepts Prompts Quickstart LLMs Quick Start Custom LLM Caching Streaming Tracking token usage Chat Models Output Parsers Retrieval Agents Chains More LangServe LangSmith LangGraph ModulesModel I/OLLMs LLMs  Large Language Models (LLMs) are a core component of LangChain. LangChain does not serve its own LLMs, but rather provides a standard interface for interacting with many different LLMs. To be specific, this interface is one that takes as input a string and returns a string.  There are lots of LLM providers (OpenAI, Cohere, Hugging Face, etc) - the LLM class is designed to provide a standard interface for all of them.  Quick Start​  Check out this quick start to get an overview of working with LLMs, including all the different methods they expose  Integrations​  For a full list of all LLM integrations that LangChain provides, please go to the Integrations page  How-To Guides​  We have several how-to guides for more advanced usage of LLMs. This includes:  How to write a custom LLM class How to cache LLM responses How to stream responses from an LLM [How to track token usage in an LLM call)(./token_usage_tracking) Previous Quickstart Next Quick Start Quick Start Integrations How-To Guides Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/local_retrieval_qa",
    "title": "Using local models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGUsing local models Using local models  The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the importance of running LLMs locally.  LangChain has integrations with many open-source LLMs that can be run locally.  See here for setup instructions for these LLMs.  For example, here we show how to run GPT4All or LLaMA2 locally (e.g., on your laptop) using local embeddings and a local LLM.  Document Loading​  First, install packages needed for local embeddings and vector storage.  %pip install --upgrade --quiet  langchain langchain-community langchainhub gpt4all chromadb    Load and split an example document.  We’ll use a blog post on agents as an example.  from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader  loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\") data = loader.load()  text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data)   Next, the below steps will download the GPT4All embeddings locally (if you don’t already have them).  from langchain_community.embeddings import GPT4AllEmbeddings from langchain_community.vectorstores import Chroma  vectorstore = Chroma.from_documents(documents=all_splits, embedding=GPT4AllEmbeddings())   Test similarity search is working with our local embeddings.  question = \"What are the approaches to Task Decomposition?\" docs = vectorstore.similarity_search(question) len(docs)  4  docs[0]  Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})  Model​ LLaMA2​  Note: new versions of llama-cpp-python use GGUF model files (see here).  If you have an existing GGML model, see here for instructions for conversion for GGUF.  And / or, you can download a GGUF converted model (e.g., here).  Finally, as noted in detail here install llama-cpp-python  %pip install --upgrade --quiet  llama-cpp-python   To enable use of GPU on Apple Silicon, follow the steps here to use the Python binding with Metal support.  In particular, ensure that conda is using the correct virtual environment that you created (miniforge3).  E.g., for me:  conda activate /Users/rlm/miniforge3/envs/llama   With this confirmed:  ! CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 /Users/rlm/miniforge3/envs/llama/bin/pip install -U llama-cpp-python --no-cache-dir  from langchain_community.llms import LlamaCpp   Setting model parameters as noted in the llama.cpp docs.  n_gpu_layers = 1  # Metal set to 1 is enough. n_batch = 512  # Should be between 1 and n_ctx, consider the amount of RAM of your Apple Silicon Chip.  # Make sure the model path is correct for your system! llm = LlamaCpp(     model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/llama-2-13b-chat.ggufv3.q4_0.bin\",     n_gpu_layers=n_gpu_layers,     n_batch=n_batch,     n_ctx=2048,     f16_kv=True,  # MUST set to True, otherwise you will run into problem after a couple of calls     verbose=True, )   Note that these indicate that Metal was enabled properly:  ggml_metal_init: allocating ggml_metal_init: using MPS  llm.invoke(\"Simulate a rap battle between Stephen Colbert and John Oliver\")  Llama.generate: prefix-match hit  llama_print_timings:        load time =  4481.74 ms llama_print_timings:      sample time =   183.05 ms /   256 runs   (    0.72 ms per token,  1398.53 tokens per second) llama_print_timings: prompt eval time =   456.05 ms /    13 tokens (   35.08 ms per token,    28.51 tokens per second) llama_print_timings:        eval time =  7375.20 ms /   255 runs   (   28.92 ms per token,    34.58 tokens per second) llama_print_timings:       total time =  8388.92 ms  by jonathan   Here's the hypothetical rap battle:  [Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I'm here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let's see who gets the most laughs while taking shots at each other  [John Oliver]: Yo, this is John Oliver, known for my own comedy show. I'm here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the '90s, but it's time to see who can out-rap whom  [Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I'm America's funnyman! Who's the one taking you? Nobody!  [John Oliver]: Hey Stephen Colbert, don't get too cocky. You may  \"by jonathan \\n\\nHere's the hypothetical rap battle:\\n\\n[Stephen Colbert]: Yo, this is Stephen Colbert, known for my comedy show. I'm here to put some sense in your mind, like an enema do-go. Your opponent? A man of laughter and witty quips, John Oliver! Now let's see who gets the most laughs while taking shots at each other\\n\\n[John Oliver]: Yo, this is John Oliver, known for my own comedy show. I'm here to take your mind on an adventure through wit and humor. But first, allow me to you to our contestant: Stephen Colbert! His show has been around since the '90s, but it's time to see who can out-rap whom\\n\\n[Stephen Colbert]: You claim to be a witty man, John Oliver, with your British charm and clever remarks. But my knows that I'm America's funnyman! Who's the one taking you? Nobody!\\n\\n[John Oliver]: Hey Stephen Colbert, don't get too cocky. You may\"  GPT4All​  Similarly, we can use GPT4All.  Download the GPT4All model binary.  The Model Explorer on the GPT4All is a great way to choose and download a model.  Then, specify the path that you downloaded to to.  E.g., for me, the model lives here:  /Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin  from langchain_community.llms import GPT4All  gpt4all = GPT4All(     model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\",     max_tokens=2048, )  Using in a chain​  We can create a summarization chain with either model by passing in the retrieved docs and a simple prompt.  It formats the prompt template using the input key values provided and passes the formatted string to GPT4All, LLama-V2, or another specified LLM.  from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import PromptTemplate  # Prompt prompt = PromptTemplate.from_template(     \"Summarize the main themes in these retrieved docs: {docs}\" )   # Chain def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   chain = {\"docs\": format_docs} | prompt | llm | StrOutputParser()  # Run question = \"What are the approaches to Task Decomposition?\" docs = vectorstore.similarity_search(question) chain.invoke(docs)  Llama.generate: prefix-match hit  llama_print_timings:        load time =  1191.88 ms llama_print_timings:      sample time =   134.47 ms /   193 runs   (    0.70 ms per token,  1435.25 tokens per second) llama_print_timings: prompt eval time = 39470.18 ms /  1055 tokens (   37.41 ms per token,    26.73 tokens per second) llama_print_timings:        eval time =  8090.85 ms /   192 runs   (   42.14 ms per token,    23.73 tokens per second) llama_print_timings:       total time = 47943.12 ms   Based on the retrieved documents, the main themes are: 1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system. 2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner. 3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence. 4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.  '\\nBased on the retrieved documents, the main themes are:\\n1. Task decomposition: The ability to break down complex tasks into smaller subtasks, which can be handled by an LLM or other components of the agent system.\\n2. LLM as the core controller: The use of a large language model (LLM) as the primary controller of an autonomous agent system, complemented by other key components such as a knowledge graph and a planner.\\n3. Potentiality of LLM: The idea that LLMs have the potential to be used as powerful general problem solvers, not just for generating well-written copies but also for solving complex tasks and achieving human-like intelligence.\\n4. Challenges in long-term planning: The challenges in planning over a lengthy history and effectively exploring the solution space, which are important limitations of current LLM-based autonomous agent systems.'  Q&A​  We can also use the LangChain Prompt Hub to store and fetch prompts that are model-specific.  Let’s try with a default RAG prompt, here.  from langchain import hub  rag_prompt = hub.pull(\"rlm/rag-prompt\") rag_prompt.messages  [HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"))]  from langchain_core.runnables import RunnablePassthrough, RunnablePick  # Chain chain = (     RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)     | rag_prompt     | llm     | StrOutputParser() )  # Run chain.invoke({\"context\": docs, \"question\": question})  Llama.generate: prefix-match hit  llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =    33.03 ms /    47 runs   (    0.70 ms per token,  1422.86 tokens per second) llama_print_timings: prompt eval time =  1387.31 ms /   242 tokens (    5.73 ms per token,   174.44 tokens per second) llama_print_timings:        eval time =  1321.62 ms /    46 runs   (   28.73 ms per token,    34.81 tokens per second) llama_print_timings:       total time =  2801.08 ms   Task can be done by down a task into smaller subtasks, using simple prompting like \"Steps for XYZ.\" or task-specific like \"Write a story outline\" for writing a novel.  {'output_text': '\\nTask can be done by down a task into smaller subtasks, using simple prompting like \"Steps for XYZ.\" or task-specific like \"Write a story outline\" for writing a novel.'}   Now, let’s try with a prompt specifically for LLaMA, which includes special tokens.  # Prompt rag_prompt_llama = hub.pull(\"rlm/rag-prompt-llama\") rag_prompt_llama.messages  ChatPromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question', 'context'], output_parser=None, partial_variables={}, template=\"[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\", template_format='f-string', validate_template=True), additional_kwargs={})])  # Chain chain = (     RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)     | rag_prompt_llama     | llm     | StrOutputParser() )  # Run chain.invoke({\"context\": docs, \"question\": question})  Llama.generate: prefix-match hit  llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =   144.81 ms /   207 runs   (    0.70 ms per token,  1429.47 tokens per second) llama_print_timings: prompt eval time =  1506.13 ms /   258 tokens (    5.84 ms per token,   171.30 tokens per second) llama_print_timings:        eval time =  6231.92 ms /   206 runs   (   30.25 ms per token,    33.06 tokens per second) llama_print_timings:       total time =  8158.41 ms    Sure, I'd be happy to help! Based on the context, here are some to task:  1. LLM with simple prompting: This using a large model (LLM) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to decompose tasks into smaller steps. 2. Task-specific: Another is to use task-specific, such as \"Write a story outline\" for writing a novel, to guide the of tasks. 3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise.  As fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error.  {'output_text': '  Sure, I\\'d be happy to help! Based on the context, here are some to task:\\n\\n1. LLM with simple prompting: This using a large model (LLM) with simple prompts like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" to decompose tasks into smaller steps.\\n2. Task-specific: Another is to use task-specific, such as \"Write a story outline\" for writing a novel, to guide the of tasks.\\n3. Human inputs:, human inputs can be used to supplement the process, in cases where the task a high degree of creativity or expertise.\\n\\nAs fores in long-term and task, one major is that LLMs to adjust plans when faced with errors, making them less robust to humans who learn from trial and error.'}  Q&A with retrieval​  Instead of manually passing in docs, we can automatically retrieve them from our vector store based on the user question.  This will use a QA default prompt (shown here) and will retrieve from the vectorDB.  retriever = vectorstore.as_retriever() qa_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | rag_prompt     | llm     | StrOutputParser() )  qa_chain.invoke(question)  Llama.generate: prefix-match hit  llama_print_timings:        load time = 11326.20 ms llama_print_timings:      sample time =   139.20 ms /   200 runs   (    0.70 ms per token,  1436.76 tokens per second) llama_print_timings: prompt eval time =  1532.26 ms /   258 tokens (    5.94 ms per token,   168.38 tokens per second) llama_print_timings:        eval time =  5977.62 ms /   199 runs   (   30.04 ms per token,    33.29 tokens per second) llama_print_timings:       total time =  7916.21 ms    Sure! Based on the context, here's my answer to your:  There are several to task,:  1. LLM-based with simple prompting, such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\" 2. Task-specific, like \"Write a story outline\" for writing a novel. 3. Human inputs to guide the process.  These can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error.  {'query': 'What are the approaches to Task Decomposition?',  'result': '  Sure! Based on the context, here\\'s my answer to your:\\n\\nThere are several to task,:\\n\\n1. LLM-based with simple prompting, such as \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n2. Task-specific, like \"Write a story outline\" for writing a novel.\\n3. Human inputs to guide the process.\\n\\nThese can be used to decompose complex tasks into smaller, more manageable subtasks, which can help improve the and effectiveness of task. However, long-term and task can being due to the need to plan over a lengthy history and explore the space., LLMs may to adjust plans when faced with errors, making them less robust to human learners who can learn from trial and error.'}  Previous Using agents Next SQL Document Loading Model LLaMA2 GPT4All Using in a chain Q&A Q&A with retrieval Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/web_scraping",
    "title": "Web scraping | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesWeb scraping Web scraping  Open In Collab  Use case​  Web research is one of the killer LLM applications:  Users have highlighted it as one of his top desired AI tools. OSS repos like gpt-researcher are growing in popularity.  Overview​  Gathering content from the web has a few components:  Search: Query to url (e.g., using GoogleSearchAPIWrapper). Loading: Url to HTML (e.g., using AsyncHtmlLoader, AsyncChromiumLoader, etc). Transforming: HTML to formatted text (e.g., using HTML2Text or Beautiful Soup). Quickstart​ pip install -q langchain-openai langchain playwright beautifulsoup4 playwright install  # Set env var OPENAI_API_KEY or load from a .env file: # import dotenv # dotenv.load_dotenv()   Scraping HTML content using a headless instance of Chromium.  The async nature of the scraping process is handled using Python’s asyncio library. The actual interaction with the web pages is handled by Playwright. from langchain_community.document_loaders import AsyncChromiumLoader from langchain_community.document_transformers import BeautifulSoupTransformer  # Load HTML loader = AsyncChromiumLoader([\"https://www.wsj.com\"]) html = loader.load()   Scrape text content tags such as <p>, <li>, <div>, and <a> tags from the HTML content:  <p>: The paragraph tag. It defines a paragraph in HTML and is used to group together related sentences and/or phrases.  <li>: The list item tag. It is used within ordered (<ol>) and unordered (<ul>) lists to define individual items within the list.  <div>: The division tag. It is a block-level element used to group other inline or block-level elements.  <a>: The anchor tag. It is used to define hyperlinks.  <span>: an inline container used to mark up a part of a text, or a part of a document.  For many news websites (e.g., WSJ, CNN), headlines and summaries are all in <span> tags.  # Transform bs_transformer = BeautifulSoupTransformer() docs_transformed = bs_transformer.transform_documents(html, tags_to_extract=[\"span\"])  # Result docs_transformed[0].page_content[0:500]  'English EditionEnglish中文 (Chinese)日本語 (Japanese) More Other Products from WSJBuy Side from WSJWSJ ShopWSJ Wine Other Products from WSJ Search Quotes and Companies Search Quotes and Companies 0.15% 0.03% 0.12% -0.42% 4.102% -0.69% -0.25% -0.15% -1.82% 0.24% 0.19% -1.10% About Evan His Family Reflects His Reporting How You Can Help Write a Message Life in Detention Latest News Get Email Updates Four Americans Released From Iranian Prison The Americans will remain under house arrest until they are '   These Documents now are staged for downstream usage in various LLM apps, as discussed below.  Loader​ AsyncHtmlLoader​  The AsyncHtmlLoader uses the aiohttp library to make asynchronous HTTP requests, suitable for simpler and lightweight scraping.  AsyncChromiumLoader​  The AsyncChromiumLoader uses Playwright to launch a Chromium instance, which can handle JavaScript rendering and more complex web interactions.  Chromium is one of the browsers supported by Playwright, a library used to control browser automation.  Headless mode means that the browser is running without a graphical user interface, which is commonly used for web scraping.  from langchain_community.document_loaders import AsyncHtmlLoader  urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"] loader = AsyncHtmlLoader(urls) docs = loader.load()  Transformer​ HTML2Text​  HTML2Text provides a straightforward conversion of HTML content into plain text (with markdown-like formatting) without any specific tag manipulation.  It’s best suited for scenarios where the goal is to extract human-readable text without needing to manipulate specific HTML elements.  Beautiful Soup​  Beautiful Soup offers more fine-grained control over HTML content, enabling specific tag extraction, removal, and content cleaning.  It’s suited for cases where you want to extract specific information and clean up the HTML content according to your needs.  from langchain_community.document_loaders import AsyncHtmlLoader  urls = [\"https://www.espn.com\", \"https://lilianweng.github.io/posts/2023-06-23-agent/\"] loader = AsyncHtmlLoader(urls) docs = loader.load()  Fetching pages: 100%|#############################################################################################################| 2/2 [00:00<00:00,  7.01it/s]  from langchain_community.document_transformers import Html2TextTransformer  html2text = Html2TextTransformer() docs_transformed = html2text.transform_documents(docs) docs_transformed[0].page_content[0:500]  \"Skip to main content  Skip to navigation\\n\\n<\\n\\n>\\n\\nMenu\\n\\n## ESPN\\n\\n  * Search\\n\\n  *   * scores\\n\\n  * NFL\\n  * MLB\\n  * NBA\\n  * NHL\\n  * Soccer\\n  * NCAAF\\n  * …\\n\\n    * Women's World Cup\\n    * LLWS\\n    * NCAAM\\n    * NCAAW\\n    * Sports Betting\\n    * Boxing\\n    * CFL\\n    * NCAA\\n    * Cricket\\n    * F1\\n    * Golf\\n    * Horse\\n    * MMA\\n    * NASCAR\\n    * NBA G League\\n    * Olympic Sports\\n    * PLL\\n    * Racing\\n    * RN BB\\n    * RN FB\\n    * Rugby\\n    * Tennis\\n    * WNBA\\n    * WWE\\n    * X Games\\n    * XFL\\n\\n  * More\"  Scraping with extraction​ LLM with function calling​  Web scraping is challenging for many reasons.  One of them is the changing nature of modern websites’ layouts and content, which requires modifying scraping scripts to accommodate the changes.  Using Function (e.g., OpenAI) with an extraction chain, we avoid having to change your code constantly when websites change.  We’re using gpt-3.5-turbo-0613 to guarantee access to OpenAI Functions feature (although this might be available to everyone by time of writing).  We’re also keeping temperature at 0 to keep randomness of the LLM down.  from langchain_openai import ChatOpenAI  llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")  Define a schema​  Next, you define a schema to specify what kind of data you want to extract.  Here, the key names matter as they tell the LLM what kind of information they want.  So, be as detailed as possible.  In this example, we want to scrape only news article’s name and summary from The Wall Street Journal website.  from langchain.chains import create_extraction_chain  schema = {     \"properties\": {         \"news_article_title\": {\"type\": \"string\"},         \"news_article_summary\": {\"type\": \"string\"},     },     \"required\": [\"news_article_title\", \"news_article_summary\"], }   def extract(content: str, schema: dict):     return create_extraction_chain(schema=schema, llm=llm).run(content)  Run the web scraper w/ BeautifulSoup​  As shown above, we’ll be using BeautifulSoupTransformer.  import pprint  from langchain.text_splitter import RecursiveCharacterTextSplitter   def scrape_with_playwright(urls, schema):     loader = AsyncChromiumLoader(urls)     docs = loader.load()     bs_transformer = BeautifulSoupTransformer()     docs_transformed = bs_transformer.transform_documents(         docs, tags_to_extract=[\"span\"]     )     print(\"Extracting content with LLM\")      # Grab the first 1000 tokens of the site     splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(         chunk_size=1000, chunk_overlap=0     )     splits = splitter.split_documents(docs_transformed)      # Process the first split     extracted_content = extract(schema=schema, content=splits[0].page_content)     pprint.pprint(extracted_content)     return extracted_content   urls = [\"https://www.wsj.com\"] extracted_content = scrape_with_playwright(urls, schema=schema)  Extracting content with LLM [{'news_article_summary': 'The Americans will remain under house arrest until '                           'they are allowed to return to the U.S. in coming '                           'weeks, following a monthslong diplomatic push by '                           'the Biden administration.',   'news_article_title': 'Four Americans Released From Iranian Prison'},  {'news_article_summary': 'Price pressures continued cooling last month, with '                           'the CPI rising a mild 0.2% from June, likely '                           'deterring the Federal Reserve from raising interest '                           'rates at its September meeting.',   'news_article_title': 'Cooler July Inflation Opens Door to Fed Pause on '                         'Rates'},  {'news_article_summary': 'The company has decided to eliminate 27 of its 30 '                           'clothing labels, such as Lark & Ro and Goodthreads, '                           'as it works to fend off antitrust scrutiny and cut '                           'costs.',   'news_article_title': 'Amazon Cuts Dozens of House Brands'},  {'news_article_summary': 'President Biden’s order comes on top of a slowing '                           'Chinese economy, Covid lockdowns and rising '                           'tensions between the two powers.',   'news_article_title': 'U.S. Investment Ban on China Poised to Deepen Divide'},  {'news_article_summary': 'The proposed trial date in the '                           'election-interference case comes on the same day as '                           'the former president’s not guilty plea on '                           'additional Mar-a-Lago charges.',   'news_article_title': 'Trump Should Be Tried in January, Prosecutors Tell '                         'Judge'},  {'news_article_summary': 'The CEO who started in June says the platform has '                           '“an entirely different road map” for the future.',   'news_article_title': 'Yaccarino Says X Is Watching Threads but Has Its Own '                         'Vision'},  {'news_article_summary': 'Students foot the bill for flagship state '                           'universities that pour money into new buildings and '                           'programs with little pushback.',   'news_article_title': 'Colleges Spend Like There’s No Tomorrow. ‘These '                         'Places Are Just Devouring Money.’'},  {'news_article_summary': 'Wildfires fanned by hurricane winds have torn '                           'through parts of the Hawaiian island, devastating '                           'the popular tourist town of Lahaina.',   'news_article_title': 'Maui Wildfires Leave at Least 36 Dead'},  {'news_article_summary': 'After its large armored push stalled, Kyiv has '                           'fallen back on the kind of tactics that brought it '                           'success earlier in the war.',   'news_article_title': 'Ukraine Uses Small-Unit Tactics to Retake Captured '                         'Territory'},  {'news_article_summary': 'President Guillermo Lasso says the Aug. 20 election '                           'will proceed, as the Andean country grapples with '                           'rising drug gang violence.',   'news_article_title': 'Ecuador Declares State of Emergency After '                         'Presidential Hopeful Killed'},  {'news_article_summary': 'This year’s hurricane season, which typically runs '                           'from June to the end of November, has been '                           'difficult to predict, climate scientists said.',   'news_article_title': 'Atlantic Hurricane Season Prediction Increased to '                         '‘Above Normal,’ NOAA Says'},  {'news_article_summary': 'The NFL is raising the price of its NFL+ streaming '                           'packages as it adds the NFL Network and RedZone.',   'news_article_title': 'NFL to Raise Price of NFL+ Streaming Packages as It '                         'Adds NFL Network, RedZone'},  {'news_article_summary': 'Russia is planning a moon mission as part of the '                           'new space race.',   'news_article_title': 'Russia’s Moon Mission and the New Space Race'},  {'news_article_summary': 'Tapestry’s $8.5 billion acquisition of Capri would '                           'create a conglomerate with more than $12 billion in '                           'annual sales, but it would still lack the '                           'high-wattage labels and diversity that have fueled '                           'LVMH’s success.',   'news_article_title': \"Why the Coach and Kors Marriage Doesn't Scare LVMH\"},  {'news_article_summary': 'The Supreme Court has blocked Purdue Pharma’s $6 '                           'billion Sackler opioid settlement.',   'news_article_title': 'Supreme Court Blocks Purdue Pharma’s $6 Billion '                         'Sackler Opioid Settlement'},  {'news_article_summary': 'The Social Security COLA is expected to rise in '                           '2024, but not by a lot.',   'news_article_title': 'Social Security COLA Expected to Rise in 2024, but '                         'Not by a Lot'}]   We can compare the headlines scraped to the page:  Looking at the LangSmith trace, we can see what is going on under the hood:  It’s following what is explained in the extraction. We call the information_extraction function on the input text. It will attempt to populate the provided schema from the url content. Research automation​  Related to scraping, we may want to answer specific questions using searched content.  We can automate the process of web research using a retriever, such as the WebResearchRetriever (docs).  Copy requirements from here:  pip install -r requirements.txt  Set GOOGLE_CSE_ID and GOOGLE_API_KEY.  from langchain.retrievers.web_research import WebResearchRetriever from langchain_community.utilities import GoogleSearchAPIWrapper from langchain_community.vectorstores import Chroma from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Vectorstore vectorstore = Chroma(     embedding_function=OpenAIEmbeddings(), persist_directory=\"./chroma_db_oai\" )  # LLM llm = ChatOpenAI(temperature=0)  # Search search = GoogleSearchAPIWrapper()   Initialize retriever with the above tools to:  Use an LLM to generate multiple relevant search queries (one LLM call) Execute a search for each query Choose the top K links per query (multiple search calls in parallel) Load the information from all chosen links (scrape pages in parallel) Index those documents into a vectorstore Find the most relevant documents for each original generated search query # Initialize web_research_retriever = WebResearchRetriever.from_llm(     vectorstore=vectorstore, llm=llm, search=search )  # Run import logging  logging.basicConfig() logging.getLogger(\"langchain.retrievers.web_research\").setLevel(logging.INFO) from langchain.chains import RetrievalQAWithSourcesChain  user_input = \"How do LLM Powered Autonomous Agents work?\" qa_chain = RetrievalQAWithSourcesChain.from_chain_type(     llm, retriever=web_research_retriever ) result = qa_chain({\"question\": user_input}) result  INFO:langchain.retrievers.web_research:Generating questions for Google Search ... INFO:langchain.retrievers.web_research:Questions for Google Search (raw): {'question': 'How do LLM Powered Autonomous Agents work?', 'text': LineList(lines=['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n'])} INFO:langchain.retrievers.web_research:Questions for Google Search: ['1. What is the functioning principle of LLM Powered Autonomous Agents?\\n', '2. How do LLM Powered Autonomous Agents operate?\\n'] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': 'LLM Powered Autonomous Agents | Hacker News', 'link': 'https://news.ycombinator.com/item?id=36488871', 'snippet': 'Jun 26, 2023 ... Exactly. A temperature of 0 means you always pick the highest probability token (i.e. the \"max\" function), while a temperature of 1 means you\\xa0...'}] INFO:langchain.retrievers.web_research:Searching for relevant urls ... INFO:langchain.retrievers.web_research:Search results: [{'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'link': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'snippet': 'Jun 23, 2023 ... Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\" , \"What are the subgoals for achieving XYZ?\" , (2) by\\xa0...'}] INFO:langchain.retrievers.web_research:New URLs to load: [] INFO:langchain.retrievers.web_research:Grabbing most relevant splits from urls...  {'question': 'How do LLM Powered Autonomous Agents work?',  'answer': \"LLM-powered autonomous agents work by using LLM as the agent's brain, complemented by several key components such as planning, memory, and tool use. In terms of planning, the agent breaks down large tasks into smaller subgoals and can reflect and refine its actions based on past experiences. Memory is divided into short-term memory, which is used for in-context learning, and long-term memory, which allows the agent to retain and recall information over extended periods. Tool use involves the agent calling external APIs for additional information. These agents have been used in various applications, including scientific discovery and generative agents simulation.\",  'sources': ''}  Going deeper​ Here’s a app that wraps this retriever with a lighweight UI. Question answering over a website​  To answer questions over a specific website, you can use Apify’s Website Content Crawler Actor, which can deeply crawl websites such as documentation, knowledge bases, help centers, or blogs, and extract text content from the web pages.  In the example below, we will deeply crawl the Python documentation of LangChain’s Chat LLM models and answer a question over it.  First, install the requirements pip install apify-client langchain-openai langchain chromadb tiktoken  Next, set OPENAI_API_KEY and APIFY_API_TOKEN in your environment variables.  The full code follows:  from langchain.docstore.document import Document from langchain.indexes import VectorstoreIndexCreator from langchain_community.utilities import ApifyWrapper  apify = ApifyWrapper() # Call the Actor to obtain text from the crawled webpages loader = apify.call_actor(     actor_id=\"apify/website-content-crawler\",     run_input={         \"startUrls\": [{\"url\": \"https://python.langchain.com/docs/integrations/chat/\"}]     },     dataset_mapping_function=lambda item: Document(         page_content=item[\"text\"] or \"\", metadata={\"source\": item[\"url\"]}     ), )  # Create a vector store based on the crawled data index = VectorstoreIndexCreator().from_loaders([loader])  # Query the vector store query = \"Are any OpenAI chat models integrated in LangChain?\" result = index.query(query) print(result)   Yes, LangChain offers integration with OpenAI chat models. You can use the ChatOpenAI class to interact with OpenAI models.  Previous Tagging Next Tool use Use case Overview Quickstart Loader AsyncHtmlLoader AsyncChromiumLoader Transformer HTML2Text Beautiful Soup Scraping with extraction LLM with function calling Define a schema Run the web scraper w/ BeautifulSoup Research automation Going deeper Question answering over a website Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/quickstart",
    "title": "Quickstart | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGQuickstart Quickstart  LangChain has a number of components designed to help build question-answering applications, and RAG applications more generally. To familiarize ourselves with these, we’ll build a simple Q&A application over a text data source. Along the way we’ll go over a typical Q&A architecture, discuss the relevant LangChain components, and highlight additional resources for more advanced Q&A techniques. We’ll also see how LangSmith can help us trace and understand our application. LangSmith will become increasingly helpful as our application grows in complexity.  Architecture​  We’ll create a typical RAG application as outlined in the Q&A introduction, which has two main components:  Indexing: a pipeline for ingesting data from a source and indexing it. This usually happens offline.  Retrieval and generation: the actual RAG chain, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model.  The full sequence from raw data to answer will look like:  Indexing​ Load: First we need to load our data. We’ll use DocumentLoaders for this. Split: Text splitters break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won’t fit in a model’s finite context window. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a VectorStore and Embeddings model. Retrieval and generation​ Retrieve: Given a user input, relevant splits are retrieved from storage using a Retriever. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data Setup​ Dependencies​  We’ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.  We’ll use the following packages:  %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4   We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  import getpass import os  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  # import dotenv  # dotenv.load_dotenv()  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()  Preview​  In this guide we’ll build a QA app over the LLM Powered Autonomous Agents blog post by Lilian Weng, which allows us to ask questions about the contents of the post.  We can create a simple indexing pipeline and RAG chain to do this in ~20 lines of code:  import bs4 from langchain import hub from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import Chroma from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Load, chunk and index the contents of the blog. loader = WebBaseLoader(     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),     bs_kwargs=dict(         parse_only=bs4.SoupStrainer(             class_=(\"post-content\", \"post-title\", \"post-header\")         )     ), ) docs = loader.load()  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() prompt = hub.pull(\"rlm/rag-prompt\") llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)   def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | llm     | StrOutputParser() )  rag_chain.invoke(\"What is Task Decomposition?\")  'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.'  # cleanup vectorstore.delete_collection()   Check out the LangSmith trace  Detailed walkthrough​  Let’s go through the above code step-by-step to really understand what’s going on.  1. Indexing: Load​  We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Documents. A Document is an object with some page_content (str) and metadata (dict).  In this case we’ll use the WebBaseLoader, which uses urllib to load HTML form web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters to the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others.  import bs4 from langchain_community.document_loaders import WebBaseLoader  # Only keep post title, headers, and content from the full HTML. bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\")) loader = WebBaseLoader(     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),     bs_kwargs={\"parse_only\": bs4_strainer}, ) docs = loader.load()  len(docs[0].page_content)  42824  print(docs[0].page_content[:500])          LLM Powered Autonomous Agents      Date: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng   Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In  Go deeper​  DocumentLoader: Object that loads data from a source as list of Documents. - Docs: Detailed documentation on how to use DocumentLoaders. - Integrations: 160+ integrations to choose from. - Interface: API reference  for the base interface.  2. Indexing: Split​  Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.  To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time.  In this case we’ll split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases.  We set add_start_index=True so that the character index at which each split Document starts within the initial Document is preserved as metadata attribute “start_index”.  from langchain.text_splitter import RecursiveCharacterTextSplitter  text_splitter = RecursiveCharacterTextSplitter(     chunk_size=1000, chunk_overlap=200, add_start_index=True ) all_splits = text_splitter.split_documents(docs)  len(all_splits)  66  len(all_splits[0].page_content)  969  all_splits[10].metadata  {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/',  'start_index': 7056}  Go deeper​  TextSplitter: Object that splits a list of Documents into smaller chunks. Subclass of DocumentTransformers. - Explore Context-aware splitters, which keep the location (“context”) of each split in the original Document: - Markdown files - Code (py or js) - Scientific papers - Interface: API reference for the base interface.  DocumentTransformer: Object that performs a transformation on a list of Documents. - Docs: Detailed documentation on how to use DocumentTransformers - Integrations - Interface: API reference for the base interface.  3. Indexing: Store​  Now we need to index our 66 text chunks so that we can search over them at runtime. The most common way to do this is to embed the contents of each document split and insert these embeddings into a vector database (or vector store). When we want to search over our splits, we take a text search query, embed it, and perform some sort of “similarity” search to identify the stored splits with the most similar embeddings to our query embedding. The simplest similarity measure is cosine similarity — we measure the cosine of the angle between each pair of embeddings (which are high dimensional vectors).  We can embed and store all of our document splits in a single command using the Chroma vector store and OpenAIEmbeddings model.  from langchain_community.vectorstores import Chroma from langchain_openai import OpenAIEmbeddings  vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())  Go deeper​  Embeddings: Wrapper around a text embedding model, used for converting text to embeddings. - Docs: Detailed documentation on how to use embeddings. - Integrations: 30+ integrations to choose from. - Interface: API reference for the base interface.  VectorStore: Wrapper around a vector database, used for storing and querying embeddings. - Docs: Detailed documentation on how to use vector stores. - Integrations: 40+ integrations to choose from. - Interface: API reference for the base interface.  This completes the Indexing portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question.  4. Retrieval and Generation: Retrieve​  Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer.  First we need to define our logic for searching over documents. LangChain defines a Retriever interface which wraps an index that can return relevant Documents given a string query.  The most common type of Retriever is the VectorStoreRetriever, which uses the similarity search capabilities of a vector store to facillitate retrieval. Any VectorStore can easily be turned into a Retriever with VectorStore.as_retriever():  retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})  retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition?\")  len(retrieved_docs)  6  print(retrieved_docs[0].page_content)  Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote. Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.  Go deeper​  Vector stores are commonly used for retrieval, but there are other ways to do retrieval, too.  Retriever: An object that returns Documents given a text query - Docs: Further documentation on the interface and built-in retrieval techniques. Some of which include: - MultiQueryRetriever generates variants of the input question to improve retrieval hit rate. - MultiVectorRetriever (diagram below) instead generates variants of the embeddings, also in order to improve retrieval hit rate. - Max marginal relevance selects for relevance and diversity among the retrieved documents to avoid passing in duplicate context. - Documents can be filtered during vector store retrieval using metadata filters. - Integrations: Integrations with retrieval services. - Interface: API reference for the base interface.  5. Retrieval and Generation: Generate​  Let’s put it all together into a chain that takes a question, retrieves relevant documents, constructs a prompt, passes that to a model, and parses the output.  We’ll use the gpt-3.5-turbo OpenAI chat model, but any LangChain LLM or ChatModel could be substituted in.  from langchain_openai import ChatOpenAI  llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)   We’ll use a prompt for RAG that is checked into the LangChain prompt hub (here).  from langchain import hub  prompt = hub.pull(\"rlm/rag-prompt\")  example_messages = prompt.invoke(     {\"context\": \"filler context\", \"question\": \"filler question\"} ).to_messages() example_messages  [HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]  print(example_messages[0].content)  You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise. Question: filler question  Context: filler context  Answer:   We’ll use the LCEL Runnable protocol to define the chain, allowing us to - pipe together components and functions in a transparent way - automatically trace our chain in LangSmith - get streaming, async, and batched calling out of the box  from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough   def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | llm     | StrOutputParser() )  for chunk in rag_chain.stream(\"What is Task Decomposition?\"):     print(chunk, end=\"\", flush=True)  Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for easier interpretation and execution by autonomous agents or models. Task decomposition can be done through various methods, such as using prompting techniques, task-specific instructions, or human inputs.   Check out the LangSmith trace  Go deeper​ Choosing a model​  ChatModel: An LLM-backed chat model. Takes in a sequence of messages and returns a message. - Docs: Detailed documentation on - Integrations: 25+ integrations to choose from. - Interface: API reference for the base interface.  LLM: A text-in-text-out LLM. Takes in a string and returns a string. - Docs - Integrations: 75+ integrations to choose from. - Interface: API reference for the base interface.  See a guide on RAG with locally-running models here.  Customizing the prompt​  As shown above, we can load prompts (e.g., this RAG prompt) from the prompt hub. The prompt can also be easily customized:  from langchain_core.prompts import PromptTemplate  template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer.  {context}  Question: {question}  Helpful Answer:\"\"\" custom_rag_prompt = PromptTemplate.from_template(template)  rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | custom_rag_prompt     | llm     | StrOutputParser() )  rag_chain.invoke(\"What is Task Decomposition?\")  'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'   Check out the LangSmith trace  Next steps​  That’s a lot of content we’ve covered in a short amount of time. There’s plenty of features, integrations, and extensions to explore in each of the above sections. Along from the Go deeper sources mentioned above, good next steps include:  Return sources: Learn how to return source documents Streaming: Learn how to stream outputs and intermediate steps Add chat history: Learn how to add chat history to your app Previous Q&A with RAG Next Returning sources Architecture Setup Dependencies LangSmith Preview Detailed walkthrough 1. Indexing: Load Go deeper 2. Indexing: Split Go deeper 3. Indexing: Store Go deeper 4. Retrieval and Generation: Retrieve Go deeper 5. Retrieval and Generation: Generate Go deeper Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/per_user",
    "title": "Per-User Retrieval | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGPer-User Retrieval Per-User Retrieval  When building a retrieval app, you often have to build it with multiple users in mind. This means that you may be storing data not just for one user, but for many different users, and they should not be able to see eachother’s data. This means that you need to be able to configure your retrieval chain to only retrieve certain information. This generally involves two steps.  Step 1: Make sure the retriever you are using supports multiple users  At the moment, there is no unified flag or filter for this in LangChain. Rather, each vectorstore and retriever may have their own, and may be called different things (namespaces, multi-tenancy, etc). For vectorstores, this is generally exposed as a keyword argument that is passed in during similarity_search. By reading the documentation or source code, figure out whether the retriever you are using supports multiple users, and, if so, how to use it.  Note: adding documentation and/or support for multiple users for retrievers that do not support it (or document it) is a GREAT way to contribute to LangChain  Step 2: Add that parameter as a configurable field for the chain  This will let you easily call the chain and configure any relevant flags at runtime. See this documentation for more information on configuration.  Step 3: Call the chain with that configurable field  Now, at runtime you can call this chain with configurable field.  Code Example​  Let’s see a concrete example of what this looks like in code. We will use Pinecone for this example.  import pinecone from langchain_community.vectorstores import Pinecone from langchain_openai import OpenAIEmbeddings  /Users/harrisonchase/.pyenv/versions/3.10.1/envs/langchain/lib/python3.10/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)   from tqdm.autonotebook import tqdm  # The environment should be the one specified next to the API key # in your Pinecone console pinecone.init(api_key=\"...\", environment=\"...\") index = pinecone.Index(\"test-example\") embeddings = OpenAIEmbeddings() vectorstore = Pinecone(index, embeddings, \"text\")  vectorstore.add_texts([\"i worked at kensho\"], namespace=\"harrison\") vectorstore.add_texts([\"i worked at facebook\"], namespace=\"ankush\")  ['ce15571e-4e2f-44c9-98df-7e83f6f63095']   The pinecone kwarg for namespace can be used to separate documents  # This will only get documents for Ankush vectorstore.as_retriever(search_kwargs={\"namespace\": \"ankush\"}).get_relevant_documents(     \"where did i work?\" )  [Document(page_content='i worked at facebook')]  # This will only get documents for Harrison vectorstore.as_retriever(     search_kwargs={\"namespace\": \"harrison\"} ).get_relevant_documents(\"where did i work?\")  [Document(page_content='i worked at kensho')]   We can now create the chain that we will use to do question-answering over  from langchain_core.output_parsers import StrOutputParser from langchain_core.prompts import ChatPromptTemplate from langchain_core.runnables import (     ConfigurableField,     RunnableBinding,     RunnableLambda,     RunnablePassthrough, ) from langchain_openai import ChatOpenAI, OpenAIEmbeddings   This is basic question-answering chain set up.  template = \"\"\"Answer the question based only on the following context: {context} Question: {question} \"\"\" prompt = ChatPromptTemplate.from_template(template)  model = ChatOpenAI()  retriever = vectorstore.as_retriever()   Here we mark the retriever as having a configurable field. All vectorstore retrievers have search_kwargs as a field. This is just a dictionary, with vectorstore specific fields  configurable_retriever = retriever.configurable_fields(     search_kwargs=ConfigurableField(         id=\"search_kwargs\",         name=\"Search Kwargs\",         description=\"The search kwargs to use\",     ) )   We can now create the chain using our configurable retriever  chain = (     {\"context\": configurable_retriever, \"question\": RunnablePassthrough()}     | prompt     | model     | StrOutputParser() )   We can now invoke the chain with configurable options. search_kwargs is the id of the configurable field. The value is the search kwargs to use for Pinecone  chain.invoke(     \"where did the user work?\",     config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"harrison\"}}}, )  'The user worked at Kensho.'  chain.invoke(     \"where did the user work?\",     config={\"configurable\": {\"search_kwargs\": {\"namespace\": \"ankush\"}}}, )  'The user worked at Facebook.'  Previous Streaming Next Using agents Code Example Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/sources",
    "title": "Returning sources | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGReturning sources Returning sources  Often in Q&A applications it’s important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.  We’ll work off of the Q&A app we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Quickstart.  Setup​ Dependencies​  We’ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.  We’ll use the following packages:  %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4   We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  import getpass import os  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  # import dotenv  # dotenv.load_dotenv()  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()  Chain without sources​  Here is the Q&A app we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Quickstart:  import bs4 from langchain import hub from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import Chroma from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnablePassthrough from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Load, chunk and index the contents of the blog. bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")) loader = WebBaseLoader(     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),     bs_kwargs={\"parse_only\": bs_strainer}, ) docs = loader.load()  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() prompt = hub.pull(\"rlm/rag-prompt\") llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)   def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   rag_chain = (     {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}     | prompt     | llm     | StrOutputParser() )  rag_chain.invoke(\"What is Task Decomposition?\")  'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through prompting techniques like Chain of Thought or Tree of Thoughts, or by using task-specific instructions or human inputs. Task decomposition helps agents plan ahead and manage complicated tasks more effectively.'  Adding sources​  With LCEL it’s easy to return the retrieved documents:  from langchain_core.runnables import RunnableParallel  rag_chain_from_docs = (     RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))     | prompt     | llm     | StrOutputParser() )  rag_chain_with_source = RunnableParallel(     {\"context\": retriever, \"question\": RunnablePassthrough()} ).assign(answer=rag_chain_from_docs)  rag_chain_with_source.invoke(\"What is Task Decomposition\")  {'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1585}),   Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2192}),   Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17804}),   Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17414}),   Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 29630}),   Document(page_content=\"(3) Task execution: Expert models execute on the specific tasks and log results.\\nInstruction:\\n\\nWith the input and the inference results, the AI assistant needs to describe the process and results. The previous stages can be formed as - User Input: {{ User Input }}, Task Planning: {{ Tasks }}, Model Selection: {{ Model Assignment }}, Task Execution: {{ Predictions }}. You must first answer the user's request in a straightforward manner. Then describe the task process and show your analysis and model inference results to the user in the first person. If inference results contain a file path, must tell the user the complete file path.\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19373})],  'question': 'What is Task Decomposition',  'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It involves transforming big tasks into multiple manageable tasks, allowing for a more systematic and organized approach to problem-solving. Thanks for asking!'}   Check out the LangSmith trace  Previous Quickstart Next Add chat history Setup Dependencies LangSmith Chain without sources Adding sources Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/streaming",
    "title": "Streaming | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGStreaming Streaming  Often in Q&A applications it’s important to show users the sources that were used to generate the answer. The simplest way to do this is for the chain to return the Documents that were retrieved in each generation.  We’ll work off of the Q&A app with sources we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Returning sources guide.  Setup​ Dependencies​  We’ll use an OpenAI chat model and embeddings and a Chroma vector store in this walkthrough, but everything shown here works with any ChatModel or LLM, Embeddings, and VectorStore or Retriever.  We’ll use the following packages:  %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai chromadb bs4   We need to set environment variable OPENAI_API_KEY, which can be done directly or loaded from a .env file like so:  import getpass import os  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()  # import dotenv  # dotenv.load_dotenv()  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\" os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()  Chain with sources​  Here is Q&A app with sources we built over the LLM Powered Autonomous Agents blog post by Lilian Weng in the Returning sources guide:  import bs4 from langchain import hub from langchain.text_splitter import RecursiveCharacterTextSplitter from langchain_community.document_loaders import WebBaseLoader from langchain_community.vectorstores import Chroma from langchain_core.output_parsers import StrOutputParser from langchain_core.runnables import RunnableParallel, RunnablePassthrough from langchain_openai import ChatOpenAI, OpenAIEmbeddings  # Load, chunk and index the contents of the blog. bs_strainer = bs4.SoupStrainer(class_=(\"post-content\", \"post-title\", \"post-header\")) loader = WebBaseLoader(     web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),     bs_kwargs={\"parse_only\": bs_strainer}, ) docs = loader.load()  text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200) splits = text_splitter.split_documents(docs) vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())  # Retrieve and generate using the relevant snippets of the blog. retriever = vectorstore.as_retriever() prompt = hub.pull(\"rlm/rag-prompt\") llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)   def format_docs(docs):     return \"\\n\\n\".join(doc.page_content for doc in docs)   rag_chain_from_docs = (     RunnablePassthrough.assign(context=(lambda x: format_docs(x[\"context\"])))     | prompt     | llm     | StrOutputParser() )  rag_chain_with_source = RunnableParallel(     {\"context\": retriever, \"question\": RunnablePassthrough()} ).assign(answer=rag_chain_from_docs)  Streaming final outputs​  With LCEL it’s easy to stream final outputs:  for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):     print(chunk)  {'question': 'What is Task Decomposition'} {'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]} {'answer': ''} {'answer': 'Task'} {'answer': ' decomposition'} {'answer': ' is'} {'answer': ' a'} {'answer': ' technique'} {'answer': ' used'} {'answer': ' to'} {'answer': ' break'} {'answer': ' down'} {'answer': ' complex'} {'answer': ' tasks'} {'answer': ' into'} {'answer': ' smaller'} {'answer': ' and'} {'answer': ' simpler'} {'answer': ' steps'} {'answer': '.'} {'answer': ' It'} {'answer': ' can'} {'answer': ' be'} {'answer': ' done'} {'answer': ' through'} {'answer': ' methods'} {'answer': ' like'} {'answer': ' Chain'} {'answer': ' of'} {'answer': ' Thought'} {'answer': ' ('} {'answer': 'Co'} {'answer': 'T'} {'answer': ')'} {'answer': ' or'} {'answer': ' Tree'} {'answer': ' of'} {'answer': ' Thoughts'} {'answer': ','} {'answer': ' which'} {'answer': ' involve'} {'answer': ' dividing'} {'answer': ' the'} {'answer': ' task'} {'answer': ' into'} {'answer': ' manageable'} {'answer': ' sub'} {'answer': 'tasks'} {'answer': ' and'} {'answer': ' exploring'} {'answer': ' multiple'} {'answer': ' reasoning'} {'answer': ' possibilities'} {'answer': ' at'} {'answer': ' each'} {'answer': ' step'} {'answer': '.'} {'answer': ' Task'} {'answer': ' decomposition'} {'answer': ' can'} {'answer': ' be'} {'answer': ' performed'} {'answer': ' by'} {'answer': ' using'} {'answer': ' simple'} {'answer': ' prompts'} {'answer': ','} {'answer': ' task'} {'answer': '-specific'} {'answer': ' instructions'} {'answer': ','} {'answer': ' or'} {'answer': ' human'} {'answer': ' inputs'} {'answer': '.'} {'answer': ''}   We can add some logic to compile our stream as it’s being returned:  output = {} curr_key = None for chunk in rag_chain_with_source.stream(\"What is Task Decomposition\"):     for key in chunk:         if key not in output:             output[key] = chunk[key]         else:             output[key] += chunk[key]         if key != curr_key:             print(f\"\\n\\n{key}: {chunk[key]}\", end=\"\", flush=True)         else:             print(chunk[key], end=\"\", flush=True)         curr_key = key output    question: What is Task Decomposition  context: [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}), Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]  answer: Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by using simple prompts, task-specific instructions, or human inputs.  {'question': 'What is Task Decomposition',  'context': [Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),   Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),   Document(page_content='The AI assistant can parse user input to several tasks: [{\"task\": task, \"id\", task_id, \"dep\": dependency_task_ids, \"args\": {\"text\": text, \"image\": URL, \"audio\": URL, \"video\": URL}}]. The \"dep\" field denotes the id of the previous task which generates a new resource that the current task relies on. A special tag \"-task_id\" refers to the generated text image, audio and video in the dependency task with id as task_id. The task MUST be selected from the following options: {{ Available Task List }}. There is a logical relationship between tasks, please note their order. If the user input can\\'t be parsed, you need to reply empty JSON. Here are several cases for your reference: {{ Demonstrations }}. The chat history is recorded as {{ Chat History }}. From this chat history, you can find the path of the user-mentioned resources for your task planning.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),   Document(page_content='Fig. 11. Illustration of how HuggingGPT works. (Image source: Shen et al. 2023)\\nThe system comprises of 4 stages:\\n(1) Task planning: LLM works as the brain and parses the user requests into multiple tasks. There are four attributes associated with each task: task type, ID, dependencies, and arguments. They use few-shot examples to guide LLM to do task parsing and planning.\\nInstruction:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})],  'answer': 'Task decomposition is a technique used to break down complex tasks into smaller and simpler steps. It can be done through methods like Chain of Thought (CoT) or Tree of Thoughts, which involve dividing the task into manageable subtasks and exploring multiple reasoning possibilities at each step. Task decomposition can be performed by using simple prompts, task-specific instructions, or human inputs.'}  Streaming intermediate steps​  Suppose we want to stream not only the final outputs of the chain, but also some intermediate steps. As an example let’s take our Chat history chain. Here we reformulate the user question before passing it to the retriever. This reformulated question is not returned as part of the final output. We could modify our chain to return the new question, but for demonstration purposes we’ll leave it as is.  from operator import itemgetter  from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder from langchain_core.tracers.log_stream import LogStreamCallbackHandler  contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\ which might reference context in the chat history, formulate a standalone question \\ which can be understood without the chat history. Do NOT answer the question, \\ just reformulate it if needed and otherwise return it as is.\"\"\" contextualize_q_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", contextualize_q_system_prompt),         MessagesPlaceholder(variable_name=\"chat_history\"),         (\"human\", \"{question}\"),     ] ) contextualize_q_chain = (contextualize_q_prompt | llm | StrOutputParser()).with_config(     tags=[\"contextualize_q_chain\"] )  qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\ Use the following pieces of retrieved context to answer the question. \\ If you don't know the answer, just say that you don't know. \\ Use three sentences maximum and keep the answer concise.\\  {context}\"\"\" qa_prompt = ChatPromptTemplate.from_messages(     [         (\"system\", qa_system_prompt),         MessagesPlaceholder(variable_name=\"chat_history\"),         (\"human\", \"{question}\"),     ] )   def contextualized_question(input: dict):     if input.get(\"chat_history\"):         return contextualize_q_chain     else:         return input[\"question\"]   rag_chain = (     RunnablePassthrough.assign(context=contextualize_q_chain | retriever | format_docs)     | qa_prompt     | llm )   To stream intermediate steps we’ll use the astream_log method. This is an async method that yields JSONPatch ops that when applied in the same order as received build up the RunState:  class RunState(TypedDict):     id: str     \"\"\"ID of the run.\"\"\"     streamed_output: List[Any]     \"\"\"List of output chunks streamed by Runnable.stream()\"\"\"     final_output: Optional[Any]     \"\"\"Final output of the run, usually the result of aggregating (`+`) streamed_output.     Only available after the run has finished successfully.\"\"\"      logs: Dict[str, LogEntry]     \"\"\"Map of run names to sub-runs. If filters were supplied, this list will     contain only the runs that matched the filters.\"\"\"   You can stream all steps (default) or include/exclude steps by name, tags or metadata. In this case we’ll only stream intermediate steps that are part of the contextualize_q_chain and the final output. Notice that when defining the contextualize_q_chain we gave it a corresponding tag, which we can now filter on.  We only show the first 20 chunks of the stream for readability:  # Needed for running async functions in Jupyter notebook: import nest_asyncio  nest_asyncio.apply()  from langchain_core.messages import HumanMessage  chat_history = []  question = \"What is Task Decomposition?\" ai_msg = rag_chain.invoke({\"question\": question, \"chat_history\": chat_history}) chat_history.extend([HumanMessage(content=question), ai_msg])  second_question = \"What are common ways of doing it?\" ct = 0 async for jsonpatch_op in rag_chain.astream_log(     {\"question\": second_question, \"chat_history\": chat_history},     include_tags=[\"contextualize_q_chain\"], ):     print(jsonpatch_op)     print(\"\\n\" + \"-\" * 30 + \"\\n\")     ct += 1     if ct > 20:         break  RunLogPatch({'op': 'replace',   'path': '',   'value': {'final_output': None,             'id': 'df0938b3-3ff2-451b-a233-6c882b640e4d',             'logs': {},             'streamed_output': []}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/RunnableSequence',   'value': {'end_time': None,             'final_output': None,             'id': '2e2af851-9e1f-4260-b004-c30dea4affe9',             'metadata': {},             'name': 'RunnableSequence',             'start_time': '2023-12-29T20:08:28.923',             'streamed_output': [],             'streamed_output_str': [],             'tags': ['seq:step:1', 'contextualize_q_chain'],             'type': 'chain'}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatPromptTemplate',   'value': {'end_time': None,             'final_output': None,             'id': '7ad34564-337c-4362-ae7a-655d79cf0ab0',             'metadata': {},             'name': 'ChatPromptTemplate',             'start_time': '2023-12-29T20:08:28.926',             'streamed_output': [],             'streamed_output_str': [],             'tags': ['seq:step:1', 'contextualize_q_chain'],             'type': 'prompt'}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatPromptTemplate/final_output',   'value': ChatPromptValue(messages=[SystemMessage(content='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.'), HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task decomposition is a technique used to break down complex tasks into smaller and more manageable subtasks. It involves dividing a task into multiple steps or subgoals, allowing an agent or model to better understand and plan for the overall task. Task decomposition can be done through various methods, such as using prompting techniques like Chain of Thought or Tree of Thoughts, task-specific instructions, or human inputs.'), HumanMessage(content='What are common ways of doing it?')])},  {'op': 'add',   'path': '/logs/ChatPromptTemplate/end_time',   'value': '2023-12-29T20:08:28.926'})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI',   'value': {'end_time': None,             'final_output': None,             'id': '228792d6-1d76-4209-8d25-08c484b6df57',             'metadata': {},             'name': 'ChatOpenAI',             'start_time': '2023-12-29T20:08:28.931',             'streamed_output': [],             'streamed_output_str': [],             'tags': ['seq:step:2', 'contextualize_q_chain'],             'type': 'llm'}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/StrOutputParser',   'value': {'end_time': None,             'final_output': None,             'id': 'f740f235-2b14-412d-9f54-53bbc4fa8fd8',             'metadata': {},             'name': 'StrOutputParser',             'start_time': '2023-12-29T20:08:29.487',             'streamed_output': [],             'streamed_output_str': [],             'tags': ['seq:step:3', 'contextualize_q_chain'],             'type': 'parser'}})  ------------------------------  RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ''},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content='')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': 'What'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content='What')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' are'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' are')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' some'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' some')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' commonly'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' commonly')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' used'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' used')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' methods'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' methods')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' or'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' or')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' approaches'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' approaches')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' for'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' for')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' task'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' task')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output_str/-',   'value': ' decomposition'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content=' decomposition')})  ------------------------------  RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': '?'},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content='?')})  ------------------------------  RunLogPatch({'op': 'add', 'path': '/logs/ChatOpenAI/streamed_output_str/-', 'value': ''},  {'op': 'add',   'path': '/logs/ChatOpenAI/streamed_output/-',   'value': AIMessageChunk(content='')})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/ChatOpenAI/final_output',   'value': {'generations': [[{'generation_info': {'finish_reason': 'stop'},                               'message': AIMessageChunk(content='What are some commonly used methods or approaches for task decomposition?'),                               'text': 'What are some commonly used methods or '                                       'approaches for task decomposition?',                               'type': 'ChatGenerationChunk'}]],             'llm_output': None,             'run': None}},  {'op': 'add',   'path': '/logs/ChatOpenAI/end_time',   'value': '2023-12-29T20:08:29.688'})  ------------------------------   If we wanted to get our retrieved docs, we could filter on name “Retriever”:  ct = 0 async for jsonpatch_op in rag_chain.astream_log(     {\"question\": second_question, \"chat_history\": chat_history},     include_names=[\"Retriever\"],     with_streamed_output_list=False, ):     print(jsonpatch_op)     print(\"\\n\" + \"-\" * 30 + \"\\n\")     ct += 1     if ct > 20:         break  RunLogPatch({'op': 'replace',   'path': '',   'value': {'final_output': None,             'id': '9d122c72-378c-41f8-96fe-3fd9a214e9bc',             'logs': {},             'streamed_output': []}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/Retriever',   'value': {'end_time': None,             'final_output': None,             'id': 'c83481fb-7ca3-4125-9280-96da0c14eee9',             'metadata': {},             'name': 'Retriever',             'start_time': '2023-12-29T20:10:13.794',             'streamed_output': [],             'streamed_output_str': [],             'tags': ['seq:step:2', 'Chroma', 'OpenAIEmbeddings'],             'type': 'retriever'}})  ------------------------------  RunLogPatch({'op': 'add',   'path': '/logs/Retriever/final_output',   'value': {'documents': [Document(page_content='Tree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),                           Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),                           Document(page_content='Resources:\\n1. Internet access for searches and information gathering.\\n2. Long Term memory management.\\n3. GPT-3.5 powered Agents for delegation of simple tasks.\\n4. File output.\\n\\nPerformance Evaluation:\\n1. Continuously review and analyze your actions to ensure you are performing to the best of your abilities.\\n2. Constructively self-criticize your big-picture behavior constantly.\\n3. Reflect on past decisions and strategies to refine your approach.\\n4. Every command has a cost, so be smart and efficient. Aim to complete tasks in the least number of steps.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),                           Document(page_content='Fig. 9. Comparison of MIPS algorithms, measured in recall@10. (Image source: Google Blog, 2020)\\nCheck more MIPS algorithms and performance comparison in ann-benchmarks.com.\\nComponent Three: Tool Use#\\nTool use is a remarkable and distinguishing characteristic of human beings. We create, modify and utilize external objects to do things that go beyond our physical and cognitive limits. Equipping LLMs with external tools can significantly extend the model capabilities.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]}},  {'op': 'add',   'path': '/logs/Retriever/end_time',   'value': '2023-12-29T20:10:14.234'})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1.')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques like')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques like Chain')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques like Chain of')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques like Chain of Thought')})  ------------------------------  RunLogPatch({'op': 'replace',   'path': '/final_output',   'value': AIMessageChunk(content='Common ways of task decomposition include:\\n1. Using prompting techniques like Chain of Thought (')})  ------------------------------   For more on how to stream intermediate steps check out the LCEL Interface docs.  Previous Add chat history Next Per-User Retrieval Setup Dependencies LangSmith Chain with sources Streaming final outputs Streaming intermediate steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/data_connection/text_embedding/",
    "title": "Text embedding models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Document loaders Text Splitters Text embedding models CacheBackedEmbeddings Vector stores Retrievers Indexing Agents Chains More LangServe LangSmith LangGraph ModulesRetrievalText embedding models Text embedding models INFO  Head to Integrations for documentation on built-in integrations with text embedding model providers.  The Embeddings class is a class designed for interfacing with text embedding models. There are lots of embedding model providers (OpenAI, Cohere, Hugging Face, etc) - this class is designed to provide a standard interface for all of them.  Embeddings create a vector representation of a piece of text. This is useful because it means we can think about text in the vector space, and do things like semantic search where we look for pieces of text that are most similar in the vector space.  The base Embeddings class in LangChain provides two methods: one for embedding documents and one for embedding a query. The former takes as input multiple texts, while the latter takes a single text. The reason for having these as two separate methods is that some embedding providers have different embedding methods for documents (to be searched over) vs queries (the search query itself).  Get started​ Setup​  To start we'll need to install the OpenAI partner package:  pip install langchain-openai   Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:  export OPENAI_API_KEY=\"...\"   If you'd prefer not to set an environment variable you can pass the key in directly via the openai_api_key named parameter when initiating the OpenAI LLM class:  from langchain_openai import OpenAIEmbeddings  embeddings_model = OpenAIEmbeddings(openai_api_key=\"...\")   Otherwise you can initialize without any params:  from langchain_openai import OpenAIEmbeddings  embeddings_model = OpenAIEmbeddings()  embed_documents​ Embed list of texts​ embeddings = embeddings_model.embed_documents(     [         \"Hi there!\",         \"Oh, hello!\",         \"What's your name?\",         \"My friends call me World\",         \"Hello World!\"     ] ) len(embeddings), len(embeddings[0])  (5, 1536)  embed_query​ Embed single query​  Embed a single piece of text for the purpose of comparing to other embedded pieces of texts.  embedded_query = embeddings_model.embed_query(\"What was the name mentioned in the conversation?\") embedded_query[:5]  [0.0053587136790156364,  -0.0004999046213924885,  0.038883671164512634,  -0.003001077566295862,  -0.00900818221271038]  Previous Retrieval Next CacheBackedEmbeddings Get started Setup embed_documents embed_query Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/use_cases/question_answering/conversational_retrieval_agents",
    "title": "Using agents | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Use cases Q&A with RAG Quickstart Returning sources Add chat history Streaming Per-User Retrieval Using agents Using local models Q&A over structured data Interacting with APIs Chatbots Extraction Summarization Tagging Web scraping Tool use Code understanding Synthetic data generation Graph querying Use casesQ&A with RAGUsing agents Using agents  This is an agent specifically optimized for doing retrieval when necessary and also holding a conversation.  To start, we will set up the retriever we want to use, and then turn it into a retriever tool. Next, we will use the high level constructor for this type of agent. Finally, we will walk through how to construct a conversational retrieval agent from components.  %pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai faiss-cpu  The Retriever​  To start, we need a retriever to use! The code here is mostly just example code. Feel free to use your own retriever and skip to the section on creating a retriever tool.  from langchain_community.document_loaders import TextLoader  loader = TextLoader(\"../../modules/state_of_the_union.txt\") documents = loader.load()  from langchain.text_splitter import CharacterTextSplitter from langchain_community.vectorstores import FAISS from langchain_openai import OpenAIEmbeddings  text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0) texts = text_splitter.split_documents(documents) embeddings = OpenAIEmbeddings() db = FAISS.from_documents(texts, embeddings)  retriever = db.as_retriever()  Retriever Tool​  Now we need to create a tool for our retriever. The main things we need to pass in are a name for the retriever as well as a description. These will both be used by the language model, so they should be informative.  from langchain.tools.retriever import create_retriever_tool  tool = create_retriever_tool(     retriever,     \"search_state_of_union\",     \"Searches and returns excerpts from the 2022 State of the Union.\", ) tools = [tool]  Agent Constructor​  Here, we will use the high level create_openai_tools_agent API to construct the agent.  Notice that beside the list of tools, the only thing we need to pass in is a language model to use. Under the hood, this agent is using the OpenAI tool-calling capabilities, so we need to use a ChatOpenAI model.  from langchain import hub  prompt = hub.pull(\"hwchase17/openai-tools-agent\") prompt.messages  [SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are a helpful assistant')),  MessagesPlaceholder(variable_name='chat_history', optional=True),  HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}')),  MessagesPlaceholder(variable_name='agent_scratchpad')]  from langchain_openai import ChatOpenAI  llm = ChatOpenAI(temperature=0)  from langchain.agents import AgentExecutor, create_openai_tools_agent  agent = create_openai_tools_agent(llm, tools, prompt) agent_executor = AgentExecutor(agent=agent, tools=tools)   We can now try it out!  result = agent_executor.invoke({\"input\": \"hi, im bob\"})  result[\"output\"]  'Hello Bob! How can I assist you today?'   Notice that it now does retrieval  result = agent_executor.invoke(     {         \"input\": \"what did the president say about ketanji brown jackson in the most recent state of the union?\"     } )  result[\"output\"]  \"In the most recent state of the union, the President mentioned Kentaji Brown Jackson. The President nominated Circuit Court of Appeals Judge Ketanji Brown Jackson to serve on the United States Supreme Court. The President described Judge Ketanji Brown Jackson as one of our nation's top legal minds who will continue Justice Breyer's legacy of excellence.\"   Notice that the follow up question asks about information previously retrieved, so no need to do another retrieval  result = agent_executor.invoke(     {\"input\": \"how long ago did the president nominate ketanji brown jackson?\"} )    > Entering new AgentExecutor chain... The President nominated Judge Ketanji Brown Jackson four days ago.  > Finished chain.  result[\"output\"]  'The President nominated Judge Ketanji Brown Jackson four days ago.'   For more on how to use agents with retrievers and other tools, head to the Agents section.  Previous Per-User Retrieval Next Using local models The Retriever Retriever Tool Agent Constructor Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/evaluation/",
    "title": "Evaluation | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation String Evaluators Comparison Evaluators Trajectory Evaluators Examples Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Evaluation Evaluation  Building applications with language models involves many moving parts. One of the most critical components is ensuring that the outcomes produced by your models are reliable and useful across a broad array of inputs, and that they work well with your application's other software components. Ensuring reliability usually boils down to some combination of application design, testing & evaluation, and runtime checks.  The guides in this section review the APIs and functionality LangChain provides to help you better evaluate your applications. Evaluation and testing are both critical when thinking about deploying LLM applications, since production environments require repeatable and useful outcomes.  LangChain offers various types of evaluators to help you measure performance and integrity on diverse data, and we hope to encourage the community to create and share other useful evaluators so everyone can improve. These docs will introduce the evaluator types, how to use them, and provide some examples of their use in real-world scenarios.  Each evaluator type in LangChain comes with ready-to-use implementations and an extensible API that allows for customization according to your unique requirements. Here are some of the types of evaluators we offer:  String Evaluators: These evaluators assess the predicted string for a given input, usually comparing it against a reference string. Trajectory Evaluators: These are used to evaluate the entire trajectory of agent actions. Comparison Evaluators: These evaluators are designed to compare predictions from two runs on a common input.  These evaluators can be used across various scenarios and can be applied to different chain and LLM implementations in the LangChain library.  We also are working to share guides and cookbooks that demonstrate how to use these evaluators in real-world scenarios, such as:  Chain Comparisons: This example uses a comparison evaluator to predict the preferred output. It reviews ways to measure confidence intervals to select statistically significant differences in aggregate preference scores across different models or prompts. LangSmith Evaluation​  LangSmith provides an integrated evaluation and tracing framework that allows you to check for regressions, compare systems, and easily identify and fix any sources of errors and performance issues. Check out the docs on LangSmith Evaluation and additional cookbooks for more detailed information on evaluating your applications.  LangChain benchmarks​  Your application quality is a function both of the LLM you choose and the prompting and data retrieval strategies you employ to provide model contexet. We have published a number of benchmark tasks within the LangChain Benchmarks package to grade different LLM systems on tasks such as:  Agent tool use Retrieval-augmented question-answering Structured Extraction  Check out the docs for examples and leaderboard information.  Reference Docs​  For detailed information on the available evaluators, including how to instantiate, configure, and customize them, check out the reference documentation directly.  🗃️ String Evaluators  8 items  🗃️ Comparison Evaluators  3 items  🗃️ Trajectory Evaluators  2 items  🗃️ Examples  1 items  Previous LangChain Templates Next String Evaluators LangSmith Evaluation LangChain benchmarks Reference Docs Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/callbacks/filecallbackhandler",
    "title": "Logging to file | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More Memory Callbacks Async callbacks Custom callback handlers Callbacks for custom chains Logging to file Multiple callback handlers Tags Token counting LangServe LangSmith LangGraph ModulesMoreCallbacksLogging to file Logging to file  This example shows how to print logs to file. It shows how to use the FileCallbackHandler, which does the same thing as StdOutCallbackHandler, but instead writes the output to file. It also uses the loguru library to log other outputs that are not captured by the handler.  from langchain.callbacks import FileCallbackHandler from langchain.chains import LLMChain from langchain.prompts import PromptTemplate from langchain_openai import OpenAI from loguru import logger  logfile = \"output.log\"  logger.add(logfile, colorize=True, enqueue=True) handler = FileCallbackHandler(logfile)  llm = OpenAI() prompt = PromptTemplate.from_template(\"1 + {number} = \")  # this chain will both print to stdout (because verbose=True) and write to 'output.log' # if verbose=False, the FileCallbackHandler will still write to 'output.log' chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler], verbose=True) answer = chain.run(number=2) logger.info(answer)    > Entering new LLMChain chain... Prompt after formatting: 1 + 2 =   > Finished chain.  2023-06-01 18:36:38.929 | INFO     | __main__:<module>:20 -   3   Now we can open the file output.log to see that the output has been captured.  %pip install --upgrade --quiet  ansi2html > /dev/null  from ansi2html import Ansi2HTMLConverter from IPython.display import HTML, display  with open(\"output.log\", \"r\") as f:     content = f.read()  conv = Ansi2HTMLConverter() html = conv.convert(content, full=True)  display(HTML(html))  > Entering new LLMChain chain... Prompt after formatting: 1 + 2 =  > Finished chain. 2023-06-01 18:36:38.929 | INFO     | __main__:<module>:20 -  3  Previous Callbacks for custom chains Next Multiple callback handlers Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/model_laboratory",
    "title": "Model comparison | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Model comparison Model comparison  Constructing your language model application will likely involved choosing between many different options of prompts, models, and even chains to use. When doing so, you will want to compare these different options on different inputs in an easy, flexible, and intuitive way.  LangChain provides the concept of a ModelLaboratory to test out and try different models.  %pip install --upgrade --quiet  langchain langchain-openai  from langchain.model_laboratory import ModelLaboratory from langchain.prompts import PromptTemplate from langchain_community.llms import Cohere, HuggingFaceHub from langchain_openai import OpenAI  llms = [     OpenAI(temperature=0),     Cohere(model=\"command-xlarge-20221108\", max_tokens=20, temperature=0),     HuggingFaceHub(repo_id=\"google/flan-t5-xl\", model_kwargs={\"temperature\": 1}), ]  model_lab = ModelLaboratory.from_llms(llms)  model_lab.compare(\"What color is a flamingo?\")  Input: What color is a flamingo?  OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1}   Flamingos are pink.  Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 20, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}   Pink  HuggingFaceHub Params: {'repo_id': 'google/flan-t5-xl', 'temperature': 1} pink  prompt = PromptTemplate(     template=\"What is the capital of {state}?\", input_variables=[\"state\"] ) model_lab_with_prompt = ModelLaboratory.from_llms(llms, prompt=prompt)  model_lab_with_prompt.compare(\"New York\")  Input: New York  OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1}   The capital of New York is Albany.  Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 20, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}   The capital of New York is Albany.  HuggingFaceHub Params: {'repo_id': 'google/flan-t5-xl', 'temperature': 1} st john s  from langchain.chains import SelfAskWithSearchChain from langchain_community.utilities import SerpAPIWrapper  open_ai_llm = OpenAI(temperature=0) search = SerpAPIWrapper() self_ask_with_search_openai = SelfAskWithSearchChain(     llm=open_ai_llm, search_chain=search, verbose=True )  cohere_llm = Cohere(temperature=0, model=\"command-xlarge-20221108\") search = SerpAPIWrapper() self_ask_with_search_cohere = SelfAskWithSearchChain(     llm=cohere_llm, search_chain=search, verbose=True )  chains = [self_ask_with_search_openai, self_ask_with_search_cohere] names = [str(open_ai_llm), str(cohere_llm)]  model_lab = ModelLaboratory(chains, names=names)  model_lab.compare(\"What is the hometown of the reigning men's U.S. Open champion?\")  Input: What is the hometown of the reigning men's U.S. Open champion?  OpenAI Params: {'model': 'text-davinci-002', 'temperature': 0.0, 'max_tokens': 256, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1}   > Entering new chain... What is the hometown of the reigning men's U.S. Open champion? Are follow up questions needed here: Yes. Follow up: Who is the reigning men's U.S. Open champion? Intermediate answer: Carlos Alcaraz. Follow up: Where is Carlos Alcaraz from? Intermediate answer: El Palmar, Spain. So the final answer is: El Palmar, Spain > Finished chain.  So the final answer is: El Palmar, Spain  Cohere Params: {'model': 'command-xlarge-20221108', 'max_tokens': 256, 'temperature': 0.0, 'k': 0, 'p': 1, 'frequency_penalty': 0, 'presence_penalty': 0}   > Entering new chain... What is the hometown of the reigning men's U.S. Open champion? Are follow up questions needed here: Yes. Follow up: Who is the reigning men's U.S. Open champion? Intermediate answer: Carlos Alcaraz. So the final answer is:  Carlos Alcaraz > Finished chain.  So the final answer is:  Carlos Alcaraz  Previous Run LLMs locally Next Data anonymization with Microsoft Presidio Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/integrations/providers/wandb_tracing",
    "title": "WandB Tracing | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Providers Anthropic AWS Google Hugging Face Microsoft OpenAI More Activeloop Deep Lake AI21 Labs Aim AINetwork Airbyte Airtable Aleph Alpha Alibaba Cloud AnalyticDB Annoy Anyscale Apify ArangoDB Argilla Arthur Arxiv Astra DB Atlas AwaDB AZLyrics BagelDB Banana Baseten Beam Beautiful Soup BiliBili NIBittensor Blackboard Brave Search CerebriumAI Chaindesk Chroma Clarifai ClearML ClickHouse Cloudflare CnosDB Cohere College Confidential Comet Confident AI Confluence Context C Transformers DashVector Databricks Datadog Tracing Datadog Logs DataForSEO DeepInfra DeepSparse Diffbot DingoDB Discord DocArray Doctran Docugami DSPy DuckDB Elasticsearch Epsilla EverNote Facebook Chat Facebook Faiss Figma Fireworks Flyte ForefrontAI Git GitBook Golden Serper - Google Search API GooseAI GPT4All Gradient Graphsignal Grobid Gutenberg Hacker News Hazy Research Helicone Hologres HTML to text iFixit IMSDb Infinity Infino Jaguar Javelin AI Gateway Jina Johnsnowlabs Konko Label Studio LanceDB LangChain Decorators ✨ Lantern Llama.cpp LLMonitor Log10 Marqo MediaWikiDump Meilisearch Metal Milvus Minimax MLflow Deployments for LLMs MLflow AI Gateway MLflow Modal ModelScope Modern Treasury Momento MongoDB Atlas Motherduck Motörhead MyScale Neo4j NLPCloud Notion DB Nuclia NVIDIA Obsidian Ollama OpenLLM OpenSearch OpenWeatherMap Outline Petals Postgres Embedding PGVector Pinecone PipelineAI Portkey Predibase Prediction Guard PromptLayer Psychic PubMed Qdrant RAGatouille Ray Serve Rebuff Reddit Redis Replicate Roam Robocorp Rockset Runhouse RWKV-4 Salute Devices SearchApi SearxNG Search API SemaDB SerpAPI Shale Protocol SingleStoreDB scikit-learn Slack spaCy Spreedly Stack Exchange StarRocks StochasticAI Streamlit Stripe Supabase (Postgres) Nebula Tair Telegram Tencent TensorFlow Datasets Tigris 2Markdown Trello Trubrics TruLens Twitter Typesense Unstructured Upstash Redis USearch Vearch Vectara Vespa WandB Tracing Weights & Biases Weather Weaviate WhatsApp WhyLabs Wikipedia Wolfram Alpha Writer Xata Xorbits Inference (Xinference) Yandex Yeager.ai YouTube Zep Zilliz Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Memory Callbacks Chat loaders Adapters Stores ProvidersMoreWandB Tracing WandB Tracing  There are two recommended ways to trace your LangChains:  Setting the LANGCHAIN_WANDB_TRACING environment variable to “true”. Using a context manager with tracing_enabled() to trace a particular block of code.  Note if the environment variable is set, all code will be traced, regardless of whether or not it’s within the context manager.  import os  os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"  # wandb documentation to configure wandb using env variables # https://docs.wandb.ai/guides/track/advanced/environment-variables # here we are configuring the wandb project name os.environ[\"WANDB_PROJECT\"] = \"langchain-tracing\"  from langchain.agents import AgentType, initialize_agent, load_tools from langchain.callbacks import wandb_tracing_enabled from langchain_openai import OpenAI  # Agent run with tracing. Ensure that OPENAI_API_KEY is set appropriately to run this example.  llm = OpenAI(temperature=0) tools = load_tools([\"llm-math\"], llm=llm)  agent = initialize_agent(     tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True )  agent.run(\"What is 2 raised to .123243 power?\")  # this should be traced # A url with for the trace sesion like the following should print in your console: # https://wandb.ai/<wandb_entity>/<wandb_project>/runs/<run_id> # The url can be used to view the trace session in wandb.  # Now, we unset the environment variable and use a context manager. if \"LANGCHAIN_WANDB_TRACING\" in os.environ:     del os.environ[\"LANGCHAIN_WANDB_TRACING\"]  # enable tracing using a context manager with wandb_tracing_enabled():     agent.run(\"What is 5 raised to .123243 power?\")  # this should be traced  agent.run(\"What is 2 raised to .123243 power?\")  # this should not be traced    > Entering new AgentExecutor chain...  I need to use a calculator to solve this. Action: Calculator Action Input: 5^.123243 Observation: Answer: 1.2193914912400514 Thought: I now know the final answer. Final Answer: 1.2193914912400514  > Finished chain.   > Entering new AgentExecutor chain...  I need to use a calculator to solve this. Action: Calculator Action Input: 2^.123243 Observation: Answer: 1.0891804557407723 Thought: I now know the final answer. Final Answer: 1.0891804557407723  > Finished chain.  '1.0891804557407723'  Previous Vespa Next Weights & Biases Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/deployments/",
    "title": "Deployment | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment LangChain Templates Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Deployment Deployment  In today's fast-paced technological landscape, the use of Large Language Models (LLMs) is rapidly expanding. As a result, it is crucial for developers to understand how to effectively deploy these models in production environments. LLM interfaces typically fall into two categories:  Case 1: Utilizing External LLM Providers (OpenAI, Anthropic, etc.) In this scenario, most of the computational burden is handled by the LLM providers, while LangChain simplifies the implementation of business logic around these services. This approach includes features such as prompt templating, chat message generation, caching, vector embedding database creation, preprocessing, etc.  Case 2: Self-hosted Open-Source Models Alternatively, developers can opt to use smaller, yet comparably capable, self-hosted open-source LLM models. This approach can significantly decrease costs, latency, and privacy concerns associated with transferring data to external LLM providers.  Regardless of the framework that forms the backbone of your product, deploying LLM applications comes with its own set of challenges. It's vital to understand the trade-offs and key considerations when evaluating serving frameworks.  Outline​  This guide aims to provide a comprehensive overview of the requirements for deploying LLMs in a production setting, focusing on:  Designing a Robust LLM Application Service Maintaining Cost-Efficiency Ensuring Rapid Iteration  Understanding these components is crucial when assessing serving systems. LangChain integrates with several open-source projects designed to tackle these issues, providing a robust framework for productionizing your LLM applications. Some notable frameworks include:  Ray Serve BentoML OpenLLM Modal Jina  These links will provide further information on each ecosystem, assisting you in finding the best fit for your LLM deployment needs.  Designing a Robust LLM Application Service​  When deploying an LLM service in production, it's imperative to provide a seamless user experience free from outages. Achieving 24/7 service availability involves creating and maintaining several sub-systems surrounding your application.  Monitoring​  Monitoring forms an integral part of any system running in a production environment. In the context of LLMs, it is essential to monitor both performance and quality metrics.  Performance Metrics: These metrics provide insights into the efficiency and capacity of your model. Here are some key examples:  Query per second (QPS): This measures the number of queries your model processes in a second, offering insights into its utilization. Latency: This metric quantifies the delay from when your client sends a request to when they receive a response. Tokens Per Second (TPS): This represents the number of tokens your model can generate in a second.  Quality Metrics: These metrics are typically customized according to the business use-case. For instance, how does the output of your system compare to a baseline, such as a previous version? Although these metrics can be calculated offline, you need to log the necessary data to use them later.  Fault tolerance​  Your application may encounter errors such as exceptions in your model inference or business logic code, causing failures and disrupting traffic. Other potential issues could arise from the machine running your application, such as unexpected hardware breakdowns or loss of spot-instances during high-demand periods. One way to mitigate these risks is by increasing redundancy through replica scaling and implementing recovery mechanisms for failed replicas. However, model replicas aren't the only potential points of failure. It's essential to build resilience against various failures that could occur at any point in your stack.  Zero down time upgrade​  System upgrades are often necessary but can result in service disruptions if not handled correctly. One way to prevent downtime during upgrades is by implementing a smooth transition process from the old version to the new one. Ideally, the new version of your LLM service is deployed, and traffic gradually shifts from the old to the new version, maintaining a constant QPS throughout the process.  Load balancing​  Load balancing, in simple terms, is a technique to distribute work evenly across multiple computers, servers, or other resources to optimize the utilization of the system, maximize throughput, minimize response time, and avoid overload of any single resource. Think of it as a traffic officer directing cars (requests) to different roads (servers) so that no single road becomes too congested.  There are several strategies for load balancing. For example, one common method is the Round Robin strategy, where each request is sent to the next server in line, cycling back to the first when all servers have received a request. This works well when all servers are equally capable. However, if some servers are more powerful than others, you might use a Weighted Round Robin or Least Connections strategy, where more requests are sent to the more powerful servers, or to those currently handling the fewest active requests. Let's imagine you're running a LLM chain. If your application becomes popular, you could have hundreds or even thousands of users asking questions at the same time. If one server gets too busy (high load), the load balancer would direct new requests to another server that is less busy. This way, all your users get a timely response and the system remains stable.  Maintaining Cost-Efficiency and Scalability​  Deploying LLM services can be costly, especially when you're handling a large volume of user interactions. Charges by LLM providers are usually based on tokens used, making a chat system inference on these models potentially expensive. However, several strategies can help manage these costs without compromising the quality of the service.  Self-hosting models​  Several smaller and open-source LLMs are emerging to tackle the issue of reliance on LLM providers. Self-hosting allows you to maintain similar quality to LLM provider models while managing costs. The challenge lies in building a reliable, high-performing LLM serving system on your own machines.  Resource Management and Auto-Scaling​  Computational logic within your application requires precise resource allocation. For instance, if part of your traffic is served by an OpenAI endpoint and another part by a self-hosted model, it's crucial to allocate suitable resources for each. Auto-scaling—adjusting resource allocation based on traffic—can significantly impact the cost of running your application. This strategy requires a balance between cost and responsiveness, ensuring neither resource over-provisioning nor compromised application responsiveness.  Utilizing Spot Instances​  On platforms like AWS, spot instances offer substantial cost savings, typically priced at about a third of on-demand instances. The trade-off is a higher crash rate, necessitating a robust fault-tolerance mechanism for effective use.  Independent Scaling​  When self-hosting your models, you should consider independent scaling. For example, if you have two translation models, one fine-tuned for French and another for Spanish, incoming requests might necessitate different scaling requirements for each.  Batching requests​  In the context of Large Language Models, batching requests can enhance efficiency by better utilizing your GPU resources. GPUs are inherently parallel processors, designed to handle multiple tasks simultaneously. If you send individual requests to the model, the GPU might not be fully utilized as it's only working on a single task at a time. On the other hand, by batching requests together, you're allowing the GPU to work on multiple tasks at once, maximizing its utilization and improving inference speed. This not only leads to cost savings but can also improve the overall latency of your LLM service.  In summary, managing costs while scaling your LLM services requires a strategic approach. Utilizing self-hosting models, managing resources effectively, employing auto-scaling, using spot instances, independently scaling models, and batching requests are key strategies to consider. Open-source libraries such as Ray Serve and BentoML are designed to deal with these complexities.  Ensuring Rapid Iteration​  The LLM landscape is evolving at an unprecedented pace, with new libraries and model architectures being introduced constantly. Consequently, it's crucial to avoid tying yourself to a solution specific to one particular framework. This is especially relevant in serving, where changes to your infrastructure can be time-consuming, expensive, and risky. Strive for infrastructure that is not locked into any specific machine learning library or framework, but instead offers a general-purpose, scalable serving layer. Here are some aspects where flexibility plays a key role:  Model composition​  Deploying systems like LangChain demands the ability to piece together different models and connect them via logic. Take the example of building a natural language input SQL query engine. Querying an LLM and obtaining the SQL command is only part of the system. You need to extract metadata from the connected database, construct a prompt for the LLM, run the SQL query on an engine, collect and feed back the response to the LLM as the query runs, and present the results to the user. This demonstrates the need to seamlessly integrate various complex components built in Python into a dynamic chain of logical blocks that can be served together.  Cloud providers​  Many hosted solutions are restricted to a single cloud provider, which can limit your options in today's multi-cloud world. Depending on where your other infrastructure components are built, you might prefer to stick with your chosen cloud provider.  Infrastructure as Code (IaC)​  Rapid iteration also involves the ability to recreate your infrastructure quickly and reliably. This is where Infrastructure as Code (IaC) tools like Terraform, CloudFormation, or Kubernetes YAML files come into play. They allow you to define your infrastructure in code files, which can be version controlled and quickly deployed, enabling faster and more reliable iterations.  CI/CD​  In a fast-paced environment, implementing CI/CD pipelines can significantly speed up the iteration process. They help automate the testing and deployment of your LLM applications, reducing the risk of errors and enabling faster feedback and iteration.  Previous Debugging Next LangChain Templates Outline Designing a Robust LLM Application Service Monitoring Fault tolerance Zero down time upgrade Load balancing Maintaining Cost-Efficiency and Scalability Self-hosting models Resource Management and Auto-Scaling Utilizing Spot Instances Independent Scaling Batching requests Ensuring Rapid Iteration Model composition Cloud providers Infrastructure as Code (IaC) CI/CD Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/safety/",
    "title": "Safety | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Amazon Comprehend Moderation Chain Constitutional chain Hugging Face prompt injection identification Logical Fallacy chain Moderation chain Safety Safety  One of the key concerns with using LLMs is that they may generate harmful or unethical text. This is an area of active research in the field. Here we present some built-in chains inspired by this research, which are intended to make the outputs of LLMs safer.  Amazon Comprehend moderation chain: Use Amazon Comprehend to detect and handle Personally Identifiable Information (PII) and toxicity. Constitutional chain: Prompt the model with a set of principles which should guide the model behavior. Hugging Face prompt injection identification: Detect and handle prompt injection attacks. Logical Fallacy chain: Checks the model output against logical fallacies to correct any deviation. Moderation chain: Check if any output text is harmful and flag it. Previous Pydantic compatibility Next Amazon Comprehend Moderation Chain Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/fallbacks",
    "title": "Fallbacks | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Fallbacks Fallbacks  When working with language models, you may often encounter issues from the underlying APIs, whether these be rate limiting or downtime. Therefore, as you go to move your LLM applications into production it becomes more and more important to safeguard against these. That’s why we’ve introduced the concept of fallbacks.  A fallback is an alternative plan that may be used in an emergency.  Crucially, fallbacks can be applied not only on the LLM level but on the whole runnable level. This is important because often times different models require different prompts. So if your call to OpenAI fails, you don’t just want to send the same prompt to Anthropic - you probably want to use a different prompt template and send a different version there.  Fallback for LLM API Errors​  This is maybe the most common use case for fallbacks. A request to an LLM API can fail for a variety of reasons - the API could be down, you could have hit rate limits, any number of things. Therefore, using fallbacks can help protect against these types of things.  IMPORTANT: By default, a lot of the LLM wrappers catch errors and retry. You will most likely want to turn those off when working with fallbacks. Otherwise the first wrapper will keep on retrying and not failing.  %pip install --upgrade --quiet  langchain langchain-openai  from langchain_community.chat_models import ChatAnthropic from langchain_openai import ChatOpenAI   First, let’s mock out what happens if we hit a RateLimitError from OpenAI  from unittest.mock import patch  import httpx from openai import RateLimitError  request = httpx.Request(\"GET\", \"/\") response = httpx.Response(200, request=request) error = RateLimitError(\"rate limit\", response=response, body=\"\")  # Note that we set max_retries = 0 to avoid retrying on RateLimits, etc openai_llm = ChatOpenAI(max_retries=0) anthropic_llm = ChatAnthropic() llm = openai_llm.with_fallbacks([anthropic_llm])  # Let's use just the OpenAI LLm first, to show that we run into an error with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):     try:         print(openai_llm.invoke(\"Why did the chicken cross the road?\"))     except RateLimitError:         print(\"Hit error\")  Hit error  # Now let's try with fallbacks to Anthropic with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):     try:         print(llm.invoke(\"Why did the chicken cross the road?\"))     except RateLimitError:         print(\"Hit error\")  content=' I don\\'t actually know why the chicken crossed the road, but here are some possible humorous answers:\\n\\n- To get to the other side!\\n\\n- It was too chicken to just stand there. \\n\\n- It wanted a change of scenery.\\n\\n- It wanted to show the possum it could be done.\\n\\n- It was on its way to a poultry farmers\\' convention.\\n\\nThe joke plays on the double meaning of \"the other side\" - literally crossing the road to the other side, or the \"other side\" meaning the afterlife. So it\\'s an anti-joke, with a silly or unexpected pun as the answer.' additional_kwargs={} example=False   We can use our “LLM with Fallbacks” as we would a normal LLM.  from langchain_core.prompts import ChatPromptTemplate  prompt = ChatPromptTemplate.from_messages(     [         (             \"system\",             \"You're a nice assistant who always includes a compliment in your response\",         ),         (\"human\", \"Why did the {animal} cross the road\"),     ] ) chain = prompt | llm with patch(\"openai.resources.chat.completions.Completions.create\", side_effect=error):     try:         print(chain.invoke({\"animal\": \"kangaroo\"}))     except RateLimitError:         print(\"Hit error\")  content=\" I don't actually know why the kangaroo crossed the road, but I can take a guess! Here are some possible reasons:\\n\\n- To get to the other side (the classic joke answer!)\\n\\n- It was trying to find some food or water \\n\\n- It was trying to find a mate during mating season\\n\\n- It was fleeing from a predator or perceived threat\\n\\n- It was disoriented and crossed accidentally \\n\\n- It was following a herd of other kangaroos who were crossing\\n\\n- It wanted a change of scenery or environment \\n\\n- It was trying to reach a new habitat or territory\\n\\nThe real reason is unknown without more context, but hopefully one of those potential explanations does the joke justice! Let me know if you have any other animal jokes I can try to decipher.\" additional_kwargs={} example=False  Fallback for Sequences​  We can also create fallbacks for sequences, that are sequences themselves. Here we do that with two different models: ChatOpenAI and then normal OpenAI (which does not use a chat model). Because OpenAI is NOT a chat model, you likely want a different prompt.  # First let's create a chain with a ChatModel # We add in a string output parser here so the outputs between the two are the same type from langchain_core.output_parsers import StrOutputParser  chat_prompt = ChatPromptTemplate.from_messages(     [         (             \"system\",             \"You're a nice assistant who always includes a compliment in your response\",         ),         (\"human\", \"Why did the {animal} cross the road\"),     ] ) # Here we're going to use a bad model name to easily create a chain that will error chat_model = ChatOpenAI(model_name=\"gpt-fake\") bad_chain = chat_prompt | chat_model | StrOutputParser()  # Now lets create a chain with the normal OpenAI model from langchain.prompts import PromptTemplate from langchain_openai import OpenAI  prompt_template = \"\"\"Instructions: You should always include a compliment in your response.  Question: Why did the {animal} cross the road?\"\"\" prompt = PromptTemplate.from_template(prompt_template) llm = OpenAI() good_chain = prompt | llm  # We can now create a final chain which combines the two chain = bad_chain.with_fallbacks([good_chain]) chain.invoke({\"animal\": \"turtle\"})  '\\n\\nAnswer: The turtle crossed the road to get to the other side, and I have to say he had some impressive determination.'  Fallback for Long Inputs​  One of the big limiting factors of LLMs is their context window. Usually, you can count and track the length of prompts before sending them to an LLM, but in situations where that is hard/complicated, you can fallback to a model with a longer context length.  short_llm = ChatOpenAI() long_llm = ChatOpenAI(model=\"gpt-3.5-turbo-16k\") llm = short_llm.with_fallbacks([long_llm])  inputs = \"What is the next number: \" + \", \".join([\"one\", \"two\"] * 3000)  try:     print(short_llm.invoke(inputs)) except Exception as e:     print(e)  This model's maximum context length is 4097 tokens. However, your messages resulted in 12012 tokens. Please reduce the length of the messages.  try:     print(llm.invoke(inputs)) except Exception as e:     print(e)  content='The next number in the sequence is two.' additional_kwargs={} example=False  Fallback to Better Model​  Often times we ask models to output format in a specific format (like JSON). Models like GPT-3.5 can do this okay, but sometimes struggle. This naturally points to fallbacks - we can try with GPT-3.5 (faster, cheaper), but then if parsing fails we can use GPT-4.  from langchain.output_parsers import DatetimeOutputParser  prompt = ChatPromptTemplate.from_template(     \"what time was {event} (in %Y-%m-%dT%H:%M:%S.%fZ format - only return this value)\" )  # In this case we are going to do the fallbacks on the LLM + output parser level # Because the error will get raised in the OutputParser openai_35 = ChatOpenAI() | DatetimeOutputParser() openai_4 = ChatOpenAI(model=\"gpt-4\") | DatetimeOutputParser()  only_35 = prompt | openai_35 fallback_4 = prompt | openai_35.with_fallbacks([openai_4])  try:     print(only_35.invoke({\"event\": \"the superbowl in 1994\"})) except Exception as e:     print(f\"Error: {e}\")  Error: Could not parse datetime string: The Super Bowl in 1994 took place on January 30th at 3:30 PM local time. Converting this to the specified format (%Y-%m-%dT%H:%M:%S.%fZ) results in: 1994-01-30T15:30:00.000Z  try:     print(fallback_4.invoke({\"event\": \"the superbowl in 1994\"})) except Exception as e:     print(f\"Error: {e}\")  1994-01-30 15:30:00  Previous Comparing Chain Outputs Next Run LLMs locally Fallback for LLM API Errors Fallback for Sequences Fallback for Long Inputs Fallback to Better Model Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/local_llms",
    "title": "Run LLMs locally | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Run LLMs locally Run LLMs locally Use case​  The popularity of projects like PrivateGPT, llama.cpp, and GPT4All underscore the demand to run LLMs locally (on your own device).  This has at least two important benefits:  Privacy: Your data is not sent to a third party, and it is not subject to the terms of service of a commercial service Cost: There is no inference fee, which is important for token-intensive applications (e.g., long-running simulations, summarization) Overview​  Running an LLM locally requires a few things:  Open-source LLM: An open-source LLM that can be freely modified and shared Inference: Ability to run this LLM on your device w/ acceptable latency Open-source LLMs​  Users can now gain access to a rapidly growing set of open-source LLMs.  These LLMs can be assessed across at least two dimensions (see figure):  Base model: What is the base-model and how was it trained? Fine-tuning approach: Was the base-model fine-tuned and, if so, what set of instructions was used?  The relative performance of these models can be assessed using several leaderboards, including:  LmSys GPT4All HuggingFace Inference​  A few frameworks for this have emerged to support inference of open-source LLMs on various devices:  llama.cpp: C++ implementation of llama inference code with weight optimization / quantization gpt4all: Optimized C backend for inference Ollama: Bundles model weights and environment into an app that runs on device and serves the LLM  In general, these frameworks will do a few things:  Quantization: Reduce the memory footprint of the raw model weights Efficient implementation for inference: Support inference on consumer hardware (e.g., CPU or laptop GPU)  In particular, see this excellent post on the importance of quantization.  With less precision, we radically decrease the memory needed to store the LLM in memory.  In addition, we can see the importance of GPU memory bandwidth sheet!  A Mac M2 Max is 5-6x faster than a M1 for inference due to the larger GPU memory bandwidth.  Quickstart​  Ollama is one way to easily run inference on macOS.  The instructions here provide details, which we summarize:  Download and run the app From command line, fetch a model from this list of options: e.g., ollama pull llama2 When the app is running, all models are automatically served on localhost:11434 from langchain_community.llms import Ollama  llm = Ollama(model=\"llama2\") llm(\"The first man on the moon was ...\")  ' The first man on the moon was Neil Armstrong, who landed on the moon on July 20, 1969 as part of the Apollo 11 mission. obviously.'   Stream tokens as they are being generated.  from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler  llm = Ollama(     model=\"llama2\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]) ) llm(\"The first man on the moon was ...\")   The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon's surface, famously declaring \"That's one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.  ' The first man to walk on the moon was Neil Armstrong, an American astronaut who was part of the Apollo 11 mission in 1969. февруари 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind\" as he took his first steps. He was followed by fellow astronaut Edwin \"Buzz\" Aldrin, who also walked on the moon during the mission.'  Environment​  Inference speed is a challenge when running models locally (see above).  To minimize latency, it is desirable to run models locally on GPU, which ships with many consumer laptops e.g., Apple devices.  And even with GPU, the available GPU memory bandwidth (as noted above) is important.  Running Apple silicon GPU​  Ollama will automatically utilize the GPU on Apple devices.  Other frameworks require the user to set up the environment to utilize the Apple GPU.  For example, llama.cpp python bindings can be configured to use the GPU via Metal.  Metal is a graphics and compute API created by Apple providing near-direct access to the GPU.  See the llama.cpp setup here to enable this.  In particular, ensure that conda is using the correct virtual environment that you created (miniforge3).  E.g., for me:  conda activate /Users/rlm/miniforge3/envs/llama   With the above confirmed, then:  CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip install -U llama-cpp-python --no-cache-dir  LLMs​  There are various ways to gain access to quantized model weights.  HuggingFace - Many quantized model are available for download and can be run with framework such as llama.cpp gpt4all - The model explorer offers a leaderboard of metrics and associated quantized models available for download Ollama - Several models can be accessed directly via pull Ollama​  With Ollama, fetch a model via ollama pull <model family>:<tag>:  E.g., for Llama-7b: ollama pull llama2 will download the most basic version of the model (e.g., smallest # parameters and 4 bit quantization) We can also specify a particular version from the model list, e.g., ollama pull llama2:13b See the full set of parameters on the API reference page from langchain_community.llms import Ollama  llm = Ollama(model=\"llama2:13b\") llm(\"The first man on the moon was ... think step by step\")  ' Sure! Here\\'s the answer, broken down step by step:\\n\\nThe first man on the moon was... Neil Armstrong.\\n\\nHere\\'s how I arrived at that answer:\\n\\n1. The first manned mission to land on the moon was Apollo 11.\\n2. The mission included three astronauts: Neil Armstrong, Edwin \"Buzz\" Aldrin, and Michael Collins.\\n3. Neil Armstrong was the mission commander and the first person to set foot on the moon.\\n4. On July 20, 1969, Armstrong stepped out of the lunar module Eagle and onto the moon\\'s surface, famously declaring \"That\\'s one small step for man, one giant leap for mankind.\"\\n\\nSo, the first man on the moon was Neil Armstrong!'  Llama.cpp​  Llama.cpp is compatible with a broad set of models.  For example, below we run inference on llama2-13b with 4 bit quantization downloaded from HuggingFace.  As noted above, see the API reference for the full set of parameters.  From the llama.cpp API reference docs, a few are worth commenting on:  n_gpu_layers: number of layers to be loaded into GPU memory  Value: 1 Meaning: Only one layer of the model will be loaded into GPU memory (1 is often sufficient).  n_batch: number of tokens the model should process in parallel  Value: n_batch Meaning: It’s recommended to choose a value between 1 and n_ctx (which in this case is set to 2048)  n_ctx: Token context window  Value: 2048 Meaning: The model will consider a window of 2048 tokens at a time  f16_kv: whether the model should use half-precision for the key/value cache  Value: True Meaning: The model will use half-precision, which can be more memory efficient; Metal only supports True. %env CMAKE_ARGS=\"-DLLAMA_METAL=on\" %env FORCE_CMAKE=1 %pip install --upgrade --quiet  llama-cpp-python --no-cache-dirclear  from langchain.callbacks.manager import CallbackManager from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler from langchain_community.llms import LlamaCpp  llm = LlamaCpp(     model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",     n_gpu_layers=1,     n_batch=512,     n_ctx=2048,     f16_kv=True,     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),     verbose=True, )   The console log will show the below to indicate Metal was enabled properly from steps above:  ggml_metal_init: allocating ggml_metal_init: using MPS  llm(\"The first man on the moon was ... Let's think step by step\")  Llama.generate: prefix-match hit  llama_print_timings:        load time =  9623.21 ms llama_print_timings:      sample time =   143.77 ms /   203 runs   (    0.71 ms per token,  1412.01 tokens per second) llama_print_timings: prompt eval time =   485.94 ms /     7 tokens (   69.42 ms per token,    14.40 tokens per second) llama_print_timings:        eval time =  6385.16 ms /   202 runs   (   31.61 ms per token,    31.64 tokens per second) llama_print_timings:       total time =  7279.28 ms   and use logical reasoning to figure out who the first man on the moon was.  Here are some clues:  1. The first man on the moon was an American. 2. He was part of the Apollo 11 mission. 3. He stepped out of the lunar module and became the first person to set foot on the moon's surface. 4. His last name is Armstrong.  Now, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong. Therefore, the first man on the moon was Neil Armstrong!  \" and use logical reasoning to figure out who the first man on the moon was.\\n\\nHere are some clues:\\n\\n1. The first man on the moon was an American.\\n2. He was part of the Apollo 11 mission.\\n3. He stepped out of the lunar module and became the first person to set foot on the moon's surface.\\n4. His last name is Armstrong.\\n\\nNow, let's use our reasoning skills to figure out who the first man on the moon was. Based on clue #1, we know that the first man on the moon was an American. Clue #2 tells us that he was part of the Apollo 11 mission. Clue #3 reveals that he was the first person to set foot on the moon's surface. And finally, clue #4 gives us his last name: Armstrong.\\nTherefore, the first man on the moon was Neil Armstrong!\"  GPT4All​  We can use model weights downloaded from GPT4All model explorer.  Similar to what is shown above, we can run inference and use the API reference to set parameters of interest.  %pip install gpt4all  from langchain_community.llms import GPT4All  llm = GPT4All(     model=\"/Users/rlm/Desktop/Code/gpt4all/models/nous-hermes-13b.ggmlv3.q4_0.bin\" )  llm(\"The first man on the moon was ... Let's think step by step\")  \".\\n1) The United States decides to send a manned mission to the moon.2) They choose their best astronauts and train them for this specific mission.3) They build a spacecraft that can take humans to the moon, called the Lunar Module (LM).4) They also create a larger spacecraft, called the Saturn V rocket, which will launch both the LM and the Command Service Module (CSM), which will carry the astronauts into orbit.5) The mission is planned down to the smallest detail: from the trajectory of the rockets to the exact movements of the astronauts during their moon landing.6) On July 16, 1969, the Saturn V rocket launches from Kennedy Space Center in Florida, carrying the Apollo 11 mission crew into space.7) After one and a half orbits around the Earth, the LM separates from the CSM and begins its descent to the moon's surface.8) On July 20, 1969, at 2:56 pm EDT (GMT-4), Neil Armstrong becomes the first man on the moon. He speaks these\"  Prompts​  Some LLMs will benefit from specific prompts.  For example, LLaMA will use special tokens.  We can use ConditionalPromptSelector to set prompt based on the model type.  # Set our LLM llm = LlamaCpp(     model_path=\"/Users/rlm/Desktop/Code/llama.cpp/models/openorca-platypus2-13b.gguf.q4_0.bin\",     n_gpu_layers=1,     n_batch=512,     n_ctx=2048,     f16_kv=True,     callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]),     verbose=True, )   Set the associated prompt based upon the model version.  from langchain.chains import LLMChain from langchain.chains.prompt_selector import ConditionalPromptSelector from langchain.prompts import PromptTemplate  DEFAULT_LLAMA_SEARCH_PROMPT = PromptTemplate(     input_variables=[\"question\"],     template=\"\"\"<<SYS>> \\n You are an assistant tasked with improving Google search \\ results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that \\ are similar to this question. The output should be a numbered list of questions \\ and each should have a question mark at the end: \\n\\n {question} [/INST]\"\"\", )  DEFAULT_SEARCH_PROMPT = PromptTemplate(     input_variables=[\"question\"],     template=\"\"\"You are an assistant tasked with improving Google search \\ results. Generate THREE Google search queries that are similar to \\ this question. The output should be a numbered list of questions and each \\ should have a question mark at the end: {question}\"\"\", )  QUESTION_PROMPT_SELECTOR = ConditionalPromptSelector(     default_prompt=DEFAULT_SEARCH_PROMPT,     conditionals=[(lambda llm: isinstance(llm, LlamaCpp), DEFAULT_LLAMA_SEARCH_PROMPT)], )  prompt = QUESTION_PROMPT_SELECTOR.get_prompt(llm) prompt  PromptTemplate(input_variables=['question'], output_parser=None, partial_variables={}, template='<<SYS>> \\n You are an assistant tasked with improving Google search results. \\n <</SYS>> \\n\\n [INST] Generate THREE Google search queries that are similar to this question. The output should be a numbered list of questions and each should have a question mark at the end: \\n\\n {question} [/INST]', template_format='f-string', validate_template=True)  # Chain llm_chain = LLMChain(prompt=prompt, llm=llm) question = \"What NFL team won the Super Bowl in the year that Justin Bieber was born?\" llm_chain.run({\"question\": question})    Sure! Here are three similar search queries with a question mark at the end:  1. Which NBA team did LeBron James lead to a championship in the year he was drafted? 2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born? 3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?   llama_print_timings:        load time = 14943.19 ms llama_print_timings:      sample time =    72.93 ms /   101 runs   (    0.72 ms per token,  1384.87 tokens per second) llama_print_timings: prompt eval time = 14942.95 ms /    93 tokens (  160.68 ms per token,     6.22 tokens per second) llama_print_timings:        eval time =  3430.85 ms /   100 runs   (   34.31 ms per token,    29.15 tokens per second) llama_print_timings:       total time = 18578.26 ms  '  Sure! Here are three similar search queries with a question mark at the end:\\n\\n1. Which NBA team did LeBron James lead to a championship in the year he was drafted?\\n2. Who won the Grammy Awards for Best New Artist and Best Female Pop Vocal Performance in the same year that Lady Gaga was born?\\n3. What MLB team did Babe Ruth play for when he hit 60 home runs in a single season?'   We also can use the LangChain Prompt Hub to fetch and / or store prompts that are model specific.  This will work with your LangSmith API key.  For example, here is a prompt for RAG with LLaMA-specific tokens.  Use cases​  Given an llm created from one of the models above, you can use it for many use cases.  For example, here is a guide to RAG with local LLMs.  In general, use cases for local LLMs can be driven by at least two factors:  Privacy: private data (e.g., journals, etc) that a user does not want to share Cost: text preprocessing (extraction/tagging), summarization, and agent simulations are token-use-intensive tasks  In addition, here is an overview on fine-tuning, which can utilize open-source LLMs.  Previous Fallbacks Next Model comparison Use case Overview Open-source LLMs Inference Quickstart Environment Running Apple silicon GPU LLMs Ollama Llama.cpp GPT4All Prompts Use cases Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/modules/callbacks/",
    "title": "Callbacks | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Get started Introduction Installation Quickstart Security LangChain Expression Language Get started Why use LCEL Interface How to Cookbook Modules Model I/O Retrieval Agents Chains More Memory Callbacks Async callbacks Custom callback handlers Callbacks for custom chains Logging to file Multiple callback handlers Tags Token counting LangServe LangSmith LangGraph ModulesMoreCallbacks Callbacks INFO  Head to Integrations for documentation on built-in callbacks integrations with 3rd-party tools.  LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks.  You can subscribe to these events by using the callbacks argument available throughout the API. This argument is list of handler objects, which are expected to implement one or more of the methods described below in more detail.  Callback handlers​  CallbackHandlers are objects that implement the CallbackHandler interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered.  class BaseCallbackHandler:     \"\"\"Base callback handler that can be used to handle callbacks from langchain.\"\"\"      def on_llm_start(         self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any     ) -> Any:         \"\"\"Run when LLM starts running.\"\"\"      def on_chat_model_start(         self, serialized: Dict[str, Any], messages: List[List[BaseMessage]], **kwargs: Any     ) -> Any:         \"\"\"Run when Chat Model starts running.\"\"\"      def on_llm_new_token(self, token: str, **kwargs: Any) -> Any:         \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"      def on_llm_end(self, response: LLMResult, **kwargs: Any) -> Any:         \"\"\"Run when LLM ends running.\"\"\"      def on_llm_error(         self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         \"\"\"Run when LLM errors.\"\"\"      def on_chain_start(         self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any     ) -> Any:         \"\"\"Run when chain starts running.\"\"\"      def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> Any:         \"\"\"Run when chain ends running.\"\"\"      def on_chain_error(         self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         \"\"\"Run when chain errors.\"\"\"      def on_tool_start(         self, serialized: Dict[str, Any], input_str: str, **kwargs: Any     ) -> Any:         \"\"\"Run when tool starts running.\"\"\"      def on_tool_end(self, output: str, **kwargs: Any) -> Any:         \"\"\"Run when tool ends running.\"\"\"      def on_tool_error(         self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any     ) -> Any:         \"\"\"Run when tool errors.\"\"\"      def on_text(self, text: str, **kwargs: Any) -> Any:         \"\"\"Run on arbitrary text.\"\"\"      def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:         \"\"\"Run on agent action.\"\"\"      def on_agent_finish(self, finish: AgentFinish, **kwargs: Any) -> Any:         \"\"\"Run on agent end.\"\"\"  Get started​  LangChain provides a few built-in handlers that you can use to get started. These are available in the langchain/callbacks module. The most basic handler is the StdOutCallbackHandler, which simply logs all events to stdout.  Note: when the verbose flag on the object is set to true, the StdOutCallbackHandler will be invoked even without being explicitly passed in.  from langchain.callbacks import StdOutCallbackHandler from langchain.chains import LLMChain from langchain_openai import OpenAI from langchain.prompts import PromptTemplate  handler = StdOutCallbackHandler() llm = OpenAI() prompt = PromptTemplate.from_template(\"1 + {number} = \")  # Constructor callback: First, let's explicitly set the StdOutCallbackHandler when initializing our chain chain = LLMChain(llm=llm, prompt=prompt, callbacks=[handler]) chain.run(number=2)  # Use verbose flag: Then, let's use the `verbose` flag to achieve the same result chain = LLMChain(llm=llm, prompt=prompt, verbose=True) chain.run(number=2)  # Request callbacks: Finally, let's use the request `callbacks` to achieve the same result chain = LLMChain(llm=llm, prompt=prompt) chain.run(number=2, callbacks=[handler])      > Entering new LLMChain chain...     Prompt after formatting:     1 + 2 =      > Finished chain.       > Entering new LLMChain chain...     Prompt after formatting:     1 + 2 =      > Finished chain.       > Entering new LLMChain chain...     Prompt after formatting:     1 + 2 =      > Finished chain.       '\\n\\n3'  Where to pass in callbacks​  The callbacks argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) in two different places:  Constructor callbacks: defined in the constructor, e.g. LLMChain(callbacks=[handler], tags=['a-tag']), which will be used for all calls made on that object, and will be scoped to that object only, e.g. if you pass a handler to the LLMChain constructor, it will not be used by the Model attached to that chain. Request callbacks: defined in the run()/apply() methods used for issuing a request, e.g. chain.run(input, callbacks=[handler]), which will be used for that specific request only, and all sub-requests that it contains (e.g. a call to an LLMChain triggers a call to a Model, which uses the same handler passed in the call() method).  The verbose argument is available on most objects throughout the API (Chains, Models, Tools, Agents, etc.) as a constructor argument, e.g. LLMChain(verbose=True), and it is equivalent to passing a ConsoleCallbackHandler to the callbacks argument of that object and all child objects. This is useful for debugging, as it will log all events to the console.  When do you want to use each of these?​ Constructor callbacks are most useful for use cases such as logging, monitoring, etc., which are not specific to a single request, but rather to the entire chain. For example, if you want to log all the requests made to an LLMChain, you would pass a handler to the constructor. Request callbacks are most useful for use cases such as streaming, where you want to stream the output of a single request to a specific websocket connection, or other similar use cases. For example, if you want to stream the output of a single request to a websocket, you would pass a handler to the call() method Previous Multiple Memory classes Next Callbacks Callback handlers Get started Where to pass in callbacks When do you want to use each of these? Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://python.langchain.com/docs/guides/pydantic_compatibility",
    "title": "Pydantic compatibility | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations Guides API More 🦜️🔗 Chat Search K Debugging Deployment Evaluation Fallbacks Run LLMs locally Model comparison Privacy Pydantic compatibility Safety Pydantic compatibility Pydantic compatibility Pydantic v2 was released in June, 2023 (https://docs.pydantic.dev/2.0/blog/pydantic-v2-final/) v2 contains has a number of breaking changes (https://docs.pydantic.dev/2.0/migration/) Pydantic v2 and v1 are under the same package name, so both versions cannot be installed at the same time LangChain Pydantic migration plan​  As of langchain>=0.0.267, LangChain will allow users to install either Pydantic V1 or V2.  Internally LangChain will continue to use V1. During this time, users can pin their pydantic version to v1 to avoid breaking changes, or start a partial migration using pydantic v2 throughout their code, but avoiding mixing v1 and v2 code for LangChain (see below).  User can either pin to pydantic v1, and upgrade their code in one go once LangChain has migrated to v2 internally, or they can start a partial migration to v2, but must avoid mixing v1 and v2 code for LangChain.  Below are two examples of showing how to avoid mixing pydantic v1 and v2 code in the case of inheritance and in the case of passing objects to LangChain.  Example 1: Extending via inheritance  YES  from pydantic.v1 import root_validator, validator  class CustomTool(BaseTool): # BaseTool is v1 code     x: int = Field(default=1)      def _run(*args, **kwargs):         return \"hello\"      @validator('x') # v1 code     @classmethod     def validate_x(cls, x: int) -> int:         return 1       CustomTool(     name='custom_tool',     description=\"hello\",     x=1, )   Mixing Pydantic v2 primitives with Pydantic v1 primitives can raise cryptic errors  NO  from pydantic import Field, field_validator # pydantic v2  class CustomTool(BaseTool): # BaseTool is v1 code     x: int = Field(default=1)      def _run(*args, **kwargs):         return \"hello\"      @field_validator('x') # v2 code     @classmethod     def validate_x(cls, x: int) -> int:         return 1       CustomTool(      name='custom_tool',     description=\"hello\",     x=1, )   Example 2: Passing objects to LangChain  YES  from langchain_core.tools import Tool from pydantic.v1 import BaseModel, Field # <-- Uses v1 namespace  class CalculatorInput(BaseModel):     question: str = Field()  Tool.from_function( # <-- tool uses v1 namespace     func=lambda question: 'hello',     name=\"Calculator\",     description=\"useful for when you need to answer questions about math\",     args_schema=CalculatorInput )   NO  from langchain_core.tools import Tool from pydantic import BaseModel, Field # <-- Uses v2 namespace  class CalculatorInput(BaseModel):     question: str = Field()  Tool.from_function( # <-- tool uses v1 namespace     func=lambda question: 'hello',     name=\"Calculator\",     description=\"useful for when you need to answer questions about math\",     args_schema=CalculatorInput )  Previous QA with private data protection Next Safety LangChain Pydantic migration plan Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  }
]
