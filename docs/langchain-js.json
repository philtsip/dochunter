[
  {
    "url": "https://js.langchain.com/docs/expression_language/",
    "title": "LangChain Expression Language (LCEL) | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression Language LangChain Expression Language (LCEL)  LangChain Expression Language or LCEL is a declarative way to easily compose chains together. Any chain constructed this way will automatically have full sync, async, and streaming support.  If you're looking for a good place to get started, check out the Cookbook section - it shows off the various Expression Language pieces in order from simple to more complex.  Interface​  The base interface shared by all LCEL objects  Cookbook​  Examples of common LCEL usage patterns  Why use LCEL​  A deeper dive into the benefits of LCEL  Previous Quickstart Next Get started Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/rag/code_understanding",
    "title": "RAG over code | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) RAG over code Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesRetrieval-augmented generation (RAG)RAG over code RAG over code Use case​  Source code analysis is one of the most popular LLM applications (e.g., GitHub Co-Pilot, Code Interpreter, Codium, and Codeium) for use-cases such as:  Q&A over the code base to understand how it works Using LLMs for suggesting refactors or improvements Using LLMs for documenting the code  Overview​  The pipeline for QA over code follows the steps we do for document question answering, with some differences:  In particular, we can employ a splitting strategy that does a few things:  Keeps each top-level function and class in the code is loaded into separate documents. Puts remaining into a separate document. Retains metadata about where each split comes from Quickstart​ yarn add @supabase/supabase-js  # Set env var OPENAI_API_KEY or load from a .env file  Loading​  We'll upload all JavaScript/TypeScript files using the DirectoryLoader and TextLoader classes.  The following script iterates over the files in the LangChain repository and loads every .ts file (a.k.a. documents):  import { DirectoryLoader } from \"langchain/document_loaders/fs/directory\"; import { TextLoader } from \"langchain/document_loaders/fs/text\"; import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";  // Define the path to the repo to perform RAG on. const REPO_PATH = \"/tmp/test_repo\";   We load the code by passing the directory path to DirectoryLoader, which will load all files with .ts extensions. These files are then passed to a TextLoader which will return the contents of the file as a string.  const loader = new DirectoryLoader(REPO_PATH, {   \".ts\": (path) => new TextLoader(path), }); const docs = await loader.load();   Next, we can create a RecursiveCharacterTextSplitter to split our code.  We'll call the static fromLanguage method to create a splitter that knows how to split JavaScript/TypeScript code.  const javascriptSplitter = RecursiveCharacterTextSplitter.fromLanguage(\"js\", {   chunkSize: 2000,   chunkOverlap: 200, }); const texts = await javascriptSplitter.splitDocuments(docs);  console.log(\"Loaded \", texts.length, \" documents.\");  Loaded 3324 documents.  RetrievalQA​  We need to store the documents in a way we can semantically search for their content.  The most common approach is to embed the contents of each document then store the embedding and document in a vector store.  When setting up the vector store retriever:  We test max marginal relevance for retrieval And 5 documents returned  In this example we'll be using Supabase, however you can pick any vector store with MMR search you'd like from our large list of integrations.  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { createClient } from \"@supabase/supabase-js\"; import { OpenAIEmbeddings } from \"@langchain/openai\"; import { SupabaseVectorStore } from \"langchain/vectorstores/supabase\";  const privateKey = process.env.SUPABASE_PRIVATE_KEY; if (!privateKey) throw new Error(`Expected env var SUPABASE_PRIVATE_KEY`);  const url = process.env.SUPABASE_URL; if (!url) throw new Error(`Expected env var SUPABASE_URL`);  const client = createClient(url, privateKey);   Once we've initialized our client we can pass it, along with some more options to the .fromDocuments method on SupabaseVectorStore.  For more instructions on how to set up Supabase, see the Supabase docs.  const vectorStore = await SupabaseVectorStore.fromDocuments(   texts,   new OpenAIEmbeddings(),   {     client,     tableName: \"documents\",     queryName: \"match_documents\",   } );  const retriever = vectorStore.asRetriever({   searchType: \"mmr\", // Use max marginal relevance search   searchKwargs: { fetchK: 5 }, });  Chat​  We'll setup our model and memory system just as we'd do for any other chatbot application.  import { ChatOpenAI } from \"@langchain/openai\";   Pipe the StringOutputParser through since both chains which use this model will also need this output parser.  const model = new ChatOpenAI({ modelName: \"gpt-4\" }).pipe(   new StringOutputParser() );   We're going to use BufferMemory as our memory chain. All this will do is take in inputs/outputs from the LLM and store them in memory.  import { BufferMemory } from \"langchain/memory\";  const memory = new BufferMemory({   returnMessages: true, // Return stored messages as instances of `BaseMessage`   memoryKey: \"chat_history\", // This must match up with our prompt template input variable. });   Now we can construct our main sequence of chains. We're going to be building ConversationalRetrievalChain using Expression Language.  import {   ChatPromptTemplate,   MessagesPlaceholder,   AIMessagePromptTemplate,   HumanMessagePromptTemplate, } from \"langchain/prompts\"; import { RunnableSequence } from \"@langchain/core/runnables\"; import { formatDocumentsAsString } from \"langchain/util/document\"; import { BaseMessage } from \"langchain/schema\"; import { StringOutputParser } from \"@langchain/core/output_parsers\";  Construct the chain​  The meat of this code understanding example will be inside a single RunnableSequence chain. Here, we'll have a single input parameter for the question, and preform retrieval for context and chat history (if available). Then we'll preform the first LLM call to rephrase the users question. Finally, using the rephrased question, context and chat history we'll query the LLM to generate the final answer which we'll return to the user.  Prompts​  Step one is to define our prompts. We need two, the first we'll use to rephrase the users question, the second we'll use to combine the documents and question.  Question generator prompt:  const questionGeneratorTemplate = ChatPromptTemplate.fromMessages([   AIMessagePromptTemplate.fromTemplate(     \"Given the following conversation about a codebase and a follow up question, rephrase the follow up question to be a standalone question.\"   ),   new MessagesPlaceholder(\"chat_history\"),   AIMessagePromptTemplate.fromTemplate(`Follow Up Input: {question} Standalone question:`), ]);   Combine documents prompt:  const combineDocumentsPrompt = ChatPromptTemplate.fromMessages([   AIMessagePromptTemplate.fromTemplate(     \"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\n\"   ),   new MessagesPlaceholder(\"chat_history\"),   HumanMessagePromptTemplate.fromTemplate(\"Question: {question}\"), ]);   Next, we'll construct both chains.  const combineDocumentsChain = RunnableSequence.from([   {     question: (output: string) => output,     chat_history: async () => {       const { chat_history } = await memory.loadMemoryVariables({});       return chat_history;     },     context: async (output: string) => {       const relevantDocs = await retriever.getRelevantDocuments(output);       return formatDocumentsAsString(relevantDocs);     },   },   combineDocumentsPrompt,   model,   new StringOutputParser(), ]);  const conversationalQaChain = RunnableSequence.from([   {     question: (i: { question: string }) => i.question,     chat_history: async () => {       const { chat_history } = await memory.loadMemoryVariables({});       return chat_history;     },   },   questionGeneratorTemplate,   model,   new StringOutputParser(),   combineDocumentsChain, ]);   These two are somewhat complex chain so let's break it down.  First, we define our single input parameter: question: string. Below this we also define chat_history which is not sourced from the user's input, but rather preforms a chat memory lookup. Next, we pipe those variables through to our prompt, model and lastly an output parser. This first part will rephrase the question, and return a single string of the rephrased question. In the next block we call the combineDocumentsChain which takes in the output from the first part of the conversationalQaChain and pipes it through to the next prompt. We also preform a retrieval call to get the relevant documents for the question and any chat history which might be present. Finally, the RunnableSequence returns the result of the model & output parser call from the combineDocumentsChain. This will return the final answer as a string to the user.  The last step is to invoke our chain!  const question = \"How can I initialize a ReAct agent?\"; const result = await conversationalQaChain.invoke({   question, });   This is also where we'd save the LLM response to memory for future context.  await memory.saveContext(   {     input: question,   },   {     output: result,   } );  console.log(result); /** The steps to initialize a ReAct agent are:  1. Import the necessary modules from their respective packages.      \\```     import { initializeAgentExecutorWithOptions } from \"langchain/agents\";     import { OpenAI } from \"@langchain/openai\";     import { SerpAPI } from \"langchain/tools\";     import { Calculator } from \"langchain/tools/calculator\";     \\```  2. Create instances of the needed tools (i.e., OpenAI model, SerpAPI, and Calculator), providing the necessary options.     \\```     const model = new OpenAI({ temperature: 0 });     const tools = [       new SerpAPI(process.env.SERPAPI_API_KEY, {         location: \"Austin,Texas,United States\",         hl: \"en\",         gl: \"us\",       }),       new Calculator(),     ];     \\```  3. Call `initializeAgentExecutorWithOptions` function with the created tools and model, and with the desired options to create an agent instance.     \\```     const executor = await initializeAgentExecutorWithOptions(tools, model, {       agentType: \"zero-shot-react-description\",     });     \\```  Note: In async environments, the steps can be wrapped in a try-catch block to handle any exceptions that might occur during execution. As shown in some of the examples, the process can be aborted using an AbortController to cancel the process after a certain period of time for fail-safe reasons.  */   See the LangSmith trace for these two chains here  Next steps​  Since we're saving our inputs/outputs in memory we can ask followups to the LLM.  Keep in mind, we're not implementing an agent with tools so it must derive answers from the relevant documents in our store. Because of this it may return answers with hallucinated imports, classes or more.  const question2 =   \"How can I import and use the Wikipedia and File System tools from LangChain instead?\"; const result2 = await conversationalQaChain.invoke({   question: question2, });  console.log(result2); /** Here is how to import and utilize WikipediaQueryRun and File System tools in the LangChain codebase:  1. First, you have to import necessary tools and classes:  \\```javascript // for file system tools import { ReadFileTool, WriteFileTool, NodeFileStore } from \"langchain/tools\";  // for wikipedia tools import { WikipediaQueryRun } from \"langchain/tools\"; \\```  2. To use File System tools, you need an instance of File Store, either `NodeFileStore` for the server-side file system or `InMemoryFileStore` for in-memory file storage:  \\```javascript // example of instancing NodeFileStore for server-side file system const store = new NodeFileStore(); \\```  3. Then you can instantiate your file system tools with this store:  \\```javascript const tools = [new ReadFileTool({ store }), new WriteFileTool({ store })]; \\```  4. To use WikipediaQueryRun tool, first you have to instance it like this:  \\```javascript const wikipediaTool = new WikipediaQueryRun({   topKResults: 3,   maxDocContentLength: 4000, }); \\```  5. After that, you can use the `call` method of the created instance for making queries. For example, to query the Wikipedia for \"Langchain\":  \\```javascript const res = await wikipediaTool.call(\"Langchain\");  console.log(res); \\```  Note: This example assumes you're running the code in an asynchronous context. For synchronous context, you may need to adjust the code accordingly. \\*/   See the LangSmith trace for this run here  Previous Use local LLMs Next Tabular Question Answering Use case Overview Quickstart Loading RetrievalQA Chat Construct the chain Prompts Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/get_started/introduction",
    "title": "Introduction | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Get startedIntroduction Introduction  LangChain is a framework for developing applications powered by language models. It enables applications that:  Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)  This framework consists of several parts.  LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks. (Python only) LangServe: A library for deploying LangChain chains as a REST API. (Python only) LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.  Together, these products simplify the entire application lifecycle:  Develop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. Deploy: Turn any chain into an API with LangServe. LangChain Libraries​  The main value props of the LangChain packages are:  Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks  Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.  Get started​  Here's how to install LangChain, set up your environment, and start building.  We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.  Read up on our Security best practices to make sure you're developing safely with LangChain.  NOTE  These docs focus on the JS/TS LangChain library. Head here for docs on the Python LangChain library.  LangChain Expression Language (LCEL)​  LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.  Overview: LCEL and its benefits Interface: The standard interface for LCEL objects How-to: Key features of LCEL Cookbook: Example code for accomplishing common tasks Modules​  LangChain provides standard, extendable interfaces and integrations for the following modules:  Model I/O​  Interface with language models  Retrieval​  Interface with application-specific data  Agents​  Let models choose which tools to use given high-level directives  Examples, ecosystem, and resources​ Use cases​  Walkthroughs and techniques for common end-to-end use cases, like:  Document question answering RAG Agents and much more... Integrations​  LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.  API reference​  Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental packages.  Developer's guide​  Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.  Community​  Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM's.  Previous Get started Next Installation LangChain Libraries Get started LangChain Expression Language (LCEL) Modules Examples, ecosystem, and resources Use cases Integrations API reference Developer's guide Community Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/guides",
    "title": "Guides | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Debugging Deployment Evaluation Fallbacks Ecosystem Guides Guides  Design guides for key parts of the development process  📄️ Debugging  If you're building with LLMs, at some point something will break, and you'll need to debug.  🗃️ Deployment  2 items  🗃️ Evaluation  4 items  📄️ Fallbacks  When working with language models, you may often encounter issues from the underlying APIs, e.g. rate limits or downtime.  Previous Security Next Debugging Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/modules/",
    "title": "Modules | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Modules Modules  LangChain provides standard, extendable interfaces and external integrations for the following modules, listed from least to most complex:  Model I/O​  Interface with language models  Data connection​  Interface with application-specific data  Chains​  Construct sequences of calls  Agents​  Let chains choose which tools to use given high-level directives  Memory​  Persist application state between runs of a chain  Callbacks​  Log and stream intermediate steps of any chain  Experimental​  Experimental modules whose abstractions have not fully settled  Previous LangChain Expression Language (LCEL) Next Model I/O Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/ecosystem",
    "title": "Ecosystem | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Integrations LangSmith Ecosystem Ecosystem 🗃️ Integrations  5 items  🔗 LangSmith Previous Fallbacks Next Integrations Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/how_to/routing",
    "title": "Route between multiple runnables | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Route between multiple runnables Cancelling requests Use RunnableMaps Add message history (memory) Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageHow toRoute between multiple runnables Route between multiple Runnables  This notebook covers how to do routing in the LangChain Expression Language.  Routing allows you to create non-deterministic chains where the output of a previous step defines the next step. Routing helps provide structure and consistency around interactions with LLMs.  There are two ways to perform routing:  Using a RunnableBranch. Writing custom factory function that takes the input of a previous step and returns a runnable. Importantly, this should return a runnable and NOT actually execute.  We'll illustrate both methods using a two step sequence where the first step classifies an input question as being about LangChain, Anthropic, or Other, then routes to a corresponding prompt chain.  Using a RunnableBranch​  A RunnableBranch is initialized with a list of (condition, runnable) pairs and a default runnable. It selects which branch by passing each condition the input it's invoked with. It selects the first condition to evaluate to True, and runs the corresponding runnable to that condition with the input.  If no provided conditions match, it runs the default runnable.  Here's an example of what it looks like in action:  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/anthropic  import { PromptTemplate } from \"@langchain/core/prompts\"; import { StringOutputParser } from \"@langchain/core/output_parsers\"; import { RunnableBranch, RunnableSequence } from \"@langchain/core/runnables\"; import { ChatAnthropic } from \"@langchain/anthropic\";  const promptTemplate =   PromptTemplate.fromTemplate(`Given the user question below, classify it as either being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`.                                       Do not respond with more than one word.  <question> {question} </question>  Classification:`);  const model = new ChatAnthropic({   modelName: \"claude-2.1\", });  const classificationChain = RunnableSequence.from([   promptTemplate,   model,   new StringOutputParser(), ]);  const classificationChainResult = await classificationChain.invoke({   question: \"how do I call Anthropic?\", }); console.log(classificationChainResult);  /*   Anthropic */  const langChainChain = PromptTemplate.fromTemplate(   `You are an expert in langchain. Always answer questions starting with \"As Harrison Chase told me\". Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const anthropicChain = PromptTemplate.fromTemplate(   `You are an expert in anthropic. \\ Always answer questions starting with \"As Dario Amodei told me\". \\ Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const generalChain = PromptTemplate.fromTemplate(   `Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const branch = RunnableBranch.from([   [     (x: { topic: string; question: string }) =>       x.topic.toLowerCase().includes(\"anthropic\"),     anthropicChain,   ],   [     (x: { topic: string; question: string }) =>       x.topic.toLowerCase().includes(\"langchain\"),     langChainChain,   ],   generalChain, ]);  const fullChain = RunnableSequence.from([   {     topic: classificationChain,     question: (input: { question: string }) => input.question,   },   branch, ]);  const result1 = await fullChain.invoke({   question: \"how do I use Anthropic?\", });  console.log(result1);  /*   AIMessage {     content: ' As Dario Amodei told me, here are some tips for how to use Anthropic:\\n' +       '\\n' +       \"First, sign up for an account on Anthropic's website. This will give you access to their conversational AI assistant named Claude. \\n\" +       '\\n' +       \"Once you've created an account, you can have conversations with Claude through their web interface. Talk to Claude like you would talk to a person, asking questions, giving instructions, etc. Claude is trained to have natural conversations and be helpful.\\n\" +       '\\n' +       \"You can also integrate Claude into your own applications using Anthropic's API. This allows you to build Claude's conversational abilities into chatbots, virtual assistants, and other AI systems you develop.\\n\" +       '\\n' +       'Anthropic is constantly working on improving Claude, so its capabilities are always expanding. Make sure to check their blog and documentation to stay up to date on the latest features.\\n' +       '\\n' +       'The key is to interact with Claude regularly so it can learn from you. The more you chat with it, the better it will become at understanding you and having personalized conversations. Over time, Claude will feel more human-like as it accumulates more conversational experience.',     additional_kwargs: {}   } */  const result2 = await fullChain.invoke({   question: \"how do I use LangChain?\", });  console.log(result2);  /*   AIMessage {     content: ' As Harrison Chase told me, here is how you use LangChain:\\n' +       '\\n' +       'First, think carefully about what you want to ask or have the AI do. Frame your request clearly and specifically. Avoid vague or overly broad prompts that could lead to unhelpful or concerning responses. \\n' +       '\\n' +       'Next, type your question or request into the chat window and send it. Be patient as the AI processes your input and generates a response. The AI will do its best to provide a helpful answer or follow your instructions, but its capabilities are limited.\\n' +       '\\n' +       'Keep your requests simple at first. Ask basic questions or have the AI summarize content or generate basic text. As you get more comfortable, you can try having the AI perform more complex tasks like answering tricky questions, generating stories, or having a conversation.\\n' +       '\\n' +       \"Pay attention to the AI's responses. If they seem off topic, nonsensical, or concerning, rephrase your prompt to steer the AI in a better direction. You may need to provide additional clarification or context to get useful results.\\n\" +       '\\n' +       'Be polite and respectful towards the AI system. Remember, it is a tool designed to be helpful, harmless, and honest. Do not try to trick, confuse, or exploit it. \\n' +       '\\n' +       'I hope these tips help you have a safe, fun and productive experience using LangChain! Let me know if you have any other questions.',     additional_kwargs: {}   } */  const result3 = await fullChain.invoke({   question: \"what is 2 + 2?\", });  console.log(result3);  /*   AIMessage {     content: ' 4',     additional_kwargs: {}   } */  API Reference: PromptTemplate from @langchain/core/prompts StringOutputParser from @langchain/core/output_parsers RunnableBranch from @langchain/core/runnables RunnableSequence from @langchain/core/runnables ChatAnthropic from @langchain/anthropic Using a custom function​  You can also use a custom function to route between different outputs. Here's an example:  import { PromptTemplate } from \"@langchain/core/prompts\"; import { StringOutputParser } from \"@langchain/core/output_parsers\"; import { RunnableSequence } from \"@langchain/core/runnables\"; import { ChatAnthropic } from \"@langchain/anthropic\";  const promptTemplate =   PromptTemplate.fromTemplate(`Given the user question below, classify it as either being about \\`LangChain\\`, \\`Anthropic\\`, or \\`Other\\`.                                       Do not respond with more than one word.  <question> {question} </question>  Classification:`);  const model = new ChatAnthropic({   modelName: \"claude-2.1\", });  const classificationChain = RunnableSequence.from([   promptTemplate,   model,   new StringOutputParser(), ]);  const classificationChainResult = await classificationChain.invoke({   question: \"how do I call Anthropic?\", }); console.log(classificationChainResult);  /*   Anthropic */  const langChainChain = PromptTemplate.fromTemplate(   `You are an expert in langchain. Always answer questions starting with \"As Harrison Chase told me\". Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const anthropicChain = PromptTemplate.fromTemplate(   `You are an expert in anthropic. \\ Always answer questions starting with \"As Dario Amodei told me\". \\ Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const generalChain = PromptTemplate.fromTemplate(   `Respond to the following question:  Question: {question} Answer:` ).pipe(model);  const route = ({ topic }: { input: string; topic: string }) => {   if (topic.toLowerCase().includes(\"anthropic\")) {     return anthropicChain;   } else if (topic.toLowerCase().includes(\"langchain\")) {     return langChainChain;   } else {     return generalChain;   } };  const fullChain = RunnableSequence.from([   {     topic: classificationChain,     question: (input: { question: string }) => input.question,   },   route, ]);  const result1 = await fullChain.invoke({   question: \"how do I use Anthropic?\", });  console.log(result1);  /*   AIMessage {     content: ' As Dario Amodei told me, here are some tips for how to use Anthropic:\\n' +       '\\n' +       \"First, sign up for an account on Anthropic's website. This will give you access to their conversational AI assistant named Claude. \\n\" +       '\\n' +       \"Once you've created an account, you can have conversations with Claude through their web interface. Talk to Claude like you would talk to a person, asking questions, giving instructions, etc. Claude is trained to have natural conversations and be helpful.\\n\" +       '\\n' +       \"You can also integrate Claude into your own applications using Anthropic's API. This allows you to build Claude's conversational abilities into chatbots, virtual assistants, and other AI systems you develop.\\n\" +       '\\n' +       'Anthropic is constantly working on improving Claude, so its capabilities are always expanding. Make sure to check their blog and documentation to stay up to date on the latest features.\\n' +       '\\n' +       'The key is to interact with Claude regularly so it can learn from you. The more you chat with it, the better it will become at understanding you and having personalized conversations. Over time, Claude will feel more human-like as it accumulates more conversational experience.',     additional_kwargs: {}   } */  const result2 = await fullChain.invoke({   question: \"how do I use LangChain?\", });  console.log(result2);  /*   AIMessage {     content: ' As Harrison Chase told me, here is how you use LangChain:\\n' +       '\\n' +       'First, think carefully about what you want to ask or have the AI do. Frame your request clearly and specifically. Avoid vague or overly broad prompts that could lead to unhelpful or concerning responses. \\n' +       '\\n' +       'Next, type your question or request into the chat window and send it. Be patient as the AI processes your input and generates a response. The AI will do its best to provide a helpful answer or follow your instructions, but its capabilities are limited.\\n' +       '\\n' +       'Keep your requests simple at first. Ask basic questions or have the AI summarize content or generate basic text. As you get more comfortable, you can try having the AI perform more complex tasks like answering tricky questions, generating stories, or having a conversation.\\n' +       '\\n' +       \"Pay attention to the AI's responses. If they seem off topic, nonsensical, or concerning, rephrase your prompt to steer the AI in a better direction. You may need to provide additional clarification or context to get useful results.\\n\" +       '\\n' +       'Be polite and respectful towards the AI system. Remember, it is a tool designed to be helpful, harmless, and honest. Do not try to trick, confuse, or exploit it. \\n' +       '\\n' +       'I hope these tips help you have a safe, fun and productive experience using LangChain! Let me know if you have any other questions.',     additional_kwargs: {}   } */  const result3 = await fullChain.invoke({   question: \"what is 2 + 2?\", });  console.log(result3);  /*   AIMessage {     content: ' 4',     additional_kwargs: {}   } */  API Reference: PromptTemplate from @langchain/core/prompts StringOutputParser from @langchain/core/output_parsers RunnableSequence from @langchain/core/runnables ChatAnthropic from @langchain/anthropic Previous Interface Next Cancelling requests Using a RunnableBranch Using a custom function Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/modules/model_io/",
    "title": "Model I/O | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Quickstart Concepts Prompts LLMs Chat Models Output Parsers Retrieval Chains Agents More Security Guides Ecosystem ModulesModel I/O Model I/O  The core element of any language model application is... the model. LangChain gives you the building blocks to interface with any language model.  Conceptual Guide​  A conceptual explanation of messages, prompts, LLMs vs ChatModels, and output parsers. You should read this section before getting started.  Quick Start​  Covers the basics of getting started working with different types of models. You should walk through this section if you want to get an overview of the functionality.  Prompts​  This section deep dives into the different types of prompt templates and how to use them.  LLMs​  This section covers functionality related to the LLM class. This is a type of model that takes a text string as input and returns a text string.  ChatModels​  This section covers functionality related to the ChatModel class. This is a type of model that takes a list of messages as input and returns a message.  Output Parsers​  Output parsers are responsible for transforming the output of LLMs and ChatModels into more structured data. This section covers the different types of output parsers.  Previous Modules Next Model I/O Conceptual Guide Quick Start Prompts LLMs ChatModels Output Parsers Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/get_started/quickstart",
    "title": "Quickstart | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Get startedQuickstart Quickstart  In this quickstart we'll show you how to:  Get setup with LangChain and LangSmith Use the most basic and common components of LangChain: prompt templates, models, and output parsers Use LangChain Expression Language, the protocol that LangChain is built on and which facilitates component chaining Build a simple application with LangChain Trace your application with LangSmith  That's a fair amount to cover! Let's dive in.  Installation​  To install LangChain run:  npm Yarn pnpm npm install langchain   For more details, see our Installation guide.  LangSmith​  Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with LangSmith.  Note that LangSmith is not needed, but it is helpful. If you do want to use LangSmith, after you sign up at the link above, make sure to set your environment variables to start logging traces:  export LANGCHAIN_TRACING_V2=\"true\" export LANGCHAIN_API_KEY=\"...\"  Building with LangChain​  LangChain enables building application that connect external sources of data and computation to LLMs.  In this quickstart, we will walk through a few different ways of doing that:  We will start with a simple LLM chain, which just relies on information in the prompt template to respond. Next, we will build a retrieval chain, which fetches data from a separate database and passes that into the prompt template. We will then add in chat history, to create a conversation retrieval chain. This allows you interact in a chat manner with this LLM, so it remembers previous questions. Finally, we will build an agent - which utilizes and LLM to determine whether or not it needs to fetch data to answer questions.  We will cover these at a high level, but keep in mind there is a lot more to each piece! We will link to more in-depth docs as appropriate.  LLM Chain​  For this getting started guide, we will provide two options: using OpenAI (available via API) or using a local open source model.  OpenAI Local  First we'll need to install the LangChain OpenAI integration package:  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai   Accessing the API requires an API key, which you can get by creating an account and heading here. Once we have a key we'll want to set it as an environment variable by running:  export OPENAI_API_KEY=\"...\"   If you'd prefer not to set an environment variable you can pass the key in directly via the openAIApiKey named parameter when initiating the OpenAI Chat Model class:  import { ChatOpenAI } from \"@langchain/openai\";  const chatModel = new ChatOpenAI({   openAIApiKey: \"...\", });   Otherwise you can initialize without any params:  import { ChatOpenAI } from \"@langchain/openai\";  const chatModel = new ChatOpenAI({});   Once you've installed and initialized the LLM of your choice, we can try using it! Let's ask it what LangSmith is - this is something that wasn't present in the training data so it shouldn't have a very good response.  await chatModel.invoke(\"what is LangSmith?\");  AIMessage {   content: 'LangSmith refers to the combination of two surnames, Lang and Smith. It is most commonly used as a fictional or hypothetical name for a person or a company. This term may also refer to specific individuals or entities named LangSmith in certain contexts.',   additional_kwargs: { function_call: undefined, tool_calls: undefined } }   We can also guide it's response with a prompt template. Prompt templates are used to convert raw user input to a better input to the LLM.  import { ChatPromptTemplate } from \"@langchain/core/prompts\";  const prompt = ChatPromptTemplate.fromMessages([   [\"system\", \"You are a world class technical documentation writer.\"],   [\"user\", \"{input}\"], ]);   We can now combine these into a simple LLM chain:  const chain = prompt.pipe(chatModel);   We can now invoke it and ask the same question:  await chain.invoke({   input: \"what is LangSmith?\", });  AIMessage {   content: 'LangSmith is a powerful programming language created for high-performance software development. It is designed to be efficient, intuitive, and capable of handling complex computations and data manipulations. With its extensive set of features and libraries, LangSmith provides developers with the tools necessary to build robust and scalable applications.\\n' +     '\\n' +     'Some key features of LangSmith include:\\n' +     '\\n' +     '1. Strong typing: LangSmith enforces type safety, preventing common programming errors and ensuring code reliability.\\n' +     '\\n' +     '2. Advanced memory management: The language provides built-in memory management mechanisms, such as automatic garbage collection, to optimize memory usage and reduce the risk of memory leaks.\\n' +     '\\n' +     '3. Multi-paradigm support: LangSmith supports both procedural and object-oriented programming paradigms, giving developers the flexibility to choose the most suitable approach for their projects.\\n' +     '\\n' +     '4. Modular design: The language promotes modular programming, allowing developers to organize their code into reusable components for easier maintenance and collaboration.\\n' +     '\\n' +     '5. High-performance libraries: LangSmith offers a rich set of libraries for various domains, including graphics, networking, database access, and more. These libraries enhance productivity by providing pre-built solutions for common tasks.\\n' +     '\\n' +     '6. Interoperability: LangSmith enables seamless integration with other programming languages, allowing developers to leverage existing codebases and resources.\\n' +     '\\n' +     \"7. Extensibility: Developers can extend LangSmith's functionality through custom libraries and modules, allowing for the creation of domain-specific solutions.\\n\" +     '\\n' +     'Overall, LangSmith aims to provide a robust and efficient development environment for creating software applications across various domains, from scientific simulations to web development and beyond.',   additional_kwargs: { function_call: undefined, tool_calls: undefined } }   The model hallucinated an incorrect answer this time, but it did respond in a more proper tone for a technical writer!  The output of a ChatModel (and therefore, of this chain) is a message. However, it's often much more convenient to work with strings. Let's add a simple output parser to convert the chat message to a string.  import { StringOutputParser } from \"@langchain/core/output_parsers\";  const outputParser = new StringOutputParser();  const llmChain = prompt.pipe(chatModel).pipe(outputParser);  await llmChain.invoke({   input: \"what is LangSmith?\", });  LangSmith is a sophisticated online language translation tool. It leverages artificial intelligence and machine learning algorithms to provide accurate and efficient translation services across multiple languages. Whether it's translating documents, websites, or text snippets, LangSmith offers a seamless, user-friendly experience while maintaining the integrity and nuances of the original content. Its advanced features include context-aware translations, language customization options, and quality assurance checks, making it an invaluable tool for businesses, individuals, and language professionals alike.  Diving deeper​  We've now successfully set up a basic LLM chain. We only touched on the basics of prompts, models, and output parsers - for a deeper dive into everything mentioned here, see this section of documentation.  Retrieval Chain​  In order to properly answer the original question (\"what is LangSmith?\") and avoid hallucinations, we need to provide additional context to the LLM. We can do this via retrieval. Retrieval is useful when you have too much data to pass to the LLM directly. You can then use a retriever to fetch only the most relevant pieces and pass those in.  In this process, we will look up relevant documents from a Retriever and then pass them into the prompt. A Retriever can be backed by anything - a SQL table, the internet, etc - but in this instance we will populate a vector store and use that as a retriever. For more information on vectorstores, see this documentation.  First, we need to load the data that we want to index. We'll use a document loader that uses the popular Cheerio npm package as a peer dependency to parse data from webpages. Install it as shown below:  npm Yarn pnpm npm install cheerio   Then, use it like this:  import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";  const loader = new CheerioWebBaseLoader(   \"https://docs.smith.langchain.com/overview\" );  const docs = await loader.load();  console.log(docs.length); console.log(docs[0].pageContent.length);  45772   Note that the size of the loaded document is large and may exceed the maximum amount of data we can pass in a single model call. We can split the document into more manageable chunks to get around this limitation and to reduce the amount of distraction to the model using a text splitter:  import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";  const splitter = new RecursiveCharacterTextSplitter();  const splitDocs = await splitter.splitDocuments(docs);  console.log(splitDocs.length); console.log(splitDocs[0].pageContent.length);  60 441   Next, we need to index the loaded documents into a vectorstore. This requires a few components, namely an embedding model and a vectorstore.  There are many options for both components. Here are some examples for accessing via OpenAI and via local models:  OpenAI Local  Make sure you have the @langchain/openai package installed and the appropriate environment variables set (these are the same as needed for the model above).  import { OpenAIEmbeddings } from \"@langchain/openai\";  const embeddings = new OpenAIEmbeddings();   Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple in-memory demo vectorstore for simplicity's sake:  Note: If you are using local embeddings, this ingestion process may take some time depending on your local hardware.  import { MemoryVectorStore } from \"langchain/vectorstores/memory\";  const vectorstore = await MemoryVectorStore.fromDocuments(   splitDocs,   embeddings );   The LangChain vectorstore class will automatically prepare each raw document using the embeddings model.  Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question.  First, let's set up the chain that takes a question and the retrieved documents and generates an answer.  import { createStuffDocumentsChain } from \"langchain/chains/combine_documents\"; import { ChatPromptTemplate } from \"@langchain/core/prompts\";  const prompt =   ChatPromptTemplate.fromTemplate(`Answer the following question based only on the provided context:  <context> {context} </context>  Question: {input}`);  const documentChain = await createStuffDocumentsChain({   llm: chatModel,   prompt, });   If we wanted to, we could run this ourselves by passing in documents directly:  import { Document } from \"@langchain/core/documents\";  await documentChain.invoke({   input: \"what is LangSmith?\",   context: [     new Document({       pageContent:         \"LangSmith is a platform for building production-grade LLM applications.\",     }),   ], });   LangSmith is a platform for building production-grade Large Language Model (LLM) applications.   However, we want the documents to first come from the retriever we just set up. That way, for a given question we can use the retriever to dynamically select the most relevant documents and pass those in.  import { createRetrievalChain } from \"langchain/chains/retrieval\";  const retriever = vectorstore.asRetriever();  const retrievalChain = await createRetrievalChain({   combineDocsChain: documentChain,   retriever, });   We can now invoke this chain. This returns an object - the response from the LLM is in the answer key:  const result = await retrievalChain.invoke({   input: \"what is LangSmith?\", });  console.log(result.answer);   LangSmith is a tool developed by LangChain that is used for debugging and monitoring LLMs, chains, and agents in order to improve their performance and reliability for use in production.  TIP  Check out this public LangSmith trace showing the steps of the retrieval chain.  This answer should be much more accurate!  Diving Deeper​  We've now successfully set up a basic retrieval chain. We only touched on the basics of retrieval - for a deeper dive into everything mentioned here, see this section of documentation.  Conversation Retrieval Chain​  The chain we've created so far can only answer single questions. One of the main types of LLM applications that people are building are chat bots. So how do we turn this chain into one that can answer follow up questions?  We can still use the createRetrievalChain function, but we need to change two things:  The retrieval method should now not just work on the most recent input, but rather should take the whole history into account. The final LLM chain should likewise take the whole history into account. Updating Retrieval​  In order to update retrieval, we will create a new chain. This chain will take in the most recent input (input) and the conversation history (chat_history) and use an LLM to generate a search query.  import { createHistoryAwareRetriever } from \"langchain/chains/history_aware_retriever\"; import { MessagesPlaceholder } from \"@langchain/core/prompts\";  const historyAwarePrompt = ChatPromptTemplate.fromMessages([   new MessagesPlaceholder(\"chat_history\"),   [\"user\", \"{input}\"],   [     \"user\",     \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\",   ], ]);  const historyAwareRetrieverChain = await createHistoryAwareRetriever({   llm: chatModel,   retriever,   rephrasePrompt: historyAwarePrompt, });   We can test this \"history aware retriever\" out by creating a situation where the user is asking a follow up question:  import { HumanMessage, AIMessage } from \"@langchain/core/messages\";  const chatHistory = [   new HumanMessage(\"Can LangSmith help test my LLM applications?\"),   new AIMessage(\"Yes!\"), ];  await historyAwareRetrieverChain.invoke({   chat_history: chatHistory,   input: \"Tell me how!\", });  TIP  Here's a public LangSmith trace of the above run!  The above trace illustrates that this returns documents about testing in LangSmith. This is because the LLM generated a new query, combining the chat history with the follow up question.  Now that we have this new retriever, we can create a new chain to continue the conversation with these retrieved documents in mind:  const historyAwareRetrievalPrompt = ChatPromptTemplate.fromMessages([   [     \"system\",     \"Answer the user's questions based on the below context:\\n\\n{context}\",   ],   new MessagesPlaceholder(\"chat_history\"),   [\"user\", \"{input}\"], ]);  const historyAwareCombineDocsChain = await createStuffDocumentsChain({   llm: chatModel,   prompt: historyAwareRetrievalPrompt, });  const conversationalRetrievalChain = await createRetrievalChain({   retriever: historyAwareRetrieverChain,   combineDocsChain: historyAwareCombineDocsChain, });   Let's now test this out end-to-end!  const result2 = await conversationalRetrievalChain.invoke({   chat_history: [     new HumanMessage(\"Can LangSmith help test my LLM applications?\"),     new AIMessage(\"Yes!\"),   ],   input: \"tell me how\", });  console.log(result2.answer);  LangSmith can help test and debug your LLM (Language Model) applications in several ways:  1. Exact Input/Output Visualization: LangSmith provides a straightforward visualization of the exact inputs and outputs for all LLM calls. This helps you understand the specific inputs provided to the model and the corresponding output generated.  2. Editing Prompts: If you encounter a bad output or want to experiment with different inputs, you can edit the prompts directly in LangSmith. By modifying the prompt, you can observe the resulting changes in the output. LangSmith includes a playground feature where you can modify prompts and re-run them multiple times to analyze the impact on the output.  3. Constructing Datasets: LangSmith simplifies the process of constructing datasets for testing changes in your application. You can quickly edit examples and add them to datasets, expanding your evaluation sets or fine-tuning your model for improved quality or reduced costs.  4. Monitoring and Troubleshooting: Once your application is ready for production, LangSmith can be used to monitor its performance. You can log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. LangSmith also allows you to associate feedback programmatically with runs, enabling you to track performance over time and pinpoint underperforming data points.  In summary, LangSmith helps you test, debug, and monitor your LLM applications, providing tools to visualize inputs/outputs, edit prompts, construct datasets, and monitor performance.  TIP  Here's a public LangSmith trace of the above run!  We can see that this gives a coherent answer - we've successfully turned our retrieval chain into a chatbot!  Agent​  We've so far created examples of chains - where each step is known ahead of time. The final thing we will create is an agent - where the LLM decides what steps to take.  NOTE: for this example we will only show how to create an agent using OpenAI models, as local models runnable on consumer hardware are not reliable enough yet.  One of the first things to do when building an agent is to decide what tools it should have access to. For this example, we will give the agent access two tools:  The retriever we just created. This will let it easily answer questions about LangSmith A search tool. This will let it easily answer questions that require up to date information.  First, let's set up a tool for the retriever we just created:  import { createRetrieverTool } from \"langchain/tools/retriever\";  const retrieverTool = await createRetrieverTool(retriever, {   name: \"langsmith_search\",   description:     \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\", });   The search tool that we will use is Tavily. This will require you to create an API key (they have generous free tier). After signing up and creating one in their dashboard, you need to set it as an environment variable:  export TAVILY_API_KEY=...   If you do not want to set up an API key, you can skip creating this tool.  import { TavilySearchResults } from \"@langchain/community/tools/tavily_search\";  const searchTool = new TavilySearchResults();   We can now create a list of the tools we want to work with:  const tools = [retrieverTool, searchTool];   Now that we have the tools, we can create an agent to use them and an executor to run the agent. We will go over this pretty quickly. For a deeper dive into what exactly is going on, check out the agent documentation pages.  import { pull } from \"langchain/hub\"; import { createOpenAIFunctionsAgent, AgentExecutor } from \"langchain/agents\";  // Get the prompt to use - you can modify this! // If you want to see the prompt in full, you can at: // https://smith.langchain.com/hub/hwchase17/openai-functions-agent const agentPrompt = await pull<ChatPromptTemplate>(   \"hwchase17/openai-functions-agent\" );  const agentModel = new ChatOpenAI({   modelName: \"gpt-3.5-turbo-1106\",   temperature: 0, });  const agent = await createOpenAIFunctionsAgent({   llm: agentModel,   tools,   prompt: agentPrompt, });  const agentExecutor = new AgentExecutor({   agent,   tools,   verbose: true, });   We can now invoke the agent and see how it responds! We can ask it questions about LangSmith:  const agentResult = await agentExecutor.invoke({   input: \"how can LangSmith help with testing?\", });  console.log(agentResult.output);  LangSmith can help with testing in the following ways:  1. Debugging: LangSmith helps in debugging unexpected end results, agent looping, slow chains, and token usage. It provides a visualization of the exact inputs/outputs to all LLM calls, making it easier to understand them.  2. Modifying Prompts: LangSmith allows you to modify prompts and observe resulting changes to the output. This feature supports OpenAI and Anthropic models and works for LLM and Chat Model calls.  3. Dataset Construction: LangSmith simplifies dataset construction for testing changes. It provides a straightforward visualization of inputs/outputs to LLM calls, allowing you to understand them easily.  4. Monitoring: LangSmith can be used to monitor applications in production by logging all traces, visualizing latency and token usage statistics, and troubleshooting specific issues as they arise. It also allows for programmatically associating feedback with runs to track performance over time.  Overall, LangSmith is a valuable tool for testing, debugging, and monitoring applications that utilize language models and agents.  TIP  Here's a public LangSmith trace of the above run!  We can ask it about the weather:  const agentResult2 = await agentExecutor.invoke({   input: \"what is the weather in SF?\", });  console.log(agentResult2.output);  The weather in San Francisco, California for December 29, 2023 is expected to have average high temperatures of 50 to 65 °F and average low temperatures of 40 to 55 °F. There may be periods of rain with a high of 59°F and winds from the SSE at 10 to 20 mph. For more detailed information, you can visit [this link](https://www.weathertab.com/en/g/o/12/united-states/california/san-francisco/).  TIP  Here's a public LangSmith trace of the above run!  We can have conversations with it:  const agentResult3 = await agentExecutor.invoke({   chat_history: [     new HumanMessage(\"Can LangSmith help test my LLM applications?\"),     new AIMessage(\"Yes!\"),   ],   input: \"Tell me how\", });  console.log(agentResult3.output);  LangSmith can help test your LLM applications by providing the following features: 1. Debugging: LangSmith helps in debugging LLMs, chains, and agents by providing a visualization of the exact inputs/outputs to all LLM calls, allowing you to understand them easily. 2. Prompt Editing: You can modify the prompt and re-run it to observe the resulting changes to the output as many times as needed using LangSmith's playground feature. 3. Monitoring: LangSmith can be used to monitor your application, log all traces, visualize latency and token usage statistics, and troubleshoot specific issues as they arise. 4. Feedback and Dataset Expansion: You can associate feedback programmatically with runs, add examples to datasets, and fine-tune a model for improved quality or reduced costs. 5. Failure Analysis: LangSmith allows you to identify how your chain can fail and monitor these failures, which can be valuable data points for testing future chain versions.  These features make LangSmith a valuable tool for testing and improving LLM applications.  TIP  Here's a public LangSmith trace of the above run!  Diving Deeper​  We've now successfully set up a basic agent. We only touched on the basics of agents - for a deeper dive into everything mentioned here, see this section of documentation.  Next steps​  We've touched on how to build an application with LangChain, and how to trace it with LangSmith. There are a lot more features than we can cover here. To continue on your journey, we recommend you read the following (in order):  All of these features are backed by LangChain Expression Language (LCEL) - a way to chain these components together. Check out that documentation to better understand how to create custom chains. Model I/O covers more details of prompts, LLMs, and output parsers. Retrieval covers more details of everything related to retrieval. Agents covers details of everything related to agents. Explore common end-to-end use cases. Read up on LangSmith, the platform for debugging, testing, monitoring and more. Previous Installation Next LangChain Expression Language (LCEL) Installation LangSmith Building with LangChain LLM Chain Diving deeper Retrieval Chain Diving Deeper Conversation Retrieval Chain Agent Diving Deeper Next steps Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/why",
    "title": "Why use LCEL? | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageWhy use LCEL? Why use LCEL?  The LangChain Expression Language was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains (we’ve seen folks successfully running in production LCEL chains with 100s of steps). To highlight a few of the reasons you might want to use LCEL:  optimised parallel execution: whenever your LCEL chains have steps that can be executed in parallel (eg if you fetch documents from multiple retrievers) we automatically do it, for the smallest possible latency. support for retries and fallbacks: more recently we’ve added support for configuring retries and fallbacks for any part of your LCEL chain. This is a great way to make your chains more reliable at scale. We’re currently working on adding streaming support for retries/fallbacks, so you can get the added reliability without any latency cost. accessing intermediate results: for more complex chains it’s often very useful to access the results of intermediate steps even before the final output is produced. This can be used let end-users know something is happening, or even just to debug your chain. We’ve added support for streaming intermediate results, and it’s available on every LangServe server. tracing with LangSmith: all chains built with LCEL have first-class tracing support, which can be used to debug your chains, or to understand what’s happening in production. To enable this all you have to do is add your LangSmith API key as an environment variable. Previous Agents Next LangChain Expression Language (LCEL) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/cookbook/",
    "title": "Cookbook | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Prompt + LLM Multiple chains Retrieval augmented generation (RAG) Querying a SQL DB Adding memory Using tools Agents Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageCookbook Cookbook  Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.  Several pages in this section include embedded interactive screencasts from Scrimba. They're a great resource for getting started - you can edit the included code whenever you want, just as if you were pair programming with a teacher!  📄️ Prompt + LLM  One of the most foundational Expression Language compositions is taking:  📄️ Multiple chains  Runnables can be used to combine multiple Chains together:  📄️ Retrieval augmented generation (RAG)  Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:  📄️ Querying a SQL DB  We can replicate our SQLDatabaseChain with Runnables.  📄️ Adding memory  This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.  📄️ Using tools  Tools are also runnables, and can therefore be used within a chain:  📄️ Agents  You can pass a Runnable into an agent.  Previous Add message history (memory) Next Prompt + LLM Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/autonomous_agents/",
    "title": "Autonomous Agents | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents SalesGPT AutoGPT BabyAGI Chatbots Extraction Use casesAutonomous Agents Autonomous Agents  Autonomous Agents are agents that designed to be more long running. You give them one or multiple long term goals, and they independently execute towards those goals. The applications combine tool usage and long term memory.  At the moment, Autonomous Agents are fairly experimental and based off of other open-source projects. By implementing these open source projects in LangChain primitives we can get the benefits of LangChain - easy switching and experimenting with multiple LLMs, usage of different vectorstores as memory, usage of LangChain's collection of tools.  📄️ SalesGPT  This notebook demonstrates an implementation of a Context-Aware AI Sales agent with a Product Knowledge Base.  📄️ AutoGPT  Original Repo//github.com/Significant-Gravitas/Auto-GPT  📄️ BabyAGI  Original Repo//github.com/yoheinakajima/babyagi  Previous Violation of Expectations Chain Next SalesGPT Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/get_started",
    "title": "Get started | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageGet started Get started  LCEL makes it easy to build complex chains from basic components, and supports out of the box functionality such as streaming, parallelism, and logging.  Basic example: prompt + model + output parser​  The most basic and common use case is chaining a prompt template and a model together. To see how this works, let's create a chain that takes a topic and generates a joke:  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai @langchain/community  import { ChatOpenAI } from \"@langchain/openai\"; import { ChatPromptTemplate } from \"@langchain/core/prompts\"; import { StringOutputParser } from \"@langchain/core/output_parsers\";  const prompt = ChatPromptTemplate.fromMessages([   [\"human\", \"Tell me a short joke about {topic}\"], ]); const model = new ChatOpenAI({}); const outputParser = new StringOutputParser();  const chain = prompt.pipe(model).pipe(outputParser);  const response = await chain.invoke({   topic: \"ice cream\", }); console.log(response); /** Why did the ice cream go to the gym? Because it wanted to get a little \"cone\"ditioning!  */  API Reference: ChatOpenAI from @langchain/openai ChatPromptTemplate from @langchain/core/prompts StringOutputParser from @langchain/core/output_parsers TIP  LangSmith trace  Notice in this line we're chaining our prompt, LLM model and output parser together:  const chain = prompt.pipe(model).pipe(outputParser);   The .pipe() method allows for chaining together any number of runnables. It will pass the output of one through to the input of the next.  Here, the prompt is passed a topic and when invoked it returns a formatted string with the {topic} input variable replaced with the string we passed to the invoke call. That string is then passed as the input to the LLM which returns a BaseMessage object. Finally, the output parser takes that BaseMessage object and returns the content of that object as a string.  1. Prompt​  prompt is a BasePromptTemplate, which means it takes in an object of template variables and produces a PromptValue. A PromptValue is a wrapper around a completed prompt that can be passed to either an LLM (which takes a string as input) or ChatModel (which takes a sequence of messages as input). It can work with either language model type because it defines logic both for producing BaseMessages and for producing a string.  import { ChatPromptTemplate } from \"@langchain/core/prompts\";  const prompt = ChatPromptTemplate.fromMessages([   [\"human\", \"Tell me a short joke about {topic}\"], ]); const promptValue = await prompt.invoke({ topic: \"ice cream\" }); console.log(promptValue); /** ChatPromptValue {   messages: [     HumanMessage {       content: 'Tell me a short joke about ice cream',       name: undefined,       additional_kwargs: {}     }   ] }  */ const promptAsMessages = promptValue.toChatMessages(); console.log(promptAsMessages); /** [   HumanMessage {     content: 'Tell me a short joke about ice cream',     name: undefined,     additional_kwargs: {}   } ]  */ const promptAsString = promptValue.toString(); console.log(promptAsString); /** Human: Tell me a short joke about ice cream  */  API Reference: ChatPromptTemplate from @langchain/core/prompts 2. Model​  The PromptValue is then passed to model. In this case our model is a ChatModel, meaning it will output a BaseMessage.  import { ChatOpenAI } from \"@langchain/openai\";  const model = new ChatOpenAI({}); const promptAsString = \"Human: Tell me a short joke about ice cream\";  const response = await model.invoke(promptAsString); console.log(response); /** AIMessage {   content: 'Sure, here you go: Why did the ice cream go to school? Because it wanted to get a little \"sundae\" education!',   name: undefined,   additional_kwargs: { function_call: undefined, tool_calls: undefined } }  */  API Reference: ChatOpenAI from @langchain/openai  If our model was an LLM, it would output a string.  import { OpenAI } from \"@langchain/openai\";  const model = new OpenAI({}); const promptAsString = \"Human: Tell me a short joke about ice cream\";  const response = await model.invoke(promptAsString); console.log(response); /** Why did the ice cream go to therapy?  Because it was feeling a little rocky road.  */  API Reference: OpenAI from @langchain/openai 3. Output parser​  And lastly we pass our model output to the outputParser, which is a BaseOutputParser meaning it takes either a string or a BaseMessage as input. The StringOutputParser specifically simple converts any input into a string.  import { AIMessage } from \"@langchain/core/messages\"; import { StringOutputParser } from \"@langchain/core/output_parsers\";  const outputParser = new StringOutputParser(); const message = new AIMessage(   'Sure, here you go: Why did the ice cream go to school? Because it wanted to get a little \"sundae\" education!' ); const parsed = await outputParser.invoke(message); console.log(parsed); /** Sure, here you go: Why did the ice cream go to school? Because it wanted to get a little \"sundae\" education!  */  API Reference: AIMessage from @langchain/core/messages StringOutputParser from @langchain/core/output_parsers RAG Search Example​  For our next example, we want to run a retrieval-augmented generation chain to add some context when responding to questions.  import { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\"; import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\"; import { Document } from \"@langchain/core/documents\"; import { ChatPromptTemplate } from \"@langchain/core/prompts\"; import {   RunnableLambda,   RunnableMap,   RunnablePassthrough, } from \"@langchain/core/runnables\"; import { StringOutputParser } from \"@langchain/core/output_parsers\";  const vectorStore = await HNSWLib.fromDocuments(   [     new Document({ pageContent: \"Harrison worked at Kensho\" }),     new Document({ pageContent: \"Bears like to eat honey.\" }),   ],   new OpenAIEmbeddings() ); const retriever = vectorStore.asRetriever(1);  const prompt = ChatPromptTemplate.fromMessages([   [     \"ai\",     `Answer the question based on only the following context:    {context}`,   ],   [\"human\", \"{question}\"], ]); const model = new ChatOpenAI({}); const outputParser = new StringOutputParser();  const setupAndRetrieval = RunnableMap.from({   context: new RunnableLambda({     func: (input: string) =>       retriever.invoke(input).then((response) => response[0].pageContent),   }).withConfig({ runName: \"contextRetriever\" }),   question: new RunnablePassthrough(), }); const chain = setupAndRetrieval.pipe(prompt).pipe(model).pipe(outputParser);  const response = await chain.invoke(\"Where did Harrison work?\"); console.log(response); /** Harrison worked at Kensho.  */  API Reference: ChatOpenAI from @langchain/openai OpenAIEmbeddings from @langchain/openai HNSWLib from @langchain/community/vectorstores/hnswlib Document from @langchain/core/documents ChatPromptTemplate from @langchain/core/prompts RunnableLambda from @langchain/core/runnables RunnableMap from @langchain/core/runnables RunnablePassthrough from @langchain/core/runnables StringOutputParser from @langchain/core/output_parsers TIP  LangSmith trace  In this chain we add some extra logic around retrieving context from a vector store.  We first instantiated our model, vector store and output parser. Then we defined our prompt, which takes in two input variables:  context -> this is a string which is returned from our vector store based on a semantic search from the input. question -> this is the question we want to ask.  Next we created a setupAndRetriever runnable. This has two components which return the values required by our prompt:  context -> this is a RunnableLambda which takes the input from the .invoke() call, makes a request to our vector store, and returns the first result. question -> this uses a RunnablePassthrough which simply passes whatever the input was through to the next step, and in our case it returns it to the key in the object we defined.  Both of these are wrapped inside a RunnableMap. This is a special type of runnable that takes an object of runnables and executes them all in parallel. It then returns an object with the same keys as the input object, but with the values replaced with the output of the runnables.  Finally, we pass the output of the setupAndRetriever to our prompt and then to our model and outputParser as before.  Previous LangChain Expression Language (LCEL) Next Interface Basic example: prompt + model + output parser 1. Prompt 2. Model 3. Output parser RAG Search Example Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/modules/agents/",
    "title": "Agents | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents Quick start Concepts Agent Types How-to Tools More Security Guides Ecosystem ModulesAgents Agents  The core idea of agents is to use a language model to choose a sequence of actions to take. In chains, a sequence of actions is hardcoded (in code). In agents, a language model is used as a reasoning engine to determine which actions to take and in which order.  Quick Start​  For a quick start to working with agents, please check out this getting started guide. This covers basics like initializing an agent, creating tools, and adding memory.  Concepts​  There are several key concepts to understand when building agents: Agents, AgentExecutor, Tools, Toolkits. For an in depth explanation, please check out this conceptual guide.  Agent Types​  There are many different types of agents to use. For a overview of the different types and when to use them, please check out this section.  Tools​  Agents are only as good as the tools they have. For a comprehensive guide on tools, please see this section.  How To Guides​  Agents have a lot of related functionality! Check out various guides including:  Building a custom agent Streaming (of both intermediate steps and tokens) Building an agent that returns structured output Lots of functionality around using AgentExecutor, including: handling parsing errors, returning intermediate steps, and capping the max number of iterations. Previous Chains Next Quick start Quick Start Concepts Agent Types Tools How To Guides Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/modules/chains/",
    "title": "Chains | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem ModulesChains Chains  Chains refer to sequences of calls - whether to an LLM, a tool, or a data preprocessing step. The primary supported way to do this is with LCEL.  LCEL is great for constructing your own chains, but it’s also nice to have chains that you can use off-the-shelf. There are two types of off-the-shelf chains that LangChain supports:  Chains that are built with LCEL. In this case, LangChain offers a higher-level constructor method. However, all that is being done under the hood is constructing a chain with LCEL. [Legacy] Chains constructed by subclassing from a legacy Chain class. These chains do not use LCEL under the hood but are rather standalone classes.  We are working creating methods that create LCEL versions of all chains. We are doing this for a few reasons.  Chains constructed in this way are nice because if you want to modify the internals of a chain you can simply modify the LCEL. These chains natively support streaming, async, and batch out of the box. These chains automatically get observability at each step.  This page contains two lists. First, a list of all LCEL chain constructors. Second, a list of all legacy Chains.  LCEL Chains​  Below is a table of all LCEL chain constructors. In addition, we report on:  Chain Constructor​  The constructor function for this chain. These are all methods that return LCEL runnables. We also link to the API documentation.  Function Calling​  Whether this requires OpenAI function calling.  Other Tools​  What other tools (if any) are used in this chain.  When to Use​  Our commentary on when to use this chain.  Chain Constructor Function Calling Other Tools When to Use createStuffDocumentsChain   This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using. createOpenAIFnRunnable ✅  If you want to use OpenAI function calling to OPTIONALLY structure an output response. You may pass in multiple functions for the chain to call, but it does not have to call it. createStructuredOutputRunnable ✅  If you want to use OpenAI function calling to FORCE the LLM to respond with a certain function. You may only pass in one function, and the chain will ALWAYS return this response. createHistoryAwareRetriever  Retriever This chain takes in conversation history and then uses that to generate a search query which is passed to the underlying retriever. createRetrievalChain  Retriever This chain takes in a user inquiry, which is then passed to the retriever to fetch relevant documents. Those documents (and original inputs) are then passed to an LLM to generate a response Legacy Chains​  Below we report on the legacy chain types that exist. We will maintain support for these until we are able to create a LCEL alternative. We cover:  Chain​  Name of the chain, or name of the constructor method. If constructor method, this will return a Chain subclass.  Function Calling​  Whether this requires OpenAI Function Calling.  Other Tools​  Other tools used in the chain.  When to Use​  Our commentary on when to use.  Chain Function Calling Other Tools When to Use  createOpenAPIChain  OpenAPI Spec Similar to APIChain, this chain is designed to interact with APIs. The main difference is this is optimized for ease of use with OpenAPI endpoints.  ConversationalRetrievalQAChain  Retriever This chain can be used to have conversations with a document. It takes in a question and (optional) previous conversation history. If there is previous conversation history, it uses an LLM to rewrite the conversation into a query to send to a retriever (otherwise it just uses the newest user input). It then fetches those documents and passes them (along with the conversation) to an LLM to respond.  StuffDocumentsChain   This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.  MapReduceDocumentsChain   This chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.  RefineDocumentsChain   This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents to refine its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).  ConstitutionalChain   This chain answers, then attempts to refine its answer based on constitutional principles that are provided. Use this when you want to enforce that a chain's answer follows some principles.  LLMChain    This chain simply combines a prompt with an LLM and an output parser. The recommended way to do this is just to use LCEL. GraphCypherQAChain  A graph that works with Cypher query language This chain constructs an Cypher query from natural language, executes that query against the graph, and then passes the results back to an LLM to respond.  createExtractionChain ✅  Uses OpenAI Function calling to extract information from text.  createExtractionChainFromZod ✅  Uses OpenAI Function calling and a Zod schema to extract information from text.  SqlDatabaseChain  Answers questions by generating and running SQL queries for a provided database.   LLMRouterChain   This chain uses an LLM to route between potential options.  MultiPromptChain   This chain routes input between multiple prompts. Use this when you have multiple potential prompts you could use to respond and want to route to just one.  MultiRetrievalQAChain  Retriever This chain uses an LLM to route input questions to the appropriate retriever for question answering.  loadQAChain  Retriever Does question answering over documents you pass in, and cites it sources. Use this over RetrievalQAChain when you want to pass in the documents directly (rather than rely on a passed retriever to get them).  APIChain  Requests Wrapper This chain uses an LLM to convert a query into an API request, then executes that request, gets back a response, and then passes that request to an LLM to respond. Prefer createOpenAPIChain if you have a spec available.  Previous Neo4j Next Agents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/modules/data_connection/",
    "title": "Retrieval | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Document loaders Text Splitters Retrievers Text embedding models Vector stores Experimental Chains Agents More Security Guides Ecosystem ModulesRetrieval Retrieval  Many LLM applications require user-specific data that is not part of the model's training set. The primary way of accomplishing this is through Retrieval Augmented Generation (RAG). In this process, external data is retrieved and then passed to the LLM when doing the generation step.  LangChain provides all the building blocks for RAG applications - from simple to complex. This section of the documentation covers everything related to the retrieval step - e.g. the fetching of the data. Although this sounds simple, it can be subtly complex. This encompasses several key modules.  Document loaders  Load documents from many different sources. LangChain provides many different document loaders as well as integrations with other major providers in the space, such as Unstructured. We provide integrations to load all types of documents (html, PDF, code) from all types of locations (private s3 buckets, public websites).  Text Splitting  A key part of retrieval is fetching only the relevant parts of documents. This involves several transformation steps in order to best prepare the documents for retrieval. One of the primary ones here is splitting (or chunking) a large document into smaller chunks. LangChain provides several different algorithms for doing this, as well as logic optimized for specific document types (code, markdown, etc).  Text embedding models  Another key part of retrieval has become creating embeddings for documents. Embeddings capture the semantic meaning of text, allowing you to quickly and efficiently find other pieces of text that are similar. LangChain provides integrations with different embedding providers and methods, from open-source to proprietary API, allowing you to choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between models.  Vector stores  With the rise of embeddings, there has emerged a need for databases to support efficient storage and searching of these embeddings. LangChain provides integrations with many different vectorstores, from open-source local ones to cloud-hosted proprietary ones, allowing you choose the one best suited for your needs. LangChain exposes a standard interface, allowing you to easily swap between vector stores.  Retrievers  Once the data is in the database, you still need to retrieve it. LangChain supports many different retrieval algorithms and is one of the places where we add the most value. We support basic methods that are easy to get started - namely simple semantic search. However, we have also added a collection of algorithms on top of this to increase performance. These include:  Parent Document Retriever: This allows you to create multiple embeddings per parent document, allowing you to look up smaller chunks but return larger context. Self Query Retriever: User questions often contain reference to something that isn't just semantic, but rather expresses some logic that can best be represented as a metadata filter. Self-query allows you to parse out the semantic part of a query from other metadata filters present in the query And more! Previous Structured output parser Next Document loaders Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases",
    "title": "Use cases | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use cases Use cases  Walkthroughs of common end-to-end use cases  🗃️ QA and Chat over Documents  3 items  🗃️ Retrieval-augmented generation (RAG)  1 items  📄️ Tabular Question Answering  Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.  📄️ Interacting with APIs  Lots of data and information is stored behind APIs.  📄️ Summarization  A common use case is wanting to summarize long documents.  🗃️ Agent Simulations  2 items  🗃️ Autonomous Agents  3 items  📄️ Chatbots  Language models are good at producing text, which makes them ideal for creating chatbots.  📄️ Extraction  Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:  Next QA and Chat over Documents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/question_answering/",
    "title": "QA and Chat over Documents | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Advanced Conversational QA Conversational Retrieval Agents Use local LLMs Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesQA and Chat over Documents QA and Chat over Documents  Chat and Question-Answering (QA) over data are popular LLM use-cases.  data can include many things, including:  Unstructured data (e.g., PDFs) Structured data (e.g., SQL) Code (e.g., Python)  Below we will review Chat and QA on Unstructured data.  Unstructured data can be loaded from many sources.  Check out the document loader integrations here to browse the set of supported loaders.  Each loader returns data as a LangChain Document.  Documents are turned into a Chat or QA app following the general steps below:  Splitting: Text splitters break Documents into splits of specified size Storage: Storage (e.g., often a vectorstore) will house and often embed the splits Retrieval: The app retrieves splits from storage (e.g., often with similar embeddings to the input question) Output: An LLM produces an answer using a prompt that includes the question and the retrieved splits  Quickstart​  Let's load this blog post on agents as an example Document.  We'll have a QA app in a few lines of code.  First, set environment variables and install packages required for the guide:  > yarn add cheerio # Or load env vars in your preferred way: > export OPENAI_API_KEY=\"...\"  1. Loading, Splitting, Storage​ 1.1 Getting started​  Specify a Document loader.  // Document loader import { CheerioWebBaseLoader } from \"langchain/document_loaders/web/cheerio\";  const loader = new CheerioWebBaseLoader(   \"https://lilianweng.github.io/posts/2023-06-23-agent/\" ); const data = await loader.load();   Split the Document into chunks for embedding and vector storage.  import { RecursiveCharacterTextSplitter } from \"langchain/text_splitter\";  const textSplitter = new RecursiveCharacterTextSplitter({   chunkSize: 500,   chunkOverlap: 0, });  const splitDocs = await textSplitter.splitDocuments(data);   Embed and store the splits in a vector database (for demo purposes we use an unoptimized, in-memory example but you can browse integrations here):  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { OpenAIEmbeddings } from \"@langchain/openai\"; import { MemoryVectorStore } from \"langchain/vectorstores/memory\";  const embeddings = new OpenAIEmbeddings();  const vectorStore = await MemoryVectorStore.fromDocuments(   splitDocs,   embeddings );   Here are the three pieces together:  1.2 Going Deeper​ 1.2.1 Integrations​  Document Loaders  Browse document loader integrations here.  See further documentation on loaders here.  Document Transformers  All can ingest loaded Documents and process them (e.g., split).  See further documentation on transformers here.  Vectorstores  Browse vectorstore integrations here.  See further documentation on vectorstores here.  2. Retrieval​ 2.1 Getting started​  Retrieve relevant splits for any question using similarity_search.  const relevantDocs = await vectorStore.similaritySearch(   \"What is task decomposition?\" );  console.log(relevantDocs.length);  // 4  2.2 Going Deeper​ 2.2.1 Retrieval​  Vectorstores are commonly used for retrieval.  But, they are not the only option.  For example, SVMs (see thread here) can also be used.  LangChain has many retrievers and retrieval methods including, but not limited to, vectorstores.  All retrievers implement some common methods, such as getRelevantDocuments().  3. QA​ 3.1 Getting started​  Distill the retrieved documents into an answer using an LLM (e.g., gpt-3.5-turbo) with RetrievalQA chain.  import { RetrievalQAChain } from \"langchain/chains\"; import { ChatOpenAI } from \"@langchain/openai\";  const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }); const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever());  const response = await chain.call({   query: \"What is task decomposition?\", }); console.log(response);  /*   {     text: 'Task decomposition refers to the process of breaking down a larger task into smaller, more manageable subgoals. By decomposing a task, it becomes easier for an agent or system to handle complex tasks efficiently. Task decomposition can be done through various methods such as using prompting or task-specific instructions, or through human inputs. It helps in planning and organizing the steps required to complete a task effectively.'   } */  3.2 Going Deeper​ 3.2.1 Integrations​  LLMs  Browse LLM integrations and further documentation here. 3.2.2 Customizing the prompt​  The prompt in RetrievalQA chain can be customized as follows.  import { RetrievalQAChain } from \"langchain/chains\"; import { ChatOpenAI } from \"@langchain/openai\"; import { PromptTemplate } from \"langchain/prompts\";  const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });  const template = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Use three sentences maximum and keep the answer as concise as possible. Always say \"thanks for asking!\" at the end of the answer. {context} Question: {question} Helpful Answer:`;  const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {   prompt: PromptTemplate.fromTemplate(template), });  const response = await chain.call({   query: \"What is task decomposition?\", });  console.log(response);  /*   {     text: 'Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals. This allows for efficient handling of complex tasks and aids in planning and organizing the steps needed to achieve the overall goal. Thanks for asking!'   } */  3.2.3 Returning source documents​  The full set of retrieved documents used for answer distillation can be returned using return_source_documents=True.  import { RetrievalQAChain } from \"langchain/chains\"; import { ChatOpenAI } from \"@langchain/openai\";  const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" });  const chain = RetrievalQAChain.fromLLM(model, vectorStore.asRetriever(), {   returnSourceDocuments: true, });  const response = await chain.call({   query: \"What is task decomposition?\", });  console.log(response.sourceDocuments[0]);  /* Document {   pageContent: 'Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.',   metadata: [Object] } */  3.2.4 Customizing retrieved docs in the LLM prompt​  Retrieved documents can be fed to an LLM for answer distillation in a few different ways.  stuff, refine, and map-reduce chains for passing documents to an LLM prompt are well summarized here.  stuff is commonly used because it simply \"stuffs\" all retrieved documents into the prompt.  The loadQAChain methods are easy ways to pass documents to an LLM using these various approaches.  import { loadQAStuffChain } from \"langchain/chains\";  const stuffChain = loadQAStuffChain(model);  const stuffResult = await stuffChain.call({   input_documents: relevantDocs,   question: \"What is task decomposition?\", });  console.log(stuffResult); /* {   text: 'Task decomposition is the process of breaking down a large task into smaller, more manageable subgoals or steps. This allows for efficient handling of complex tasks by focusing on one subgoal at a time. Task decomposition can be done through various methods such as using simple prompting, task-specific instructions, or human inputs.' } */  4. Chat​ 4.1 Getting started​  To keep chat history, we use a variant of the previous chain called a ConversationalRetrievalQAChain. First, specify a Memory buffer to track the conversation inputs / outputs.  import { ConversationalRetrievalQAChain } from \"langchain/chains\"; import { BufferMemory } from \"langchain/memory\"; import { ChatOpenAI } from \"@langchain/openai\";  const memory = new BufferMemory({   memoryKey: \"chat_history\",   returnMessages: true, });   Next, we initialize and call the chain:  const model = new ChatOpenAI({ modelName: \"gpt-3.5-turbo\" }); const chain = ConversationalRetrievalQAChain.fromLLM(   model,   vectorStore.asRetriever(),   {     memory,   } );  const result = await chain.call({   question: \"What are some of the main ideas in self-reflection?\", }); console.log(result);  /* {   text: 'Some main ideas in self-reflection include:\\n' +     '\\n' +     '1. Iterative Improvement: Self-reflection allows autonomous agents to improve by continuously refining past action decisions and correcting mistakes.\\n' +     '\\n' +     '2. Trial and Error: Self-reflection plays a crucial role in real-world tasks where trial and error are inevitable. It helps agents learn from failed trajectories and make adjustments for future actions.\\n' +     '\\n' +     '3. Constructive Criticism: Agents engage in constructive self-criticism of their big-picture behavior to identify areas for improvement.\\n' +     '\\n' +     '4. Decision and Strategy Refinement: Reflection on past decisions and strategies enables agents to refine their approach and make more informed choices.\\n' +     '\\n' +     '5. Efficiency and Optimization: Self-reflection encourages agents to be smart and efficient in their actions, aiming to complete tasks in the least number of steps.\\n' +     '\\n' +     'These ideas highlight the importance of self-reflection in enhancing performance and guiding future actions.' } */   The Memory buffer has context to resolve \"it\" (\"self-reflection\") in the below question.  const followupResult = await chain.call({   question: \"How does the Reflexion paper handle it?\", }); console.log(followupResult);  /* {   text: \"The Reflexion paper introduces a framework that equips agents with dynamic memory and self-reflection capabilities to improve their reasoning skills. The approach involves showing the agent two-shot examples, where each example consists of a failed trajectory and an ideal reflection on how to guide future changes in the agent's plan. These reflections are then added to the agent's working memory as context for querying a language model. The agent uses this self-reflection information to make decisions on whether to start a new trial or continue with the current plan.\" } */  4.2 Going deeper​  The documentation on ConversationalRetrievalQAChain offers a few extensions, such as streaming and source documents.  Previous Use cases Next Advanced Conversational QA Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/",
    "title": "Use cases | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use cases Use cases  Walkthroughs of common end-to-end use cases  🗃️ QA and Chat over Documents  3 items  🗃️ Retrieval-augmented generation (RAG)  1 items  📄️ Tabular Question Answering  Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables.  📄️ Interacting with APIs  Lots of data and information is stored behind APIs.  📄️ Summarization  A common use case is wanting to summarize long documents.  🗃️ Agent Simulations  2 items  🗃️ Autonomous Agents  3 items  📄️ Chatbots  Language models are good at producing text, which makes them ideal for creating chatbots.  📄️ Extraction  Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:  Next QA and Chat over Documents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/interface",
    "title": "Interface | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageInterface Interface  In an effort to make it as easy as possible to create custom chains, we've implemented a \"Runnable\" protocol that most components implement. This is a standard interface with a few different methods, which make it easy to define custom chains as well as making it possible to invoke them in a standard way. The standard interface exposed includes:  stream: stream back chunks of the response invoke: call the chain on an input batch: call the chain on a list of inputs streamLog: stream back intermediate steps as they happen, in addition to the final response  The input type varies by component :  Component Input Type Prompt Object Retriever Single string LLM, ChatModel Single string, list of chat messages or PromptValue Tool Single string, or object, depending on the tool OutputParser The output of an LLM or ChatModel  The output type also varies by component :  Component Output Type LLM String ChatModel ChatMessage Prompt PromptValue Retriever List of documents Tool Depends on the tool OutputParser Depends on the parser  You can combine runnables (and runnable-like objects such as functions and objects whose values are all functions) into sequences in two ways:  Call the .pipe instance method, which takes another runnable-like as an argument Use the RunnableSequence.from([]) static method with an array of runnable-likes, which will run in sequence when invoked  See below for examples of how this looks.  Stream​ TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { ChatOpenAI } from \"@langchain/openai\"; import { PromptTemplate } from \"@langchain/core/prompts\";  const model = new ChatOpenAI({}); const promptTemplate = PromptTemplate.fromTemplate(   \"Tell me a joke about {topic}\" );  const chain = promptTemplate.pipe(model);  const stream = await chain.stream({ topic: \"bears\" });  // Each chunk has the same interface as a chat message for await (const chunk of stream) {   console.log(chunk?.content); }  /* Why don't bears wear shoes?  Because they have bear feet! */  API Reference: ChatOpenAI from @langchain/openai PromptTemplate from @langchain/core/prompts Invoke​ import { ChatOpenAI } from \"@langchain/openai\"; import { PromptTemplate } from \"@langchain/core/prompts\"; import { RunnableSequence } from \"@langchain/core/runnables\";  const model = new ChatOpenAI({}); const promptTemplate = PromptTemplate.fromTemplate(   \"Tell me a joke about {topic}\" );  // You can also create a chain using an array of runnables const chain = RunnableSequence.from([promptTemplate, model]);  const result = await chain.invoke({ topic: \"bears\" });  console.log(result); /*   AIMessage {     content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",   } */  API Reference: ChatOpenAI from @langchain/openai PromptTemplate from @langchain/core/prompts RunnableSequence from @langchain/core/runnables Batch​ import { ChatOpenAI } from \"@langchain/openai\"; import { PromptTemplate } from \"@langchain/core/prompts\";  const model = new ChatOpenAI({}); const promptTemplate = PromptTemplate.fromTemplate(   \"Tell me a joke about {topic}\" );  const chain = promptTemplate.pipe(model);  const result = await chain.batch([{ topic: \"bears\" }, { topic: \"cats\" }]);  console.log(result); /*   [     AIMessage {       content: \"Why don't bears wear shoes?\\n\\nBecause they have bear feet!\",     },     AIMessage {       content: \"Why don't cats play poker in the wild?\\n\\nToo many cheetahs!\"     }   ] */  API Reference: ChatOpenAI from @langchain/openai PromptTemplate from @langchain/core/prompts  You can also pass a batchOptions argument to the call. There are options to set maximum concurrency and whether or not to return exceptions instead of throwing them (useful for gracefully handling failures!):  import { ChatOpenAI } from \"@langchain/openai\"; import { PromptTemplate } from \"@langchain/core/prompts\";  const model = new ChatOpenAI({   modelName: \"badmodel\", }); const promptTemplate = PromptTemplate.fromTemplate(   \"Tell me a joke about {topic}\" );  const chain = promptTemplate.pipe(model);  const result = await chain.batch(   [{ topic: \"bears\" }, { topic: \"cats\" }],   {},   { returnExceptions: true, maxConcurrency: 1 } );  console.log(result); /*   [     NotFoundError: The model `badmodel` does not exist       at Function.generate (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6)       at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)       at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)       at process.processTicksAndRejections (node:internal/process/task_queues:95:5)       at async file:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29       at RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {     status: 404,     NotFoundError: The model `badmodel` does not exist         at Function.generate (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/error.ts:71:6)         at OpenAI.makeStatusError (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:381:13)         at OpenAI.makeRequest (/Users/jacoblee/langchain/langchainjs/node_modules/openai/src/core.ts:442:15)         at process.processTicksAndRejections (node:internal/process/task_queues:95:5)         at async file:///Users/jacoblee/langchain/langchainjs/langchain/dist/chat_models/openai.js:514:29         at RetryOperation._fn (/Users/jacoblee/langchain/langchainjs/node_modules/p-retry/index.js:50:12) {       status: 404,   ] */  API Reference: ChatOpenAI from @langchain/openai PromptTemplate from @langchain/core/prompts Stream log​  All runnables also have a method called .streamLog() which is used to stream all or part of the intermediate steps of your chain/sequence as they happen.  This is useful to show progress to the user, to use intermediate results, or to debug your chain. You can stream all steps (default) or include/exclude steps by name, tags or metadata.  This method yields JSONPatch ops that when applied in the same order as received build up the RunState.  Here's an example with streaming intermediate documents from a retrieval chain:  import { HNSWLib } from \"@langchain/community/vectorstores/hnswlib\"; import { ChatOpenAI, OpenAIEmbeddings } from \"@langchain/openai\"; import { formatDocumentsAsString } from \"langchain/util/document\"; import { StringOutputParser } from \"@langchain/core/output_parsers\"; import {   RunnablePassthrough,   RunnableSequence, } from \"@langchain/core/runnables\"; import {   ChatPromptTemplate,   HumanMessagePromptTemplate,   SystemMessagePromptTemplate, } from \"@langchain/core/prompts\";  // Initialize the LLM to use to answer the question. const model = new ChatOpenAI({});  const vectorStore = await HNSWLib.fromTexts(   [     \"mitochondria is the powerhouse of the cell\",     \"mitochondria is made of lipids\",   ],   [{ id: 1 }, { id: 2 }],   new OpenAIEmbeddings() );  // Initialize a retriever wrapper around the vector store const vectorStoreRetriever = vectorStore.asRetriever();  // Create a system & human prompt for the chat model const SYSTEM_TEMPLATE = `Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. ---------------- {context}`; const messages = [   SystemMessagePromptTemplate.fromTemplate(SYSTEM_TEMPLATE),   HumanMessagePromptTemplate.fromTemplate(\"{question}\"), ]; const prompt = ChatPromptTemplate.fromMessages(messages);  const chain = RunnableSequence.from([   {     context: vectorStoreRetriever.pipe(formatDocumentsAsString),     question: new RunnablePassthrough(),   },   prompt,   model,   new StringOutputParser(), ]);  const stream = await chain.streamLog(\"What is the powerhouse of the cell?\");  for await (const chunk of stream) {   console.log(JSON.stringify(chunk));   console.log(); }  /*   {\"ops\":[{\"op\":\"replace\",\"path\":\"\",\"value\":{\"id\":\"5a79d2e7-171a-4034-9faa-63af88e5a451\",\"streamed_output\":[],\"logs\":{}}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableMap\",\"value\":{\"id\":\"5948dd9f-b827-45f8-9fa6-74e5cc972a56\",\"name\":\"RunnableMap\",\"type\":\"chain\",\"tags\":[\"seq:step:1\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:46.664Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableSequence\",\"value\":{\"id\":\"e9e9ef5e-3a04-4110-9a24-517c929b9137\",\"name\":\"RunnableSequence\",\"type\":\"chain\",\"tags\":[\"context\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:46.804Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnablePassthrough\",\"value\":{\"id\":\"4c79d835-87e5-4ff8-b560-987aea83c0e4\",\"name\":\"RunnablePassthrough\",\"type\":\"chain\",\"tags\":[\"question\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:46.805Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnablePassthrough/final_output\",\"value\":{\"output\":\"What is the powerhouse of the cell?\"}},{\"op\":\"add\",\"path\":\"/logs/RunnablePassthrough/end_time\",\"value\":\"2023-12-23T00:20:46.947Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/VectorStoreRetriever\",\"value\":{\"id\":\"1e169f18-711e-47a3-910e-ee031f70b6e0\",\"name\":\"VectorStoreRetriever\",\"type\":\"retriever\",\"tags\":[\"seq:step:1\",\"hnswlib\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:47.082Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/VectorStoreRetriever/final_output\",\"value\":{\"documents\":[{\"pageContent\":\"mitochondria is the powerhouse of the cell\",\"metadata\":{\"id\":1}},{\"pageContent\":\"mitochondria is made of lipids\",\"metadata\":{\"id\":2}}]}},{\"op\":\"add\",\"path\":\"/logs/VectorStoreRetriever/end_time\",\"value\":\"2023-12-23T00:20:47.398Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableLambda\",\"value\":{\"id\":\"a0d61a88-8282-42be-8949-fb0e8f8f67cd\",\"name\":\"RunnableLambda\",\"type\":\"chain\",\"tags\":[\"seq:step:2\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:47.495Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableLambda/final_output\",\"value\":{\"output\":\"mitochondria is the powerhouse of the cell\\n\\nmitochondria is made of lipids\"}},{\"op\":\"add\",\"path\":\"/logs/RunnableLambda/end_time\",\"value\":\"2023-12-23T00:20:47.604Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableSequence/final_output\",\"value\":{\"output\":\"mitochondria is the powerhouse of the cell\\n\\nmitochondria is made of lipids\"}},{\"op\":\"add\",\"path\":\"/logs/RunnableSequence/end_time\",\"value\":\"2023-12-23T00:20:47.690Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/RunnableMap/final_output\",\"value\":{\"question\":\"What is the powerhouse of the cell?\",\"context\":\"mitochondria is the powerhouse of the cell\\n\\nmitochondria is made of lipids\"}},{\"op\":\"add\",\"path\":\"/logs/RunnableMap/end_time\",\"value\":\"2023-12-23T00:20:47.780Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatPromptTemplate\",\"value\":{\"id\":\"5b6cff77-0c52-4218-9bde-d92c33ad12f3\",\"name\":\"ChatPromptTemplate\",\"type\":\"prompt\",\"tags\":[\"seq:step:2\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:47.864Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatPromptTemplate/final_output\",\"value\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"prompt_values\",\"ChatPromptValue\"],\"kwargs\":{\"messages\":[{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"messages\",\"SystemMessage\"],\"kwargs\":{\"content\":\"Use the following pieces of context to answer the question at the end.\\nIf you don't know the answer, just say that you don't know, don't try to make up an answer.\\n----------------\\nmitochondria is the powerhouse of the cell\\n\\nmitochondria is made of lipids\",\"additional_kwargs\":{}}},{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"messages\",\"HumanMessage\"],\"kwargs\":{\"content\":\"What is the powerhouse of the cell?\",\"additional_kwargs\":{}}}]}}},{\"op\":\"add\",\"path\":\"/logs/ChatPromptTemplate/end_time\",\"value\":\"2023-12-23T00:20:47.956Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI\",\"value\":{\"id\":\"0cc3b220-ca7f-4fd3-88d5-bea1f7417c3d\",\"name\":\"ChatOpenAI\",\"type\":\"llm\",\"tags\":[\"seq:step:3\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:48.126Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/StrOutputParser\",\"value\":{\"id\":\"47d9bd52-c14a-420d-8d52-1106d751581c\",\"name\":\"StrOutputParser\",\"type\":\"parser\",\"tags\":[\"seq:step:4\"],\"metadata\":{},\"start_time\":\"2023-12-23T00:20:48.666Z\",\"streamed_output_str\":[]}}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\"\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\"\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\"The\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\"The\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" mitochond\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" mitochond\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\"ria\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\"ria\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" is\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" is\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" the\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" the\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" powerhouse\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" powerhouse\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" of\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" of\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" the\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" the\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\" cell\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\" cell\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\".\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\".\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/streamed_output_str/-\",\"value\":\"\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/streamed_output/-\",\"value\":\"\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/final_output\",\"value\":{\"generations\":[[{\"text\":\"The mitochondria is the powerhouse of the cell.\",\"generationInfo\":{\"prompt\":0,\"completion\":0},\"message\":{\"lc\":1,\"type\":\"constructor\",\"id\":[\"langchain_core\",\"messages\",\"AIMessageChunk\"],\"kwargs\":{\"content\":\"The mitochondria is the powerhouse of the cell.\",\"additional_kwargs\":{}}}}]]}},{\"op\":\"add\",\"path\":\"/logs/ChatOpenAI/end_time\",\"value\":\"2023-12-23T00:20:48.841Z\"}]}    {\"ops\":[{\"op\":\"add\",\"path\":\"/logs/StrOutputParser/final_output\",\"value\":{\"output\":\"The mitochondria is the powerhouse of the cell.\"}},{\"op\":\"add\",\"path\":\"/logs/StrOutputParser/end_time\",\"value\":\"2023-12-23T00:20:48.945Z\"}]}    {\"ops\":[{\"op\":\"replace\",\"path\":\"/final_output\",\"value\":{\"output\":\"The mitochondria is the powerhouse of the cell.\"}}]} */  API Reference: HNSWLib from @langchain/community/vectorstores/hnswlib ChatOpenAI from @langchain/openai OpenAIEmbeddings from @langchain/openai formatDocumentsAsString from langchain/util/document StringOutputParser from @langchain/core/output_parsers RunnablePassthrough from @langchain/core/runnables RunnableSequence from @langchain/core/runnables ChatPromptTemplate from @langchain/core/prompts HumanMessagePromptTemplate from @langchain/core/prompts SystemMessagePromptTemplate from @langchain/core/prompts Previous Get started Next Route between multiple runnables Stream Invoke Batch Stream log Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/security",
    "title": "Security | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Security Security  LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources.  Best Practices​  When building such applications developers should remember to follow good security practices:  Limit Permissions: Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), etc. as appropriate for your application. Anticipate Potential Misuse: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it’s safest to assume that any LLM able to use those credentials may in fact delete data. Defense in Depth: No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It’s best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use.  Risks of not doing so include, but are not limited to:  Data corruption or loss. Unauthorized access to confidential information. Compromised performance or availability of critical resources.  Example scenarios with mitigation strategies:  A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials.  If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications.  Reporting a Vulnerability​  Please report security vulnerabilities by email to security@langchain.dev. This will ensure the issue is promptly triaged and acted upon as needed.  Enterprise solutions​  LangChain offers enterprise solutions for customers who have additional security requirements. Please contact us at sales@langchain.dev.  Previous Experimental Next Guides Best Practices Reporting a Vulnerability Enterprise solutions Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/additional_resources/tutorials",
    "title": "Tutorials | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Tutorials  Below are links to tutorials and courses on LangChain.js. For written guides on common use cases for LangChain.js, check out the use cases and guides sections.  Deeplearning.ai​  We've partnered with Deeplearning.ai and Andrew Ng on a LangChain.js short course.  It covers LCEL and other building blocks you can combine to build more complex chains, as well as fundamentals around loading data for retrieval augmented generation (RAG). Try it for free below:  Build LLM Apps with LangChain.js Scrimba interactive guides​  Scrimba is a code-learning platform that allows you to interactively edit and run code while watching a video walkthrough.  We've partnered with Scrimba on course materials (called \"scrims\") that teach the fundamentals of building with LangChain.js - check them out below, and check back for more as they become available!  Learn LangChain.js​ Learn LangChain.js on Scrimba  An full end-to-end course that walks through how to build a chatbot that can answer questions about a provided document. A great introduction to LangChain and a great first project for learning how to use LangChain Expression Language primitives to perform retrieval!  LangChain Expression Language (LCEL)​ The basics (PromptTemplate + LLM) Adding an output parser Attaching function calls to a model Composing multiple chains Retrieval chains Conversational retrieval chains (\"Chat with Docs\") Deeper dives​ Setting up a new PromptTemplate Setting up ChatOpenAI parameters Attaching stop sequences LangChain Expression Language Cheatsheet​  For a quick reference for LangChain Expression Language, check out this overview/cheatsheet made by @zhanghaili0610:  Deeplearning.ai Scrimba interactive guides Learn LangChain.js LangChain Expression Language (LCEL) Deeper dives LangChain Expression Language Cheatsheet Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/contributing",
    "title": "Developer Guide | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Developer Guide Contributing to LangChain  👋 Hi there! Thank you for being interested in contributing to LangChain. As an open source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infra, or better documentation.  To contribute to this project, please follow a \"fork and pull request\" workflow. Please do not try to push directly to this repo unless you are a maintainer.  Quick Links​ Not sure what to work on?​  If you are not sure what to work on, we have a few suggestions:  Look at the issues with the help wanted label. These are issues that we think are good targets for contributors. If you are interested in working on one of these, please comment on the issue so that we can assign it to you. And if you have any questions let us know, we're happy to guide you! At the moment our main focus is reaching parity with the Python version for features and base functionality. If you are interested in working on a specific integration or feature, please let us know and we can help you get started. New abstractions​  We aim to keep the same core APIs between the Python and JS versions of LangChain, where possible. As such we ask that if you have an idea for a new abstraction, please open an issue first to discuss it. This will help us make sure that the API is consistent across both versions. If you're not sure what to work on, we recommend looking at the links above first.  Want to add a specific integration?​  LangChain supports several different types of integrations with third-party providers and frameworks, including LLM providers (e.g. OpenAI), vector stores (e.g. FAISS), document loaders (e.g. Apify) persistent message history stores (e.g. Redis), and more.  We welcome such contributions, but ask that you read our dedicated integration contribution guide for specific details and patterns to consider before opening a pull request.  These should generally reside in the libs/langchain-community workspace and be imported as @langchain/community/module/name, but more in-depth integrations or suites of integrations may also reside in separate packages that depend on and extend @langchain/core. See @langchain/google-genai for an example.  To make creating packages like this easier, we offer the create-langchain-integration utility that will automatically scaffold a repo with support for both ESM + CJS entrypoints. You can run it like this:  $ npx create-langchain-integration  Want to add a feature that's already in Python?​  If you're interested in contributing a feature that's already in the LangChain Python repo and you'd like some help getting started, you can try pasting code snippets and classes into the LangChain Python to JS translator.  It's a chat interface wrapping a fine-tuned gpt-3.5-turbo instance trained on prior ported features. This allows the model to innately take into account LangChain-specific code style and imports.  It's an ongoing project, and feedback on runs will be used to improve the LangSmith dataset for further fine-tuning! Try it out below:  https://langchain-translator.vercel.app/  🗺️ Contributing Guidelines​ 🚩 GitHub Issues​  Our issues page contains with bugs, improvements, and feature requests.  If you start working on an issue, please assign it to yourself.  If you are adding an issue, please try to keep it focused on a single modular bug/improvement/feature. If the two issues are related, or blocking, please link them rather than keep them as one single one.  We will try to keep these issues as up to date as possible, though with the rapid rate of development in this field some may get out of date. If you notice this happening, please just let us know.  🙋 Getting Help​  Although we try to have a developer setup to make it as easy as possible for others to contribute (see below) it is possible that some pain point may arise around environment setup, linting, documentation, or other. Should that occur, please contact a maintainer! Not only do we want to help get you unblocked, but we also want to make sure that the process is smooth for future contributors.  In a similar vein, we do enforce certain linting, formatting, and documentation standards in the codebase. If you are finding these difficult (or even just annoying) to work with, feel free to contact a maintainer for help - we do not want these to get in the way of getting good code into the codebase.  🏭 Release process​  As of now, LangChain has an ad hoc release process: releases are cut with high frequency via by a developer and published to npm.  LangChain follows the semver versioning standard. However, as pre-1.0 software, even patch releases may contain non-backwards-compatible changes.  If your contribution has made its way into a release, we will want to give you credit on Twitter (only if you want though)! If you have a Twitter account you would like us to mention, please let us know in the PR or in another manner.  Integration releases​  You can invoke the release flow by calling yarn release from the package root.  There are three parameters which can be passed to this script, one required and two optional.  Required: --workspace <workspace name>. eg: --workspace @langchain/core (always appended as the first flag when running yarn release) Optional: --bump-deps eg --bump-deps Will find all packages in the repo which depend on this workspace and checkout a new branch, update the dep version, run yarn install, commit & push to new branch. Optional: --tag <tag> eg --tag beta Add a tag to the NPM release.  This script automatically bumps the package version, creates a new release branch with the changes, pushes the branch to GitHub, uses release-it to automatically release to NPM, and more depending on the flags passed.  Halfway through this script, you'll be prompted to enter an NPM OTP (typically from an authenticator app). This value is not stored anywhere and is only used to authenticate the NPM release.  Full example: yarn release @langchain/core --bump-deps --tag beta.  🛠️ Tooling​  This project uses the following tools, which are worth getting familiar with if you plan to contribute:  yarn (v3.4.1) - dependency management eslint - enforcing standard lint rules prettier - enforcing standard code formatting jest - testing code TypeDoc - reference doc generation from comments Docusaurus - static site generation for documentation 🚀 Quick Start​  Clone this repo, then cd into it:  cd langchainjs   Next, try running the following common tasks:  ✅ Common Tasks​  Our goal is to make it as easy as possible for you to contribute to this project. All of the below commands should be run from within a workspace directory (e.g. langchain, libs/langchain-community) unless otherwise noted.  cd langchain   Or, if you are working on a community integration:  cd libs/langchain-community  Setup​  To get started, you will need to install the dependencies for the project. To do so, run:  yarn   Then, you will need to switch directories into langchain-core and build core by running:  cd ../langchain-core yarn yarn build  Linting​  We use eslint to enforce standard lint rules. To run the linter, run:  yarn lint  Formatting​  We use prettier to enforce code formatting style. To run the formatter, run:  yarn format   To just check for formatting differences, without fixing them, run:  yarn format:check  Testing​  In general, tests should be added within a tests/ folder alongside the modules they are testing.  Unit tests cover modular logic that does not require calls to outside APIs.  If you add new logic, please add a unit test. Unit tests should be called *.test.ts.  To run only unit tests, run:  yarn test  Running a single test​  To run a single test, run the following from within a workspace:  yarn test:single /path/to/yourtest.test.ts   This is useful for developing individual features.  Integration tests cover logic that requires making calls to outside APIs (often integration with other services).  If you add support for a new external API, please add a new integration test. Integration tests should be called *.int.test.ts.  Note that most integration tests require credentials or other setup. You will likely need to set up a langchain/.env or libs/langchain-community/.env file like the example here.  We generally recommend only running integration tests with yarn test:single, but if you want to run all integration tests, run:  yarn test:integration  Building​  To build the project, run:  yarn build  Adding an Entrypoint​  LangChain exposes multiple subpaths the user can import from, e.g.  import { OpenAI } from \"langchain/llms/openai\";   We call these subpaths \"entrypoints\". In general, you should create a new entrypoint if you are adding a new integration with a 3rd party library. If you're adding self-contained functionality without any external dependencies, you can add it to an existing entrypoint.  In order to declare a new entrypoint that users can import from, you should edit the langchain/scripts/create-entrypoints.js or libs/langchain-community/scripts/create-entrypoints.js script. To add an entrypoint tools that imports from tools/index.ts you'd add the following to the entrypoints variable:  const entrypoints = {   // ...   tools: \"tools/index\", };   If you're adding a new integration which requires installing a third party depencency, you must add the entrypoint to the requiresOptionalDependency array, also located inside langchain/scripts/create-entrypoints.js or libs/langchain-community/scripts/create-entrypoints.js.  const requiresOptionalDependency = [   // ...   \"tools/index\", ];   This will make sure the entrypoint is included in the published package, and in generated documentation.  Documentation​ Contribute Documentation​  Docs are largely autogenerated by TypeDoc from the code.  For that reason, we ask that you add good documentation to all classes and methods.  Similar to linting, we recognize documentation can be annoying. If you do not want to do it, please contact a project maintainer, and they can help you with it. We do not want this to be a blocker for good code getting contributed.  Documentation and the skeleton lives under the docs/ folder. Example code is imported from under the examples/ folder.  Running examples​  If you add a new major piece of functionality, it is helpful to add an example to showcase how to use it. Most of our users find examples to be the most helpful kind of documentation.  Examples can be added in the examples/src directory, e.g. examples/src/path/to/example. This example can then be invoked with yarn example path/to/example at the top level of the repo.  To run examples that require an environment variable, you'll need to add a .env file under examples/.env.  Build Documentation Locally​  To generate and view the documentation locally, change to the project root and run yarn to ensure dependencies get installed in both the docs/ and examples/ workspaces:  cd .. yarn   Then run:  yarn docs  Advanced​  Environment tests test whether LangChain works across different JS environments, including Node.js (both ESM and CJS), Edge environments (eg. Cloudflare Workers), and browsers (using Webpack).  To run the environment tests with Docker, run the following command from the project root:  yarn test:exports:docker  Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/",
    "title": "Providers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores Providers Providers  LangChain.js integration providers.  📄️ Anthropic  All functionality related to Anthropic models.  📄️ AWS  All functionality related to Amazon AWS platform  📄️ Google  Functionality related to Google Cloud Platform  📄️ Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  📄️ OpenAI  All functionality related to OpenAI  Next Anthropic Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/expression_language/cookbook",
    "title": "Cookbook | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Prompt + LLM Multiple chains Retrieval augmented generation (RAG) Querying a SQL DB Adding memory Using tools Agents Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem LangChain Expression LanguageCookbook Cookbook  Example code for accomplishing common tasks with the LangChain Expression Language (LCEL). These examples show how to compose different Runnable (the core LCEL interface) components to achieve various tasks. If you're just getting acquainted with LCEL, the Prompt + LLM page is a good place to start.  Several pages in this section include embedded interactive screencasts from Scrimba. They're a great resource for getting started - you can edit the included code whenever you want, just as if you were pair programming with a teacher!  📄️ Prompt + LLM  One of the most foundational Expression Language compositions is taking:  📄️ Multiple chains  Runnables can be used to combine multiple Chains together:  📄️ Retrieval augmented generation (RAG)  Let's now look at adding in a retrieval step to a prompt and an LLM, which adds up to a \"retrieval-augmented generation\" chain:  📄️ Querying a SQL DB  We can replicate our SQLDatabaseChain with Runnables.  📄️ Adding memory  This shows how to add memory to an arbitrary chain. Right now, you can use the memory classes but need to hook them up manually.  📄️ Using tools  Tools are also runnables, and can therefore be used within a chain:  📄️ Agents  You can pass a Runnable into an agent.  Previous Add message history (memory) Next Prompt + LLM Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/get_started",
    "title": "Get started | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Get started Get started  Get started with LangChain  📄️ Introduction  LangChain is a framework for developing applications powered by language models. It enables applications that:  📄️ Installation  Supported Environments  📄️ Quickstart  In this quickstart we'll show you how to:  Next Introduction Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/community",
    "title": "Community navigator | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Community navigator  Hi! Thanks for being here. We’re lucky to have a community of so many passionate developers building with LangChain–we have so much to teach and learn from each other. Community members contribute code, host meetups, write blog posts, amplify each other’s work, become each other's customers and collaborators, and so much more.  Whether you’re new to LangChain, looking to go deeper, or just want to get more exposure to the world of building with LLMs, this page can point you in the right direction.  🦜 Contribute to LangChain  🌍 Meetups, Events, and Hackathons  📣 Help Us Amplify Your Work  💬 Stay in the loop  🦜 Contribute to LangChain  LangChain is the product of over 5,000+ contributions by 1,500+ contributors, and there is still so much to do together. Here are some ways to get involved:  Open a pull request: we’d appreciate all forms of contributions–new features, infrastructure improvements, better documentation, bug fixes, etc. If you have an improvement or an idea, we’d love to work on it with you. Read our contributor guidelines: We ask contributors to follow a \"fork and pull request\" workflow, run a few local checks for formatting, linting, and testing before submitting, and follow certain documentation and testing conventions. Become an expert: our experts help the community by answering product questions in Discord. If that’s a role you’d like to play, we’d be so grateful! (And we have some special experts-only goodies/perks we can tell you more about). Send us an email to introduce yourself at hello@langchain.dev and we’ll take it from there! Integrate with LangChain: if your product integrates with LangChain–or aspires to–we want to help make sure the experience is as smooth as possible for you and end users. Send us an email at hello@langchain.dev and tell us what you’re working on. Become an Integration Maintainer: Partner with our team to ensure your integration stays up-to-date and talk directly with users (and answer their inquiries) in our Discord. Introduce yourself at hello@langchain.dev if you’d like to explore this role. 🌍 Meetups, Events, and Hackathons  One of our favorite things about working in AI is how much enthusiasm there is for building together. We want to help make that as easy and impactful for you as possible!  Find a meetup, hackathon, or webinar: you can find the one for you on on our global events calendar. Submit an event to our calendar: email us at events@langchain.dev with a link to your event page! We can also help you spread the word with our local communities. Host a meetup: If you want to bring a group of builders together, we want to help! We can publicize your event on our event calendar/Twitter, share with our local communities in Discord, send swag, or potentially hook you up with a sponsor. Email us at events@langchain.dev to tell us about your event! Become a meetup sponsor: we often hear from groups of builders that want to get together, but are blocked or limited on some dimension (space to host, budget for snacks, prizes to distribute, etc.). If you’d like to help, send us an email to events@langchain.dev we can share more about how it works! Speak at an event: meetup hosts are always looking for great speakers, presenters, and panelists. If you’d like to do that at an event, send us an email to hello@langchain.dev with more information about yourself, what you want to talk about, and what city you’re based in and we’ll try to match you with an upcoming event! Tell us about your LLM community: If you host or participate in a community that would welcome support from LangChain and/or our team, send us an email at hello@langchain.dev and let us know how we can help. 📣 Help Us Amplify Your Work  If you’re working on something you’re proud of, and think the LangChain community would benefit from knowing about it, we want to help you show it off.  Post about your work and mention us: we love hanging out on Twitter to see what people in the space are talking about and working on. If you tag @langchainai, we’ll almost certainly see it and can show you some love. Publish something on our blog: if you’re writing about your experience building with LangChain, we’d love to post (or crosspost) it on our blog! E-mail hello@langchain.dev with a draft of your post! Or even an idea for something you want to write about. Get your product onto our integrations hub: Many developers take advantage of our seamless integrations with other products, and come to our integrations hub to find out who those are. If you want to get your product up there, tell us about it (and how it works with LangChain) at hello@langchain.dev. ☀️ Stay in the loop  Here’s where our team hangs out, talks shop, spotlights cool work, and shares what we’re up to. We’d love to see you there too.  Twitter: we post about what we’re working on and what cool things we’re seeing in the space. If you tag @langchainai in your post, we’ll almost certainly see it, and can snow you some love! Discord: connect with with >30k developers who are building with LangChain GitHub: open pull requests, contribute to a discussion, and/or contribute Subscribe to our bi-weekly Release Notes: a twice/month email roundup of the coolest things going on in our orbit Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms",
    "title": "Providers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores Providers Providers  LangChain.js integration providers.  📄️ Anthropic  All functionality related to Anthropic models.  📄️ AWS  All functionality related to Amazon AWS platform  📄️ Google  Functionality related to Google Cloud Platform  📄️ Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  📄️ OpenAI  All functionality related to OpenAI  Next Anthropic Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/get_started/installation",
    "title": "Installation | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Get startedInstallation Installation Supported Environments​  LangChain is written in TypeScript and can be used in:  Node.js (ESM and CommonJS) - 18.x, 19.x, 20.x Cloudflare Workers Vercel / Next.js (Browser, Serverless and Edge functions) Supabase Edge Functions Browser Deno Bun  However, note that individual integrations may not be supported in all environments.  Installation​  To get started, install LangChain with the following command:  npm Yarn pnpm npm install -S langchain  TypeScript​  LangChain is written in TypeScript and provides type definitions for all of its public APIs.  Installing integration packages​  LangChain supports packages that contain specific module integrations with third-party providers. They can be as specific as @langchain/google-genai, which contains integrations just for Google AI Studio models, or as broad as @langchain/community, which contains broader variety of community contributed integrations.  These packages, as well as the main LangChain package, all depend on @langchain/core, which contains the base abstractions that these integration packages extend.  To ensure that all integrations and their types interact with each other properly, it is important that they all use the same version of @langchain/core. The best way to guarantee this is to add a \"resolutions\" or \"overrides\" field like the following in your project's package.json. The name will depend on your package manager:  If you are using yarn:  yarn package.json {   \"name\": \"your-project\",   \"version\": \"0.0.0\",   \"private\": true,   \"engines\": {     \"node\": \">=18\"   },   \"dependencies\": {     \"@langchain/google-genai\": \"^0.0.2\",     \"langchain\": \"0.0.207\"   },   \"resolutions\": {     \"@langchain/core\": \"0.1.5\"   } }   Or for npm:  npm package.json {   \"name\": \"your-project\",   \"version\": \"0.0.0\",   \"private\": true,   \"engines\": {     \"node\": \">=18\"   },   \"dependencies\": {     \"@langchain/google-genai\": \"^0.0.2\",     \"langchain\": \"0.0.207\"   },   \"overrides\": {     \"@langchain/core\": \"0.1.5\"   } }   Or for pnpm:  pnpm package.json {   \"name\": \"your-project\",   \"version\": \"0.0.0\",   \"private\": true,   \"engines\": {     \"node\": \">=18\"   },   \"dependencies\": {     \"@langchain/google-genai\": \"^0.0.2\",     \"langchain\": \"0.0.207\"   },   \"pnpm\": {     \"overrides\": {       \"@langchain/core\": \"0.1.5\"     }   } }  @langchain/community​  The @langchain/community package contains third-party integrations. It is automatically installed along with langchain, but can also be used separately with just @langchain/core. Install with:  npm Yarn pnpm npm install @langchain/community  @langchain/core​  The @langchain/core package contains base abstractions that the rest of the LangChain ecosystem uses, along with the LangChain Expression Language. It is automatically installed along with langchain, but can also be used separately. Install with:  npm Yarn pnpm npm install @langchain/core  TIP  See this section for general instructions on installing integration packages.  Loading the library​ ESM​  LangChain provides an ESM build targeting Node.js environments. You can import it using the following syntax:  npm Yarn pnpm npm install @langchain/openai  import { OpenAI } from \"@langchain/openai\";   If you are using TypeScript in an ESM project we suggest updating your tsconfig.json to include the following:  tsconfig.json {   \"compilerOptions\": {     ...     \"target\": \"ES2020\", // or higher     \"module\": \"nodenext\",   } }  CommonJS​  LangChain provides a CommonJS build targeting Node.js environments. You can import it using the following syntax:  const { OpenAI } = require(\"@langchain/openai\");  Cloudflare Workers​  LangChain can be used in Cloudflare Workers. You can import it using the following syntax:  import { OpenAI } from \"@langchain/openai\";  Vercel / Next.js​  LangChain can be used in Vercel / Next.js. We support using LangChain in frontend components, in Serverless functions and in Edge functions. You can import it using the following syntax:  import { OpenAI } from \"@langchain/openai\";  Deno / Supabase Edge Functions​  LangChain can be used in Deno / Supabase Edge Functions. You can import it using the following syntax:  import { OpenAI } from \"https://esm.sh/@langchain/openai\";   or  import { OpenAI } from \"npm:@langchain/openai\";   We recommend looking at our Supabase Template for an example of how to use LangChain in Supabase Edge Functions.  Browser​  LangChain can be used in the browser. In our CI we test bundling LangChain with Webpack and Vite, but other bundlers should work too. You can import it using the following syntax:  import { OpenAI } from \"@langchain/openai\";  Unsupported: Node.js 16​  We do not support Node.js 16, but if you still want to run LangChain on Node.js 16, you will need to follow the instructions in this section. We do not guarantee that these instructions will continue to work in the future.  You will have to make fetch available globally, either:  run your application with NODE_OPTIONS='--experimental-fetch' node ..., or install node-fetch and follow the instructions here  You'll also need to polyfill ReadableStream by installing:  npm Yarn pnpm npm i web-streams-polyfill   And then adding it to the global namespace in your main entrypoint:  import \"web-streams-polyfill/es6\";   Additionally you'll have to polyfill structuredClone, eg. by installing core-js and following the instructions here.  If you are running Node.js 18+, you do not need to do anything.  Previous Introduction Next Quickstart Supported Environments Installation TypeScript Installing integration packages @langchain/community @langchain/core Loading the library ESM CommonJS Cloudflare Workers Vercel / Next.js Deno / Supabase Edge Functions Browser Unsupported: Node.js 16 Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/tabular",
    "title": "Tabular Question Answering | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesTabular Question Answering Tabular Question Answering  Lots of data and information is stored in tabular data, whether it be csvs, excel sheets, or SQL tables. This page covers all resources available in LangChain for working with data in this format.  Chains​  If you are just getting started, and you have relatively small/simple tabular data, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.  SQL Database Chain Agents​  Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger databases and more complex schemas.  SQL Agent Previous RAG over code Next Interacting with APIs Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/summarization",
    "title": "Summarization | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesSummarization Summarization  A common use case is wanting to summarize long documents. This naturally runs into the context window limitations. Unlike in question-answering, you can't just do some semantic search hacks to only select the chunks of text most relevant to the question (because, in this case, there is no particular question - you want to summarize everything). So what do you do then?  To get started, we would recommend checking out the summarization chain, which attacks this problem in a recursive manner.  Summarization Chain Example​  Here's an example of how you can use the RefineDocumentsChain to summarize documents loaded from a YouTube video:  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/anthropic  import { loadSummarizationChain } from \"langchain/chains\"; import { SearchApiLoader } from \"langchain/document_loaders/web/searchapi\"; import { TokenTextSplitter } from \"langchain/text_splitter\"; import { PromptTemplate } from \"@langchain/core/prompts\"; import { ChatAnthropic } from \"@langchain/anthropic\";  const loader = new SearchApiLoader({   engine: \"youtube_transcripts\",   video_id: \"WTOm65IZneg\", });  const docs = await loader.load();  const splitter = new TokenTextSplitter({   chunkSize: 10000,   chunkOverlap: 250, });  const docsSummary = await splitter.splitDocuments(docs);  const llmSummary = new ChatAnthropic({   modelName: \"claude-2.1\",   temperature: 0.3, });  const summaryTemplate = ` You are an expert in summarizing YouTube videos. Your goal is to create a summary of a podcast. Below you find the transcript of a podcast: -------- {text} --------  The transcript of the podcast will also be used as the basis for a question and answer bot. Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific.  Total output will be a summary of the video and a list of example questions the user could ask of the video.  SUMMARY AND QUESTIONS: `;  const SUMMARY_PROMPT = PromptTemplate.fromTemplate(summaryTemplate);  const summaryRefineTemplate = ` You are an expert in summarizing YouTube videos. Your goal is to create a summary of a podcast. We have provided an existing summary up to a certain point: {existing_answer}  Below you find the transcript of a podcast: -------- {text} --------  Given the new context, refine the summary and example questions. The transcript of the podcast will also be used as the basis for a question and answer bot. Provide some examples questions and answers that could be asked about the podcast. Make these questions very specific. If the context isn't useful, return the original summary and questions. Total output will be a summary of the video and a list of example questions the user could ask of the video.  SUMMARY AND QUESTIONS: `;  const SUMMARY_REFINE_PROMPT = PromptTemplate.fromTemplate(   summaryRefineTemplate );  const summarizeChain = loadSummarizationChain(llmSummary, {   type: \"refine\",   verbose: true,   questionPrompt: SUMMARY_PROMPT,   refinePrompt: SUMMARY_REFINE_PROMPT, });  const summary = await summarizeChain.run(docsSummary);  console.log(summary);  /*   Here is a summary of the key points from the podcast transcript:    - Jimmy helps provide hearing aids and cochlear implants to deaf and hard-of-hearing people who can't afford them. He helps over 1,000 people hear again.    - Jimmy surprises recipients with $10,000 cash gifts in addition to the hearing aids. He also gifts things like jet skis, basketball game tickets, and trips to concerts.    - Jimmy travels internationally to provide hearing aids, visiting places like Mexico, Guatemala, Brazil, South Africa, Malawi, and Indonesia.     - Jimmy donates $100,000 to organizations around the world that teach sign language.    - The recipients are very emotional and grateful to be able to hear their loved ones again.    Here are some example questions and answers about the podcast:    Q: How many people did Jimmy help regain their hearing?   A: Jimmy helped over 1,000 people regain their hearing.    Q: What types of hearing devices did Jimmy provide to the recipients?   A: Jimmy provided cutting-edge hearing aids and cochlear implants.    Q: In addition to the hearing devices, what surprise gifts did Jimmy give some recipients?   A: In addition to hearing devices, Jimmy surprised some recipients with $10,000 cash gifts, jet skis, basketball game tickets, and concert tickets.    Q: What countries did Jimmy travel to in order to help people?   A: Jimmy traveled to places like Mexico, Guatemala, Brazil, South Africa, Malawi, and Indonesia.    Q: How much money did Jimmy donate to organizations that teach sign language?   A: Jimmy donated $100,000 to sign language organizations around the world.    Q: How did the recipients react when they were able to hear again?   A: The recipients were very emotional and grateful, with many crying tears of joy at being able to hear their loved ones again. */  API Reference: loadSummarizationChain from langchain/chains SearchApiLoader from langchain/document_loaders/web/searchapi TokenTextSplitter from langchain/text_splitter PromptTemplate from @langchain/core/prompts ChatAnthropic from @langchain/anthropic Previous Interacting with APIs Next Agent Simulations Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/chatbots",
    "title": "Chatbots | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesChatbots Chatbots  Language models are good at producing text, which makes them ideal for creating chatbots. Aside from the base prompts/LLMs, an important concept to know for Chatbots is memory. Most chat based applications rely on remembering what happened in previous interactions, which memory is designed to help with.  You might find the following pages interesting:  Memory concepts and examples: Explanation of key concepts related to memory along with how-to's and examples. Conversation Agent: A notebook walking through how to create an agent optimized for conversation. Previous BabyAGI Next Extraction Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/extraction",
    "title": "Extraction | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesExtraction Extraction  Most APIs and databases still deal with structured information. Therefore, in order to better work with those, it can be useful to extract structured information from text. Examples of this include:  Extracting a structured row to insert into a database from a sentence Extracting multiple rows to insert into a database from a long document Extracting the correct API parameters from a user query  This work is extremely related to output parsing. Output parsers are responsible for instructing the LLM to respond in a specific format. In this case, the output parsers specify the format of the data you would like to extract from the document. Then, in addition to the output format instructions, the prompt should also contain the data you would like to extract information from.  While normal output parsers are good enough for basic structuring of response data, when doing extraction you often want to extract more complicated or nested structures.  With tool/function calling​  Tool/function calling is a powerful way to perform extraction. At a high level, function calling encourages the model to respond in a structured format. By specifying one or more JSON schemas that you want the LLM to use, you can guide the LLM to \"fill in the blanks\" and populate proper values for the keys to the JSON.  Here's a concrete example using OpenAI's tool calling features. Note that this requires either the gpt-3.5-turbo-1106 or gpt-4-1106-preview models.  We'll use Zod, a popular open source package, to format schema in OpenAI's tool format:  npm Yarn pnpm $ npm install zod zod-to-json-schema  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { z } from \"zod\"; import { zodToJsonSchema } from \"zod-to-json-schema\"; import { ChatOpenAI } from \"@langchain/openai\"; import { JsonOutputToolsParser } from \"langchain/output_parsers\"; import { ChatPromptTemplate } from \"@langchain/core/prompts\";  const EXTRACTION_TEMPLATE = `Extract and save the relevant entities mentioned \\ in the following passage together with their properties.  If a property is not present and is not required in the function parameters, do not include it in the output.`;  const prompt = ChatPromptTemplate.fromMessages([   [\"system\", EXTRACTION_TEMPLATE],   [\"human\", \"{input}\"], ]);  const person = z.object({   name: z.string().describe(\"The person's name\"),   age: z.string().describe(\"The person's age\"), });  const model = new ChatOpenAI({   modelName: \"gpt-3.5-turbo-1106\",   temperature: 0, }).bind({   tools: [     {       type: \"function\",       function: {         name: \"person\",         description: \"A person\",         parameters: zodToJsonSchema(person),       },     },   ], });  const parser = new JsonOutputToolsParser(); const chain = prompt.pipe(model).pipe(parser);  const res = await chain.invoke({   input: \"jane is 2 and bob is 3\", });  console.log(res); /*   [     { name: 'person', arguments: { name: 'jane', age: '2' } },     { name: 'person', arguments: { name: 'bob', age: '3' } }   ] */  API Reference: ChatOpenAI from @langchain/openai JsonOutputToolsParser from langchain/output_parsers ChatPromptTemplate from @langchain/core/prompts Previous Chatbots With tool/function calling Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/api",
    "title": "Interacting with APIs | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Autonomous Agents Chatbots Extraction Use casesInteracting with APIs Interacting with APIs  Lots of data and information is stored behind APIs. This page covers all resources available in LangChain for working with APIs.  Chains​  If you are just getting started and you have relatively simple APIs, you should get started with chains. Chains are a sequence of predetermined steps, so they are good to get started with as they give you more control and let you understand what is happening better.  OpenAPI Chain Agents​  Agents are more complex, and involve multiple queries to the LLM to understand what to do. The downside of agents are that you have less control. The upside is that they are more powerful, which allows you to use them on larger and more complex schemas.  OpenAPI Agent Previous Tabular Question Answering Next Summarization Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/use_cases/agent_simulations/",
    "title": "Agent Simulations | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Use cases QA and Chat over Documents Retrieval-augmented generation (RAG) Tabular Question Answering Interacting with APIs Summarization Agent Simulations Generative Agents Violation of Expectations Chain Autonomous Agents Chatbots Extraction Use casesAgent Simulations Agent Simulations  Agent simulations involve taking multiple agents and having them interact with each other.  They tend to use a simulation environment with an LLM as their \"core\" and helper classes to prompt them to ingest certain inputs such as prebuilt \"observations\", and react to new stimuli.  They also benefit from long-term memory so that they can preserve state between interactions.  Like Autonomous Agents, Agent Simulations are still experimental and based on papers such as this one.  📄️ Generative Agents  This script implements a generative agent based on the paper Generative Agents: Interactive Simulacra of Human Behavior by Park, et. al.  📄️ Violation of Expectations Chain  This page demonstrates how to use the ViolationOfExpectationsChain. This chain extracts insights from chat conversations  Previous Summarization Next Generative Agents Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/chat_memory",
    "title": "Chat Memory | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Cassandra Chat Memory Cloudflare D1-Backed Chat Memory Convex Chat Memory DynamoDB-Backed Chat Memory Firestore Chat Memory Momento-Backed Chat Memory MongoDB Chat Memory Motörhead Memory PlanetScale Chat Memory Redis-Backed Chat Memory Upstash Redis-Backed Chat Memory Xata Chat Memory Zep Memory Stores ComponentsChat Memory Chat Memory 📄️ Cassandra Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Cassandra cluster.  📄️ Cloudflare D1-Backed Chat Memory  This integration is only supported in Cloudflare Workers.  📄️ Convex Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for Convex.  📄️ DynamoDB-Backed Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a DynamoDB instance.  📄️ Firestore Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a firestore.  📄️ Momento-Backed Chat Memory  For distributed, serverless persistence across chat sessions, you can swap in a Momento-backed chat message history.  📄️ MongoDB Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a MongoDB instance.  📄️ Motörhead Memory  Motörhead is a memory server implemented in Rust. It automatically handles incremental summarization in the background and allows for stateless applications.  📄️ PlanetScale Chat Memory  Because PlanetScale works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.  📄️ Redis-Backed Chat Memory  For longer-term persistence across chat sessions, you can swap out the default in-memory chatHistory that backs chat memory classes like BufferMemory for a Redis instance.  📄️ Upstash Redis-Backed Chat Memory  Because Upstash Redis works via a REST API, you can use this with Vercel Edge, Cloudflare Workers and other Serverless environments.  📄️ Xata Chat Memory  Xata is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a  📄️ Zep Memory  Zep is a memory server that stores, summarizes, embeds, indexes, and enriches conversational AI chat histories, autonomous agent histories, document Q&A histories and exposes them via simple, low-latency APIs.  Previous VectorStore Agent Toolkit Next Cassandra Chat Memory Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/document_transformers",
    "title": "Document transformers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers html-to-text @mozilla/readability OpenAI functions metadata tagger Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsDocument transformers Document transformers 📄️ html-to-text  When ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.  📄️ @mozilla/readability  When ingesting HTML documents for later retrieval, we are often interested only in the actual content of the webpage rather than semantics.  📄️ OpenAI functions metadata tagger  It can often be useful to tag ingested documents with structured metadata, such as the title, tone, or length of a document, to allow for more targeted similarity search later. However, for large numbers of documents, performing this labelling process manually can be tedious.  Previous YouTube transcripts Next html-to-text Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/aws",
    "title": "AWS | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ProvidersAWS AWS  All functionality related to Amazon AWS platform  LLMs​ Bedrock​  See a usage example.  import { Bedrock } from \"langchain/llms/bedrock\";  SageMaker Endpoint​  Amazon SageMaker is a system that can build, train, and deploy machine learning (ML) models with fully managed infrastructure, tools, and workflows.  We use SageMaker to host our model and expose it as the SageMaker Endpoint.  See a usage example.  import {   SagemakerEndpoint,   SageMakerLLMContentHandler, } from \"langchain/llms/sagemaker_endpoint\";  Text Embedding Models​ Bedrock​  See a usage example.  import { BedrockEmbeddings } from \"langchain/embeddings/bedrock\";  Document loaders​ AWS S3 Directory and File​  Amazon Simple Storage Service (Amazon S3) is an object storage service. AWS S3 Directory >AWS S3 Buckets  See a usage example for S3FileLoader.  npm Yarn pnpm npm install @aws-sdk/client-s3  import { S3Loader } from \"langchain/document_loaders/web/s3\";  Memory​ AWS DynamoDB​  AWS DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability.  We have to configure the AWS CLI.  npm Yarn pnpm npm install @aws-sdk/client-dynamodb   See a usage example.  import { DynamoDBChatMessageHistory } from \"langchain/stores/message/dynamodb\";  Previous Anthropic Next Google LLMs Bedrock SageMaker Endpoint Text Embedding Models Bedrock Document loaders AWS S3 Directory and File Memory AWS DynamoDB Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/google",
    "title": "Google | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ProvidersGoogle Google  Functionality related to Google Cloud Platform  Chat models​ ChatGoogleGenerativeAI​  Access Gemini models such as gemini-pro and gemini-pro-vision through the ChatGoogleGenerativeAI class.  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/google-genai   Configure your API key.  export GOOGLE_API_KEY=your-api-key  import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\";  const model = new ChatGoogleGenerativeAI({   modelName: \"gemini-pro\",   maxOutputTokens: 2048, });  // Batch and stream are also supported const res = await model.invoke([   [     \"human\",     \"What would be a good company name for a company that makes colorful socks?\",   ], ]);   Gemini vision models support image inputs when providing a single human message. For example:  const visionModel = new ChatGoogleGenerativeAI({   modelName: \"gemini-pro-vision\",   maxOutputTokens: 2048, }); const image = fs.readFileSync(\"./hotdog.jpg\").toString(\"base64\"); const input2 = [   new HumanMessage({     content: [       {         type: \"text\",         text: \"Describe the following image.\",       },       {         type: \"image_url\",         image_url: `data:image/png;base64,${image}`,       },     ],   }), ];  const res = await visionModel.invoke(input2);   The value of image_url must be a base64 encoded image (e.g., data:image/png;base64,abcd124).  Vertex AI​  Access PaLM chat models like chat-bison and codechat-bison via Google Cloud.  import { ChatGoogleVertexAI } from \"langchain/chat_models/googlevertexai\";  LLMs​ Vertex AI​  Access PaLM LLMs like text-bison and code-bison via Google Cloud.  import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";  Model Garden​  Access PaLM and hundreds of OSS models via Vertex AI Model Garden.  import { GoogleVertexAI } from \"langchain/llms/googlevertexai\";  Vector Store​ Vertex AI Vector Search​  Vertex AI Vector Search, formerly known as Vertex AI Matching Engine, provides the industry's leading high-scale low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.  import { MatchingEngine } from \"langchain/vectorstores/googlevertexai\";  Tools​ Google Search​ Set up a Custom Search Engine, following these instructions Get an API Key and Custom Search Engine ID from the previous step, and set them as environment variables GOOGLE_API_KEY and GOOGLE_CSE_ID respectively  There exists a GoogleCustomSearch utility which wraps this API. To import this utility:  import { GoogleCustomSearch } from \"langchain/tools\";   We can easily load this wrapper as a Tool (to use with an Agent). We can do this with:  const tools = [new GoogleCustomSearch({})]; // Pass this variable into your agent.  Previous AWS Next Microsoft Chat models ChatGoogleGenerativeAI Vertex AI LLMs Vertex AI Model Garden Vector Store Vertex AI Vector Search Tools Google Search Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/retrievers",
    "title": "Retrievers | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Chaindesk Retriever ChatGPT Plugin Retriever HyDE Retriever Amazon Kendra Retriever Metal Retriever Remote Retriever Supabase Hybrid Search Tavily Search API Time-Weighted Retriever Vector Store Vespa Retriever Zep Retriever Tools Agents and toolkits Chat Memory Stores ComponentsRetrievers Retrievers 📄️ Chaindesk Retriever  This example shows how to use the Chaindesk Retriever in a RetrievalQAChain to retrieve documents from a Chaindesk.ai datastore.  📄️ ChatGPT Plugin Retriever  This example shows how to use the ChatGPT Retriever Plugin within LangChain.  📄️ HyDE Retriever  This example shows how to use the HyDE Retriever, which implements Hypothetical Document Embeddings (HyDE) as described in this paper.  📄️ Amazon Kendra Retriever  Amazon Kendra is an intelligent search service provided by Amazon Web Services (AWS). It utilizes advanced natural language processing (NLP) and machine learning algorithms to enable powerful search capabilities across various data sources within an organization. Kendra is designed to help users find the information they need quickly and accurately, improving productivity and decision-making.  📄️ Metal Retriever  This example shows how to use the Metal Retriever in a RetrievalQAChain to retrieve documents from a Metal index.  📄️ Remote Retriever  This example shows how to use a Remote Retriever in a RetrievalQAChain to retrieve documents from a remote server.  📄️ Supabase Hybrid Search  Langchain supports hybrid search with a Supabase Postgres database. The hybrid search combines the postgres pgvector extension (similarity search) and Full-Text Search (keyword search) to retrieve documents. You can add documents via SupabaseVectorStore addDocuments function. SupabaseHybridKeyWordSearch accepts embedding, supabase client, number of results for similarity search, and number of results for keyword search as parameters. The getRelevantDocuments function produces a list of documents that has duplicates removed and is sorted by relevance score.  📄️ Tavily Search API  Tavily's Search API is a search engine built specifically for AI agents (LLMs), delivering real-time, accurate, and factual results at speed.  📄️ Time-Weighted Retriever  A Time-Weighted Retriever is a retriever that takes into account recency in addition to similarity. The scoring algorithm is:  📄️ Vector Store  Once you've created a Vector Store, the way to use it as a Retriever is very simple:  📄️ Vespa Retriever  This shows how to use Vespa.ai as a LangChain retriever.  📄️ Zep Retriever  This example shows how to use the Zep Retriever in a RetrievalQAChain to retrieve documents from Zep memory store.  Previous Zep Next Chaindesk Retriever Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/tools",
    "title": "Tools | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools ChatGPT Plugins Connery Action Tool Discord Tool Gmail Tool Google Calendar Tool Google Places Tool Agent with AWS Lambda Python interpreter tool SearchApi tool Searxng Search tool Tavily Search Web Browser Tool Wikipedia tool WolframAlpha Tool Agent with Zapier NLA Integration Agents and toolkits Chat Memory Stores ComponentsTools Tools 📄️ ChatGPT Plugins  This example shows how to use ChatGPT Plugins within LangChain abstractions.  📄️ Connery Action Tool  Using this tool, you can integrate individual Connery Action into your LangChain agent.  📄️ Discord Tool  The Discord Tool gives your agent the ability to search, read, and write messages to discord channels.  📄️ Gmail Tool  The Gmail Tool allows your agent to create and view messages from a linked email account.  📄️ Google Calendar Tool  The Google Calendar Tools allow your agent to create and view Google Calendar events from a linked calendar.  📄️ Google Places Tool  The Google Places Tool allows your agent to utilize the Google Places API in order to find addresses,  📄️ Agent with AWS Lambda  Full docs here//docs.aws.amazon.com/lambda/index.html  📄️ Python interpreter tool  This tool executes code and can potentially perform destructive actions. Be careful that you trust any code passed to it!  📄️ SearchApi tool  The SearchApi tool connects your agents and chains to the internet.  📄️ Searxng Search tool  The SearxngSearch tool connects your agents and chains to the internet.  📄️ Tavily Search  Tavily Search is a robust search API tailored specifically for LLM Agents. It seamlessly integrates with diverse data sources to ensure a superior, relevant search experience.  📄️ Web Browser Tool  The Webbrowser Tool gives your agent the ability to visit a website and extract information. It is described to the agent as  📄️ Wikipedia tool  The WikipediaQueryRun tool connects your agents and chains to Wikipedia.  📄️ WolframAlpha Tool  The WolframAlpha tool connects your agents and chains to WolframAlpha's state-of-the-art computational intelligence engine.  📄️ Agent with Zapier NLA Integration  Full docs here//nla.zapier.com/start/  Previous Zep Retriever Next ChatGPT Plugins Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/components",
    "title": "Components | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores Components Components  LangChain.js feature integrations with third party libraries, services and more.  🗃️ LLMs  25 items  🗃️ Chat models  23 items  🗃️ Document loaders  2 items  🗃️ Document transformers  3 items  🗃️ Text embedding models  18 items  🗃️ Vector stores  39 items  🗃️ Retrievers  12 items  🗃️ Tools  15 items  🗃️ Agents and toolkits  6 items  🗃️ Chat Memory  13 items  🗃️ Stores  6 items  Previous OpenAI Next LLMs Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/openai",
    "title": "OpenAI | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ProvidersOpenAI OpenAI  All functionality related to OpenAI  OpenAI is American artificial intelligence (AI) research laboratory consisting of the non-profit OpenAI Incorporated and its for-profit subsidiary corporation OpenAI Limited Partnership. OpenAI conducts AI research with the declared intention of promoting and developing a friendly AI. OpenAI systems run on an Azure-based supercomputing platform from Microsoft.  The OpenAI API is powered by a diverse set of models with different capabilities and price points.  ChatGPT is the Artificial Intelligence (AI) chatbot developed by OpenAI.  Installation and Setup​ Get an OpenAI api key and set it as an environment variable (OPENAI_API_KEY) LLM​  See a usage example.  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { OpenAI } from \"@langchain/openai\";  Chat model​  See a usage example.  import { ChatOpenAI } from \"@langchain/openai\";  Text Embedding Model​  See a usage example  import { OpenAIEmbeddings } from \"@langchain/openai\";  Retriever​  See a usage example.  import { ChatGPTPluginRetriever } from \"langchain/retrievers/remote\";  Chain​ import { OpenAIModerationChain } from \"langchain/chains\";  Previous Microsoft Next Components Installation and Setup LLM Chat model Text Embedding Model Retriever Chain Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/toolkits",
    "title": "Agents and toolkits | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Connery Toolkit JSON Agent Toolkit OpenAPI Agent Toolkit AWS Step Functions Toolkit SQL Agent Toolkit VectorStore Agent Toolkit Chat Memory Stores ComponentsAgents and toolkits Agents and toolkits 📄️ Connery Toolkit  Using this toolkit, you can integrate Connery Actions into your LangChain agent.  📄️ JSON Agent Toolkit  This example shows how to load and use an agent with a JSON toolkit.  📄️ OpenAPI Agent Toolkit  This example shows how to load and use an agent with a OpenAPI toolkit.  📄️ AWS Step Functions Toolkit  AWS Step Functions are a visual workflow service that helps developers use AWS services to build distributed applications, automate processes, orchestrate microservices, and create data and machine learning (ML) pipelines.  📄️ SQL Agent Toolkit  This example shows how to load and use an agent with a SQL toolkit.  📄️ VectorStore Agent Toolkit  This example shows how to load and use an agent with a vectorstore toolkit.  Previous Agent with Zapier NLA Integration Next Connery Toolkit Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/vectorstores",
    "title": "Vector stores | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Memory AnalyticDB Astra DB Azure Cosmos DB Cassandra Chroma ClickHouse CloseVector Cloudflare Vectorize Convex Elasticsearch Faiss Google Vertex AI Matching Engine HNSWLib LanceDB Milvus Momento Vector Index (MVI) MongoDB Atlas MyScale Neo4j Vector Index OpenSearch PGVector Pinecone Prisma Qdrant Redis Rockset SingleStore Supabase Tigris TypeORM Typesense USearch Vectara Vercel Postgres Voy Weaviate Xata Zep Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsVector stores Vector stores 📄️ Memory  MemoryVectorStore is an in-memory, ephemeral vectorstore that stores embeddings in-memory and does an exact, linear search for the most similar embeddings. The default similarity metric is cosine similarity, but can be changed to any of the similarity metrics supported by ml-distance.  📄️ AnalyticDB  AnalyticDB for PostgreSQL is a massively parallel processing (MPP) data warehousing service that is designed to analyze large volumes of data online.  📄️ Astra DB  Only available on Node.js.  📄️ Azure Cosmos DB  Azure Cosmos DB for MongoDB vCore makes it easy to create a database with full native MongoDB support. You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account’s connection string. Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that’s stored in Azure Cosmos DB.  📄️ Cassandra  Only available on Node.js.  📄️ Chroma  Chroma is a AI-native open-source vector database focused on developer productivity and happiness. Chroma is licensed under Apache 2.0.  📄️ ClickHouse  Only available on Node.js.  📄️ CloseVector  available on both browser and Node.js  📄️ Cloudflare Vectorize  If you're deploying your project in a Cloudflare worker, you can use Cloudflare Vectorize with LangChain.js.  📄️ Convex  LangChain.js supports Convex as a vector store, and supports the standard similarity search.  📄️ Elasticsearch  Only available on Node.js.  📄️ Faiss  Only available on Node.js.  📄️ Google Vertex AI Matching Engine  Only available on Node.js.  📄️ HNSWLib  Only available on Node.js.  📄️ LanceDB  LanceDB is an embedded vector database for AI applications. It is open source and distributed with an Apache-2.0 license.  📄️ Milvus  Milvus is a vector database built for embeddings similarity search and AI applications.  📄️ Momento Vector Index (MVI)  MVI: the most productive, easiest to use, serverless vector index for your data. To get started with MVI, simply sign up for an account. There's no need to handle infrastructure, manage servers, or be concerned about scaling. MVI is a service that scales automatically to meet your needs. Whether in Node.js, browser, or edge, Momento has you covered.  📄️ MongoDB Atlas  Only available on Node.js.  📄️ MyScale  Only available on Node.js.  📄️ Neo4j Vector Index  Neo4j is an open-source graph database with integrated support for vector similarity search.  📄️ OpenSearch  Only available on Node.js.  📄️ PGVector  To enable vector search in a generic PostgreSQL database, LangChain.js supports using the pgvector Postgres extension.  📄️ Pinecone  Only available on Node.js.  📄️ Prisma  For augmenting existing models in PostgreSQL database with vector search, Langchain supports using Prisma together with PostgreSQL and pgvector Postgres extension.  📄️ Qdrant  Qdrant is a vector similarity search engine. It provides a production-ready service with a convenient API to store, search, and manage points - vectors with an additional payload.  📄️ Redis  Redis is a fast open source, in-memory data store.  📄️ Rockset  Rockset is a real-time analyitics SQL database that runs in the cloud.  📄️ SingleStore  SingleStoreDB is a high-performance distributed SQL database that supports deployment both in the cloud and on-premise. It provides vector storage, as well as vector functions like dotproduct and euclideandistance, thereby supporting AI applications that require text similarity matching.  📄️ Supabase  Langchain supports using Supabase Postgres database as a vector store, using the pgvector postgres extension. Refer to the Supabase blog post for more information.  📄️ Tigris  Tigris makes it easy to build AI applications with vector embeddings.  📄️ TypeORM  To enable vector search in a generic PostgreSQL database, LangChain.js supports using TypeORM with the pgvector Postgres extension.  📄️ Typesense  Vector store that utilizes the Typesense search engine.  📄️ USearch  Only available on Node.js.  📄️ Vectara  Vectara is a platform for building GenAI applications. It provides an easy-to-use API for document indexing and querying that is managed by Vectara and is optimized for performance and accuracy.  📄️ Vercel Postgres  LangChain.js supports using the @vercel/postgres package to use generic Postgres databases  📄️ Voy  Voy is a WASM vector similarity search engine written in Rust.  📄️ Weaviate  Weaviate is an open source vector database that stores both objects and vectors, allowing for combining vector search with structured filtering. LangChain connects to Weaviate via the weaviate-ts-client package, the official Typescript client for Weaviate.  📄️ Xata  Xata is a serverless data platform, based on PostgreSQL. It provides a type-safe TypeScript/JavaScript SDK for interacting with your database, and a UI for managing your data.  📄️ Zep  Zep is an open source long-term memory store for LLM applications. Zep makes it easy to add relevant documents,  Previous Voyage AI Next Memory Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/document_loaders",
    "title": "Document loaders | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders File Loaders Web Loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsDocument loaders Document loaders 🗃️ File Loaders  14 items  🗃️ Web Loaders  23 items  Previous YandexGPT Next File Loaders Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/text_embedding",
    "title": "Text embedding models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Azure OpenAI Bedrock Cloudflare Workers AI Cohere Google AI Google PaLM Google Vertex AI Gradient AI HuggingFace Inference Llama CPP Minimax Mistral AI Ollama OpenAI TensorFlow Together AI HuggingFace Transformers Voyage AI Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsText embedding models Text embedding models 📄️ Azure OpenAI  The OpenAIEmbeddings class can also use the OpenAI API on Azure to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.  📄️ Bedrock  Amazon Bedrock is a fully managed service that makes base models from Amazon and third-party model providers accessible through an API.  📄️ Cloudflare Workers AI  If you're deploying your project in a Cloudflare worker, you can use Cloudflare's built-in Workers AI embeddings with LangChain.js.  📄️ Cohere  The CohereEmbeddings class uses the Cohere API to generate embeddings for a given text.  📄️ Google AI  You can access Google's generative AI embeddings models through  📄️ Google PaLM  This integration does not support embeddings-* model. Check Google AI embeddings.  📄️ Google Vertex AI  The GoogleVertexAIEmbeddings class uses Google's Vertex AI PaLM models  📄️ Gradient AI  The GradientEmbeddings class uses the Gradient AI API to generate embeddings for a given text.  📄️ HuggingFace Inference  This Embeddings integration uses the HuggingFace Inference API to generate embeddings for a given text using by default the sentence-transformers/distilbert-base-nli-mean-tokens model. You can pass a different model name to the constructor to use a different model.  📄️ Llama CPP  Only available on Node.js.  📄️ Minimax  The MinimaxEmbeddings class uses the Minimax API to generate embeddings for a given text.  📄️ Mistral AI  The MistralAIEmbeddings class uses the Mistral AI API to generate embeddings for a given text.  📄️ Ollama  The OllamaEmbeddings class uses the /api/embeddings route of a locally hosted Ollama server to generate embeddings for given texts.  📄️ OpenAI  The OpenAIEmbeddings class uses the OpenAI API to generate embeddings for a given text. By default it strips new line characters from the text, as recommended by OpenAI, but you can disable this by passing stripNewLines: false to the constructor.  📄️ TensorFlow  This Embeddings integration runs the embeddings entirely in your browser or Node.js environment, using TensorFlow.js. This means that your data isn't sent to any third party, and you don't need to sign up for any API keys. However, it does require more memory and processing power than the other integrations.  📄️ Together AI  The TogetherAIEmbeddings class uses the Together AI API to generate embeddings for a given text.  📄️ HuggingFace Transformers  The TransformerEmbeddings class uses the Transformers.js package to generate embeddings for a given text.  📄️ Voyage AI  The VoyageEmbeddings class uses the Voyage AI REST API to generate embeddings for a given text.  Previous OpenAI functions metadata tagger Next Azure OpenAI Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/microsoft",
    "title": "Microsoft | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ProvidersMicrosoft Microsoft  All functionality related to Microsoft Azure and other Microsoft products.  LLM​ Azure OpenAI​  Microsoft Azure, often referred to as Azure is a cloud computing platform run by Microsoft, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). Microsoft Azure supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems.  Azure OpenAI is an Azure service with powerful language models from OpenAI including the GPT-3, Codex and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.  Set the environment variables to get access to the Azure OpenAI service.  Inside an environment variables file (.env).  AZURE_OPENAI_API_KEY=\"YOUR-API-KEY\" AZURE_OPENAI_API_VERSION=\"YOUR-BASE-URL\" AZURE_OPENAI_API_INSTANCE_NAME=\"YOUR-INSTANCE-NAME\" AZURE_OPENAI_API_DEPLOYMENT_NAME=\"YOUR-DEPLOYMENT-NAME\" AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME=\"YOUR-EMBEDDINGS-NAME\"   See a usage example.  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/openai  import { OpenAI } from \"@langchain/openai\";  Text Embedding Models​ Azure OpenAI​  See a usage example  import { OpenAIEmbeddings } from \"@langchain/openai\";  const embeddings = new OpenAIEmbeddings({   azureOpenAIApiKey: \"YOUR-API-KEY\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY   azureOpenAIApiVersion: \"YOUR-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION   azureOpenAIApiInstanceName: \"{MY_INSTANCE_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME   azureOpenAIApiDeploymentName: \"{DEPLOYMENT_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_EMBEDDINGS_DEPLOYMENT_NAME });  Chat Models​ Azure OpenAI​  See a usage example  import { ChatOpenAI } from \"@langchain/openai\";  const model = new ChatOpenAI({   temperature: 0.9,   azureOpenAIApiKey: \"SOME_SECRET_VALUE\", // In Node.js defaults to process.env.AZURE_OPENAI_API_KEY   azureOpenAIApiVersion: \"YOUR-API-VERSION\", // In Node.js defaults to process.env.AZURE_OPENAI_API_VERSION   azureOpenAIApiInstanceName: \"{MY_INSTANCE_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_INSTANCE_NAME   azureOpenAIApiDeploymentName: \"{DEPLOYMENT_NAME}\", // In Node.js defaults to process.env.AZURE_OPENAI_API_DEPLOYMENT_NAME });  Document loaders​ Azure Blob Storage​  Azure Blob Storage is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data.  Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol, Network File System (NFS) protocol, and Azure Files REST API. Azure Files are based on the Azure Blob Storage.  Azure Blob Storage is designed for:  Serving images or documents directly to a browser. Storing files for distributed access. Streaming video and audio. Writing to log files. Storing data for backup and restore, disaster recovery, and archiving. Storing data for analysis by an on-premises or Azure-hosted service. npm Yarn pnpm npm install @azure/storage-blob   See a usage example for the Azure Blob Storage.  import { AzureBlobStorageContainerLoader } from \"langchain/document_loaders/web/azure_blob_storage_container\";   See a usage example for the Azure Files.  import { AzureBlobStorageFileLoader } from \"langchain/document_loaders/web/azure_blob_storage_file\";  Previous Google Next OpenAI LLM Azure OpenAI Text Embedding Models Azure OpenAI Chat Models Azure OpenAI Document loaders Azure Blob Storage Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/llms/",
    "title": "LLMs | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs AI21 AlephAlpha AWS SageMakerEndpoint Azure OpenAI Bedrock Cloudflare Workers AI Cohere Fake LLM Fireworks Google PaLM Google Vertex AI Gradient AI HuggingFaceInference Llama CPP NIBittensor Ollama OpenAI PromptLayer OpenAI RaycastAI Replicate Together AI WatsonX AI Writer YandexGPT Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsLLMs LLMs Features (natively supported)​  All LLMs implement the Runnable interface, which comes with default implementations of all methods, ie. invoke, batch, stream, map. This gives all LLMs basic support for invoking, streaming, batching and mapping requests, which by default is implemented as below:  Streaming support defaults to returning an AsyncIterator of a single value, the final result returned by the underlying LLM provider. This obviously doesn't give you token-by-token streaming, which requires native support from the LLM provider, but ensures your code that expects an iterator of tokens can work for any of our LLM integrations. Batch support defaults to calling the underlying LLM in parallel for each input. The concurrency can be controlled with the maxConcurrency key in RunnableConfig. Map support defaults to calling .invoke across all instances of the array which it was called on.  Each LLM integration can optionally provide native implementations for invoke, streaming or batch, which, for providers that support it, can be more efficient. The table shows, for each integration, which features have been implemented with native support.  Model Invoke Stream Batch AI21 ✅ ❌ ✅ AlephAlpha ✅ ❌ ✅ CloudflareWorkersAI ✅ ✅ ✅ Cohere ✅ ❌ ✅ Fireworks ✅ ✅ ✅ GooglePaLM ✅ ❌ ✅ HuggingFaceInference ✅ ❌ ✅ LlamaCpp ✅ ✅ ✅ Ollama ✅ ✅ ✅ OpenAIChat ✅ ✅ ✅ PromptLayerOpenAIChat ✅ ✅ ✅ OpenAI ✅ ✅ ✅ OpenAIChat ✅ ✅ ✅ PromptLayerOpenAI ✅ ✅ ✅ PromptLayerOpenAIChat ✅ ✅ ✅ Portkey ✅ ✅ ✅ Replicate ✅ ❌ ✅ SageMakerEndpoint ✅ ✅ ✅ Writer ✅ ❌ ✅ YandexGPT ✅ ❌ ✅ Previous Components Next LLMs Features (natively supported) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/chat/",
    "title": "Chat models | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Alibaba Tongyi Anthropic Anthropic Functions Azure OpenAI Baidu Wenxin Bedrock Cloudflare Workers AI Cohere Fake LLM Fireworks Google AI Google PaLM Google Vertex AI Llama CPP Minimax Mistral AI NIBittensorChatModel Ollama Ollama Functions OpenAI PromptLayer OpenAI YandexGPT Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ComponentsChat models Chat models Features (natively supported)​  All ChatModels implement the Runnable interface, which comes with default implementations of all methods, ie. invoke, batch, stream. This gives all ChatModels basic support for invoking, streaming and batching, which by default is implemented as below:  Streaming support defaults to returning an AsyncIterator of a single value, the final result returned by the underlying ChatModel provider. This obviously doesn't give you token-by-token streaming, which requires native support from the ChatModel provider, but ensures your code that expects an iterator of tokens can work for any of our ChatModel integrations. Batch support defaults to calling the underlying ChatModel in parallel for each input. The concurrency can be controlled with the maxConcurrency key in RunnableConfig. Map support defaults to calling .invoke across all instances of the array which it was called on.  Each ChatModel integration can optionally provide native implementations to truly enable invoke, streaming or batching requests. The table shows, for each integration, which features have been implemented with native support.  Model Invoke Stream Batch ChatAnthropic ✅ ✅ ✅ ChatBaiduWenxin ✅ ❌ ✅ ChatCloudflareWorkersAI ✅ ✅ ✅ ChatFireworks ✅ ✅ ✅ ChatGooglePaLM ✅ ❌ ✅ ChatLlamaCpp ✅ ✅ ✅ ChatMinimax ✅ ❌ ✅ ChatOllama ✅ ✅ ✅ ChatOpenAI ✅ ✅ ✅ PromptLayerChatOpenAI ✅ ✅ ✅ PortkeyChat ✅ ✅ ✅ ChatYandexGPT ✅ ❌ ✅ Previous YandexGPT Next Chat models Features (natively supported) Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/platforms/anthropic",
    "title": "Anthropic | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Anthropic AWS Google Microsoft OpenAI Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores ProvidersAnthropic Anthropic  All functionality related to Anthropic models.  Anthropic is an AI safety and research company, and is the creator of Claude. This page covers all integrations between Anthropic models and LangChain.  Prompting Overview​  Claude is chat-based model, meaning it is trained on conversation data. However, it is a text based API, meaning it takes in single string. It expects this string to be in a particular format. This means that it is up the user to ensure that is the case. LangChain provides several utilities and helper functions to make sure prompts that you write - whether formatted as a string or as a list of messages - end up formatted correctly.  Specifically, Claude is trained to fill in text for the Assistant role as part of an ongoing dialogue between a human user (Human:) and an AI assistant (Assistant:). Prompts sent via the API must contain \\n\\nHuman: and \\n\\nAssistant: as the signals of who's speaking. The final turn must always be \\n\\nAssistant: - the input string cannot have \\n\\nHuman: as the final role.  Because Claude is chat-based but accepts a string as input, it can be treated as either a LangChain ChatModel or LLM. This means there are two wrappers in LangChain - ChatAnthropic and Anthropic. It is generally recommended to use the ChatAnthropic wrapper, and format your prompts as ChatMessages (we will show examples of this below). This is because it keeps your prompt in a general format that you can easily then also use with other models (should you want to). However, if you want more fine-grained control over the prompt, you can use the Anthropic wrapper - we will show and example of this as well. The Anthropic wrapper however is deprecated, as all functionality can be achieved in a more generic way using ChatAnthropic.  Prompting Best Practices​  Anthropic models have several prompting best practices compared to OpenAI models.  System Messages may only be the first message  Anthropic models require any system messages to be the first one in your prompts.  ChatAnthropic​  ChatAnthropic is a subclass of LangChain's ChatModel, meaning it works best with ChatPromptTemplate. You can import this wrapper with the following code:  TIP  See this section for general instructions on installing integration packages.  npm Yarn pnpm npm install @langchain/anthropic  import { ChatAnthropic } from \"@langchain/anthropic\"; const model = new ChatAnthropic({});   When working with ChatModels, it is preferred that you design your prompts as ChatPromptTemplates. Here is an example below of doing that:  import { ChatPromptTemplate } from \"langchain/prompts\";  const prompt = ChatPromptTemplate.fromMessages([   [\"system\", \"You are a helpful chatbot\"],   [\"human\", \"Tell me a joke about {topic}\"], ]);   You can then use this in a chain as follows:  const chain = prompt.pipe(model); await chain.invoke({ topic: \"bears\" });  Previous Providers Next AWS Prompting Overview Prompting Best Practices ChatAnthropic Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/integrations/stores/",
    "title": "Stores | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Providers Components LLMs Chat models Document loaders Document transformers Text embedding models Vector stores Retrievers Tools Agents and toolkits Chat Memory Stores File System Store In Memory Store IORedis Upstash Redis Vercel KV ComponentsStores Stores  Storing data in key value format is quick and efficient, and can be a powerful tool for LLM applications. The BaseStore class provides a simple interface for getting, setting, deleting and iterating over lists of key value pairs.  The public API of BaseStore in LangChain JS offers four main methods:  abstract mget(keys: K[]): Promise<(V | undefined)[]>;  abstract mset(keyValuePairs: [K, V][]): Promise<void>;  abstract mdelete(keys: K[]): Promise<void>;  abstract yieldKeys(prefix?: string): AsyncGenerator<K | string>;   The m prefix stands for multiple, and indicates that these methods can be used to get, set and delete multiple key value pairs at once. The yieldKeys method is a generator function that can be used to iterate over all keys in the store, or all keys with a given prefix.  It's that simple!  So far LangChain.js has two base integrations for BaseStore:  InMemoryStore LocalFileStore (Node.js only) Use Cases​ Chat history​  If you're building web apps with chat, the BaseStore family of integrations can come in very handy for storing and retrieving chat history.  Caching​  The BaseStore family can be a useful alternative to our other caching integrations. For example the LocalFileStore allows for persisting data through the file system. It also is incredibly fast, so your users will be able to access cached data in a snap.  See the individual sections for deeper dives on specific storage providers.  Reading Data​ In Memory​  Reading data is simple with KV stores. Below is an example using the InMemoryStore and the .mget() method. We'll also set our generic value type to string so we can have type safety setting our strings.  Import the InMemoryStore class.  import { InMemoryStore } from \"langchain/storage/in_memory\";   Instantiate a new instance and pass string as our generic for the value type.  const store = new InMemoryStore<string>();   Next we can call .mset() to write multiple values at once.  const data: [string, string][] = [   [\"key1\", \"value1\"],   [\"key2\", \"value2\"], ];  await store.mset(data);   Finally, call the .mget() method to retrieve the values from our store.  const data = await store.mget([\"key1\", \"key2\"]);  console.log(data); /**  * [\"value1\", \"value2\"]  */  File System​  When using the file system integration we need to instantiate via the fromPath method. This is required because it needs to preform checks to ensure the directory exists and is readable/writable. You also must use a directory when using LocalFileStore because each entry is stored as a unique file in the directory.  import { LocalFileStore } from \"langchain/storage/file_system\";  const pathToStore = \"./my-store-directory\"; const store = await LocalFileStore.fromPath(pathToStore);   To do this we can define an encoder for initially setting our data, and a decoder for when we retrieve data.  const encoder = new TextEncoder(); const decoder = new TextDecoder();  const data: [string, Uint8Array][] = [   [\"key1\", encoder.encode(new Date().toDateString())],   [\"key2\", encoder.encode(new Date().toDateString())], ];  await store.mset(data);  const data = await store.mget([\"key1\", \"key2\"]);  console.log(data.map((v) => decoder.decode(v))); /**  * [ 'Wed Jan 03 2024', 'Wed Jan 03 2024' ]  */  Writing Data​ In Memory​  Writing data is simple with KV stores. Below is an example using the InMemoryStore and the .mset() method. We'll also set our generic value type to Date so we can have type safety setting our dates.  Import the InMemoryStore class.  import { InMemoryStore } from \"langchain/storage/in_memory\";   Instantiate a new instance and pass Date as our generic for the value type.  const store = new InMemoryStore<Date>();   Finally we can call .mset() to write multiple values at once.  const data: [string, Date][] = [   [\"date1\", new Date()],   [\"date2\", new Date()], ];  await store.mset(data);  File System​  When using the file system integration we need to instantiate via the fromPath method. This is required because it needs to preform checks to ensure the directory exists and is readable/writable. You also must use a directory when using LocalFileStore because each entry is stored as a unique file in the directory.  import { LocalFileStore } from \"langchain/storage/file_system\";  const pathToStore = \"./my-store-directory\"; const store = await LocalFileStore.fromPath(pathToStore);   When defining our data we must convert the values to Uint8Array because the file system integration only supports binary data.  To do this we can define an encoder for initially setting our data, and a decoder for when we retrieve data.  const encoder = new TextEncoder(); const decoder = new TextDecoder();  const data: [string, Uint8Array][] = [   [\"key1\", encoder.encode(new Date().toDateString())],   [\"key2\", encoder.encode(new Date().toDateString())], ];  await store.mset(data);  Previous Zep Memory Next File System Store Use Cases Chat history Caching Reading Data In Memory File System Writing Data In Memory File System Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  },
  {
    "url": "https://js.langchain.com/docs/get_started/introduction/",
    "title": "Introduction | 🦜️🔗 Langchain",
    "date": "2024-01-17",
    "content": "Skip to main content 🦜️🔗 LangChain Docs Use cases Integrations API Reference More Also by LangChain Search K Get started Introduction Installation Quickstart LangChain Expression Language Get started Interface How to Cookbook Why use LCEL? Modules Model I/O Retrieval Chains Agents More Security Guides Ecosystem Get startedIntroduction Introduction  LangChain is a framework for developing applications powered by language models. It enables applications that:  Are context-aware: connect a language model to sources of context (prompt instructions, few shot examples, content to ground its response in, etc.) Reason: rely on a language model to reason (about how to answer based on provided context, what actions to take, etc.)  This framework consists of several parts.  LangChain Libraries: The Python and JavaScript libraries. Contains interfaces and integrations for a myriad of components, a basic run time for combining these components into chains and agents, and off-the-shelf implementations of chains and agents. LangChain Templates: A collection of easily deployable reference architectures for a wide variety of tasks. (Python only) LangServe: A library for deploying LangChain chains as a REST API. (Python only) LangSmith: A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.  Together, these products simplify the entire application lifecycle:  Develop: Write your applications in LangChain/LangChain.js. Hit the ground running using Templates for reference. Productionize: Use LangSmith to inspect, test and monitor your chains, so that you can constantly improve and deploy with confidence. Deploy: Turn any chain into an API with LangServe. LangChain Libraries​  The main value props of the LangChain packages are:  Components: composable tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks  Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.  Get started​  Here's how to install LangChain, set up your environment, and start building.  We recommend following our Quickstart guide to familiarize yourself with the framework by building your first LangChain application.  Read up on our Security best practices to make sure you're developing safely with LangChain.  NOTE  These docs focus on the JS/TS LangChain library. Head here for docs on the Python LangChain library.  LangChain Expression Language (LCEL)​  LCEL is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt + LLM” chain to the most complex chains.  Overview: LCEL and its benefits Interface: The standard interface for LCEL objects How-to: Key features of LCEL Cookbook: Example code for accomplishing common tasks Modules​  LangChain provides standard, extendable interfaces and integrations for the following modules:  Model I/O​  Interface with language models  Retrieval​  Interface with application-specific data  Agents​  Let models choose which tools to use given high-level directives  Examples, ecosystem, and resources​ Use cases​  Walkthroughs and techniques for common end-to-end use cases, like:  Document question answering RAG Agents and much more... Integrations​  LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. Check out our growing list of integrations.  API reference​  Head to the reference section for full documentation of all classes and methods in the LangChain and LangChain Experimental packages.  Developer's guide​  Check out the developer's guide for guidelines on contributing and help getting your dev environment set up.  Community​  Head to the Community navigator to find places to ask questions, share feedback, meet other developers, and dream about the future of LLM's.  Previous Get started Next Installation LangChain Libraries Get started LangChain Expression Language (LCEL) Modules Examples, ecosystem, and resources Use cases Integrations API reference Developer's guide Community Community Discord Twitter GitHub Python JS/TS More Homepage Blog Copyright © 2024 LangChain, Inc."
  }
]
