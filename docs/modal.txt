[{"url":"https://modal.com/docs/examples/youtube_face_detection","title":"Face detection on YouTube videos | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Face detection on YouTube videos  This is an example that uses OpenCV as well as video utilities pytube and moviepy to process video files in parallel.  The face detection is a pretty simple model built into OpenCV and is not state of the art.  The result The Python code  We start by setting up the container image we need. This requires installing a few dependencies needed for OpenCV as well as downloading the face detection model  import os  import modal  OUTPUT_DIR = \"/tmp/\" FACE_CASCADE_FN = \"haarcascade_frontalface_default.xml\"  image = (     modal.Image.debian_slim()     .apt_install(\"libgl1-mesa-glx\", \"libglib2.0-0\", \"wget\", \"git\")     .run_commands(         f\"wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/{FACE_CASCADE_FN} -P /root\"     )     .pip_install(         \"pytube @ git+https://github.com/modal-labs/pytube\",         \"opencv-python~=4.7.0.72\",         \"moviepy~=1.0.3\",     ) ) stub = modal.Stub(\"example-youtube-face-detection\", image=image)  with image.imports():     import cv2     import moviepy.editor     import pytube Copy  For temporary storage and sharing of downloaded movie clips, we use a network file system.  net_file_system = modal.NetworkFileSystem.new() Copy Face detection function  The face detection function takes three arguments:  A filename to the source clip A time slice denoted by start and a stop in seconds  The function extracts the subclip from the movie file (which is stored on the network file system), runs face detection on every frame in its slice, and stores the resulting video back to the shared storage.  @stub.function(network_file_systems={\"/clips\": net_file_system}, timeout=600) def detect_faces(fn, start, stop):     # Extract the subclip from the video     clip = moviepy.editor.VideoFileClip(fn).subclip(start, stop)      # Load face detector     face_cascade = cv2.CascadeClassifier(f\"/root/{FACE_CASCADE_FN}\")      # Run face detector on frames     imgs = []     for img in clip.iter_frames():         gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)         faces = face_cascade.detectMultiScale(gray, 1.1, 4)         for x, y, w, h in faces:             cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)         imgs.append(img)      # Create mp4 of result     out_clip = moviepy.editor.ImageSequenceClip(imgs, fps=clip.fps)     out_fn = f\"/clips/{start:04d}.mp4\"     out_clip.write_videofile(out_fn)     return out_fn Copy Modal entrypoint function  This ‘entrypoint’ into Modal controls the main flow of the program:  Download the video from YouTube Fan-out face detection of individual 1s clips Stitch the results back into a new video @stub.function(network_file_systems={\"/clips\": net_file_system}, retries=1) def process_video(url):     print(f\"Downloading video from '{url}'\")     yt = pytube.YouTube(url)     stream = yt.streams.filter(file_extension=\"mp4\").first()     fn = stream.download(output_path=\"/clips/\", max_retries=5)      # Get duration     duration = moviepy.editor.VideoFileClip(fn).duration      # Create (start, stop) intervals     intervals = [(fn, offset, offset + 1) for offset in range(int(duration))]      print(\"Processing each range of 1s intervals using a Modal map\")     out_fns = list(detect_faces.starmap(intervals))      print(\"Converting detections to video clips\")     out_clips = [moviepy.editor.VideoFileClip(out_fn) for out_fn in out_fns]      print(\"Concatenating results\")     final_clip = moviepy.editor.concatenate_videoclips(out_clips)     final_fn = \"/clips/out.mp4\"     final_clip.write_videofile(final_fn)      # Return the full image data     with open(final_fn, \"rb\") as f:         return os.path.basename(fn), f.read() Copy Local entrypoint  The code we run locally to fire up the Modal job is quite simple  Take a YouTube URL on the command line Run the Modal function Store the output data @stub.local_entrypoint() def main(youtube_url: str = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"):     fn, movie_data = process_video.remote(youtube_url)     abs_fn = os.path.join(OUTPUT_DIR, fn)     print(f\"writing results to {abs_fn}\")     with open(abs_fn, \"wb\") as f:         f.write(movie_data) Copy Running the script  Running this script should take approximately a minute or less. It might output a lot of warnings to standard error. These are generally harmless.  Note that we don’t preserve the sound in the video.  Further directions  As you can tell from the resulting video, this face detection model is not state of the art. It has plenty of false positives (non-faces being labeled faces) and false negatives (real faces not being labeled). For better model, consider a modern one based on deep learning.  Face detection on YouTube videos The result The Python code Face detection function Modal entrypoint function Local entrypoint Running the script Further directions Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 03_scaling_out/youtube_face_detection.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/discord-musicgen","title":"BoomBot: Create your own music samples on Discord | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler BoomBot: Create your own music samples on Discord  (quick links: try it out on Discord; watch demo with audio; view source code)  MusicGen is the latest milestone language model in conditional music generation, with great results. We wanted a space to easily play around with the model and share our creations, so we created a Discord community featuring BoomBot, an on-demand music sample generator.  You can call BoomBot in the Discord server by simply typing /generate, then prompting it with a text description of the music you’d like to create, and even a file of the melody you’d like to condition on, along with other specifiable parameters.  Everything, from the backend API to our React frontend, is deployed serverlessly using Modal, and the code is available here.  Code overview  BoomBot runs the MusicGen model remotely in a GPU-accelerated container — when a user interacts with the Discord bot using the slash command /generate, Discord sends a webhook call to Modal, which then generates a music sample using the text description and other user inputs from the command. We also deploy a React single page application as static files into a Modal web endpoint for our web app.  We go into detail into each of these steps below, and provide commands for running each of them individually. To follow along, clone the repo and set up a Discord token for yourself.  Language model  We use Audiocraft, a PyTorch library that provides the code and models for MusicGen, and load both the 3.3B large and 1.5B melody models to use depending on the user input (large for just text, melody for text and melody). We can install all our dependencies and “bake” both models into our image to avoid downloading our models during inference and take advantage of Modal’s incredibly fast cold-start times:  def download_models():     from audiocraft.models import MusicGen      MusicGen.get_pretrained(\"large\")     MusicGen.get_pretrained(\"melody\")   image = (     modal.Image.debian_slim()     .apt_install(\"git\", \"ffmpeg\")     .pip_install(         \"pynacl\", # for Discord authentication         \"torch\",         \"soundfile\",         \"pydub\",         \"git+https://github.com/facebookresearch/audiocraft.git\",     )     .run_function(download_models, gpu=\"any\") ) stub.image = image Copy  We then write our model code within Modal’s @stub.cls decorator, with the generate function processing the user input and generating audio as bytes that we can save to a file later.  It’s useful to test out our model directly before we build out our backend API, so we define a local_entrypoint. We can then run our model from the CLI using inputs of our choice:  modal run main.py --prompt “soothing jazz” --duration 20 --format \"mp3\" --melody \"https://cdn.discordapp.com/ephemeral-attachments/1124040605304094870/1124371596484804880/bach.mp3\" Copy Discord bot  Now that we’ve loaded our model and wrote our function, we’d like to trigger it from Discord. We can do this using slash commands — a feature that lets you register a text command on Discord that triggers a custom webhook when a user interacts with it. We handle all our Discord events in a FastAPI app in bot.py. Luckily, we can deploy this app easily and serverlessly using Modal’s @asgi_app decorator.  Create a Discord app  To connect our model to a Discord bot, we’re first going to create an application on the Discord Developer Portal.  Go to the Discord Developer Portal and login with your Discord account. On the portal, go to Applications and create a new application by clicking New Application in the top right next to your profile picture. Copy your Discord application’s Public Key (in General Information) and create a custom Modal secret for it. On Modal’s secret creation page, paste the public key as the secret value with the key DISCORD_PUBLIC_KEY, then name this secret boombot-discord-secret.  Then go back to your application on the Discord Developer Portal, as we need to do a few more things to finish setting up our bot.  Register a Slash Command  Next, we’re going to register a command for our Discord app via an HTTP endpoint.  Run the following command in your terminal, replacing the appropriate variable inputs. BOT_TOKEN can be found by resetting your token in the application’s Bot section, and CLIENT_ID is the Application ID available in General Information.  BOT_TOKEN='replace_with_bot_token' CLIENT_ID='replace_with_application_id' curl -X POST \\ -H 'Content-Type: application/json' \\ -H \"Authorization: Bot $BOT_TOKEN\" \\ -d '{   \"name\":\"generate\",   \"description\":\"generate music\",   \"options\":[     {       \"name\":\"prompt\",       \"description\":\"Describe the music you want to generate\",       \"type\":3,       \"required\":true     },     {       \"name\":\"duration\",       \"description\":\"Duration of clip, in seconds (max 60)\",       \"type\":4,       \"required\":false,       \"min_value\":2,       \"max_value\":60     },     {       \"name\":\"melody\",       \"description\":\"File of melody you'\\''d like to condition (takes first 30 secs if longer)\",       \"type\":11,       \"required\":false     },     {       \"name\":\"format\",       \"description\":\"Desired format of output (default .wav)\",       \"type\":3,       \"required\":false,       \"choices\":[         {           \"name\": \".wav\",           \"value\": \"wav\"         },         {           \"name\": \".mp3\",           \"value\": \"mp3\"         }       ]     }   ] }' \"https://discord.com/api/v10/applications/$CLIENT_ID/commands\" Copy  This will register a Slash Command for your bot named generate, and has a few parameters like prompt, duration, and format. More information about the command structure can be found in the Discord docs here.  Deploy a Modal web endpoint  We then create a POST /generate endpoint in bot.py using FastAPI and Modal’s @asgi_app decorator to handle interactions with our Discord app (so that every time a user does a slash command, we can respond to it).  Note that since Discord requires an interaction response within 3 seconds, we use spawn to kick off Audiocraft.generate as a background task while returning a defer message to Discord within the time limit. We then update our response with the results once the model has finished running.  You can deploy this app by running the following command from your root directory:  modal deploy src.bot Copy  Copy the Modal URL that is printed in the output and go back to your application’s General Information section on the Discord Developer Portal. Paste the URL, making sure to append the path of your POST endpoint, in the Interactions Endpoint URL field, then click Save Changes. If your endpoint is valid, it will properly save and you can start receiving interactions via this web endpoint.  Finish setting up Discord bot  To start using the Slash Command you just set up, you need to invite the bot to a Discord server. To do so, go to your application’s OAuth2 section on the Discord Developer Portal. Select applications.commands as the scope of your bot and copy the invite URL that is generated at the bottom of the page.  Paste this URL in your browser, then select your desired server (create a new server if needed) and click Authorize. Now you can open your Discord server and type /{name of your slash command} - your bot should be connected and ready for you to use!  React frontend  We also added a simple React frontend to our FastAPI app to serve as a landing page for our Discord server.  We added the frontend to our existing app in bot.py by simply using Mount to mount our local directory at /assets in our container, then instructing FastAPI to serve this static file directory at our root path.  For a more in-depth tutorial on deploying your web app on Modal, refer to our Document OCR web app example.  Run this app on Modal  All the source code for this example can be found on Github.  If you’re interested in learning more about Modal, check out our docs and other examples.  BoomBot: Create your own music samples on Discord Code overview Language model Discord bot Create a Discord app Register a Slash Command Deploy a Modal web endpoint Finish setting up Discord bot React frontend Run this app on Modal © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/slack-finetune","title":"DoppelBot: Replace your CEO with an LLM | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler DoppelBot: Replace your CEO with an LLM  (quick links: add to your own Slack; source code)  Internally at Modal, we spend a lot of time talking to each other on Slack. Now, with the advent of open-source large language models, we had started to wonder if all of this wasn’t a bit redundant. Could we have these language models bike-shed on Slack for us, so we could spend our time on higher leverage activities such as paddleboarding in Tahiti instead?  To test this, we fine-tuned OpenLLaMa on Erik’s Slack messages, and @erik-bot was born.  Since then, @erik-bot has been an invaluable asset to us, in areas ranging from API design to legal advice to thought leadership.  We were planning on releasing the weights for @erik-bot to the world, but all our metrics have been going up and to the right a little too much since we’ve launched him…  So, we are releasing the next best thing. DoppelBot is a Slack bot that you can install in your own workspace, and fine-tune on your own Slack messages. Follow the instructions here to replace your own CEO with an LLM today.  All the components—scraping, fine-tuning, inference and slack event handlers run on Modal, and the code itself is open-source and available here. If you’re new to Modal, it’s worth reiterating that all of these components are also serverless and scale to zero. This means that you can deploy and forget about them, because you’ll only pay for compute when your app is used!  How it works  DoppelBot uses the Slack SDK to scrape messages from a Slack workspace, and converts them into prompt/response pairs. It uses these to fine-tune a language model using Low-Rank Adaptation (LoRA), a technique that produces a small adapter that can be merged with the base model when needed, instead of modifying all the parameters in the base model. The fine-tuned adapters for each user are stored in a Modal NetworkFileSystem. When a user @s the bot, Slack sends a webhook call to Modal, which loads the adapter for that user and generates a response.  We go into detail into each of these steps below, and provide commands for running each of them individually. To follow along, clone the repo and set up a Slack token for yourself.  Scraping slack  The scraper uses Modal’s .map() to fetch messages from all public channels in parallel. Each thread is split into contiguous messages from the target users and continguous messages from other users. These become our question/response pairs. Later, these will be fed into the model as prompts in the following format:  You are {user}, employee at a fast-growing startup. Below is an input conversation that takes place in the company's internal Slack. Write a response that appropriately continues the conversation.  ### Input: {question}  ### Response: {response} Copy  Initial versions of the model were prone to generating short responses — unsurprising, because a majority of Slack communication is pretty terse. Adding a minimum character length for the target user’s messages fixed this.  If you’re following along at home, you can run the scraper with the following command:  modal run src.scrape::scrape --user=\"<user>\" Copy  Scraped results are stored in a Modal NetworkFileSystem, so they can be used by the next step.  Fine-tuning  Next, we use the prompts to fine-tune a language model. We chose OpenLLaMa 7B because of its permissive license and high quality relative to its small size. Fine-tuning is done using Low-Rank Adaptation (LoRA), a parameter-efficient fine-tuning technique that produces a small adapter that can be merged with the base model when needed (~60MB for the rank we’re using).  Our fine-tuning implementation is based on the excellent alpaca-lora repo that uses the same technique to fine-tune LLaMa using the Alpaca dataset.  Because of the typically small sample sizes we’re working with, training for longer than a couple hundred steps (with our batch size of 128) quickly led to overfitting. Admittedly, we haven’t thoroughly evaluated the hyperparameter space yet — do reach out to us if you’re interested in collaborating on this!  To try this step yourself, run:  modal run src.finetune --user=\"<user>\" Copy Inference  At inference time, loading the model with the LoRA adapter for a user takes 15-20s, so it’s important that we avoid doing this for every incoming request. We also need to maintain separate pools of containers for separate users (since we can only load one model into memory at once). To accomplish this, we’re using the hottest new Modal feature: parametrized functions.  With parametrized functions, every user model gets its own pool of containers that scales up when there are incoming requests, and scales to 0 when there’s none. Here’s what that looks like stripped down to the essentials:  @stub.cls(gpu=A100(memory=40)) class OpenLlamaModel():     def __init__(self, user: str):          base_model = LlamaForCausalLM.from_pretrained(...)         model = PeftModel.from_pretrained(base_model, f\"/vol/models/{user}\")         ...      @method()     def generate(self, input: str):        output = self.model.generate(...) Copy  The rest of inference.py is just calling generate from the transformers library with the input formatted as a prompt.  If you’ve fine-tuned a model already in the previous step, you can run inference using it now:  modal run src.inference --user=\"<user>\" Copy  (We have a list of sample inputs in the file, but you can also try it out with your own messages!)  Slack Bot  Finally, it all comes together in bot.py. As you might have guessed, all events from Slack are handled by serverless Modal functions. We handle 3 types of events:  url_verification: To verify that this is a Slack app, Slack expects us to return a challenge string. app_mention: When the bot is mentioned in a channel, we retrieve the recent messages from that thread, do some basic cleaning and call the user’s model to generate a response. model = OpenLlamaModel.remote(user, team_id) result = model.generate(messages) Copy doppel slash command: This command kicks off the scraping -> finetuning pipeline for the user.  To deploy the slackbot in its entirety, you need to run:  modal deploy src.bot Copy Multi-Workspace Support  Everything we’ve talked about so far is for a single-workspace Slack app. To make it work with multiple workspaces, we’ll need to handle workspace installation and authentication with OAuth, and also store some state for each workspace.  Luckily, Slack’s Bolt framework provides a complete (but frugally documented) OAuth implemention. A neat feature is that the OAuth state can be backed by a file system, so all we need to do is point Bolt at a Modal NetworkFileSystem, and then we don’t need to worry about managing this state ourselves.  To store state for each workspace, we’re using Neon, a serverless Postgres database that’s really easy to set up and just works. If you’re interested in developing a multi-workspace app, follow our instructions on how to set up Neon with Modal.  Next Steps  If you’ve made it this far, you have just found a way to increase your team’s productivity by 10x! Congratulations on the well-earned vacation! 🎉  If you’re interested in learning more about Modal, check out our docs and other examples.  DoppelBot: Replace your CEO with an LLM How it works Scraping slack Fine-tuning Inference Slack Bot Multi-Workspace Support Next Steps © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/stable_diffusion_xl_turbo","title":"Stable Diffusion XL Turbo Image-to-image | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Stable Diffusion XL Turbo Image-to-image  This example is similar to the Stable Diffusion XL example, but it’s a distilled model trained for real-time synthesis and is image-to-image. Learn more about it here.  Input prompt: dog wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k  Input Output    Basic setup from io import BytesIO from pathlib import Path  from modal import Image, Stub, build, enter, gpu, method Copy Define a container image image = Image.debian_slim().pip_install(     \"Pillow~=10.1.0\",     \"diffusers~=0.24\",     \"transformers~=4.35\",  # This is needed for `import torch`     \"accelerate~=0.25\",  # Allows `device_map=\"auto\"``, which allows computation of optimized device_map     \"safetensors~=0.4\",  # Enables safetensor format as opposed to using unsafe pickle format )  stub = Stub(\"stable-diffusion-xl-turbo\", image=image)  with image.imports():     import torch     from diffusers import AutoPipelineForImage2Image     from diffusers.utils import load_image     from huggingface_hub import snapshot_download     from PIL import Image Copy Load model and run inference  The container lifecycle __enter__ function loads the model at startup. Then, we evaluate it in the inference function.  To avoid excessive cold-starts, we set the idle timeout to 240 seconds, meaning once a GPU has loaded the model it will stay online for 4 minutes before spinning down. This can be adjusted for cost/experience trade-offs.  @stub.cls(gpu=gpu.A10G(), container_idle_timeout=240) class Model:     @build()     def download_models(self):         # Ignore files that we don't need to speed up download time.         ignore = [             \"*.bin\",             \"*.onnx_data\",             \"*/diffusion_pytorch_model.safetensors\",         ]          snapshot_download(\"stabilityai/sdxl-turbo\", ignore_patterns=ignore)      @enter()     def enter(self):         self.pipe = AutoPipelineForImage2Image.from_pretrained(             \"stabilityai/sdxl-turbo\",             torch_dtype=torch.float16,             variant=\"fp16\",             device_map=\"auto\",         )      @method()     def inference(self, image_bytes, prompt):         init_image = load_image(Image.open(BytesIO(image_bytes))).resize(             (512, 512)         )         num_inference_steps = 4         strength = 0.9         # \"When using SDXL-Turbo for image-to-image generation, make sure that num_inference_steps * strength is larger or equal to 1\"         # See: https://huggingface.co/stabilityai/sdxl-turbo         assert num_inference_steps * strength >= 1          image = self.pipe(             prompt,             image=init_image,             num_inference_steps=num_inference_steps,             strength=strength,             guidance_scale=0.0,         ).images[0]          byte_stream = BytesIO()         image.save(byte_stream, format=\"PNG\")         image_bytes = byte_stream.getvalue()          return image_bytes   DEFAULT_IMAGE_PATH = Path(__file__).parent / \"demo_images/dog.png\"   @stub.local_entrypoint() def main(     image_path=DEFAULT_IMAGE_PATH,     prompt=\"dog wizard, gandalf, lord of the rings, detailed, fantasy, cute, adorable, Pixar, Disney, 8k\", ):     with open(image_path, \"rb\") as image_file:         input_image_bytes = image_file.read()         output_image_bytes = Model().inference.remote(input_image_bytes, prompt)      dir = Path(\"/tmp/stable-diffusion-xl-turbo\")     if not dir.exists():         dir.mkdir(exist_ok=True, parents=True)      output_path = dir / \"output.png\"     print(f\"Saving it to {output_path}\")     with open(output_path, \"wb\") as f:         f.write(output_image_bytes) Copy Running the model  We can run the model with different parameters using the following command,  modal run stable_diffusion_xl_turbo.py --prompt=\"harry potter, glasses, wizard\" --image-path=\"dog.png\" Copy Stable Diffusion XL Turbo Image-to-image Basic setup Define a container image Load model and run inference Running the model Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/stable_diffusion/stable_diffusion_xl_turbo.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference","title":"API Reference | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config API Reference  This is the API reference for the modal Python package, which allows you to run distributed applications on Modal.  The reference is intended to be limited to low-level descriptions of various programmatic functionality. If you’re just getting started with Modal, we would instead recommend looking at the guide first.  API Reference © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/hackernews_alerts","title":"Hacker News Slackbot | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Hacker News Slackbot  In this example, we use Modal to deploy a cron job that periodically queries Hacker News for new posts matching a given search term, and posts the results to Slack.  Import and define the stub  Let’s start off with imports, and defining a Modal stub.  import os from datetime import datetime, timedelta  import modal  stub = modal.Stub(\"example-hn-bot\") Copy  Now, let’s define an image that has the slack-sdk package installed, in which we can run a function that posts a slack message.  slack_sdk_image = modal.Image.debian_slim().pip_install(\"slack-sdk\") Copy Defining the function and importing the secret  Our Slack bot will need access to a bot token. We can use Modal’s Secrets interface to accomplish this. To quickly create a Slack bot secret, navigate to the create secret page, select the Slack secret template from the list options, and follow the instructions in the “Where to find the credentials?” panel. Name your secret hn-bot-slack, so that the code in this example still works.  Now, we define the function post_to_slack, which simply instantiates the Slack client using our token, and then uses it to post a message to a given channel name.  @stub.function(     image=slack_sdk_image, secret=modal.Secret.from_name(\"hn-bot-slack\") ) async def post_to_slack(message: str):     import slack_sdk      client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])     client.chat_postMessage(channel=\"hn-alerts\", text=message) Copy Searching Hacker News  We are going to use Algolia’s Hacker News Search API to query for posts matching a given search term in the past X days. Let’s define our search term and query period.  QUERY = \"serverless\" WINDOW_SIZE_DAYS = 1 Copy  Let’s also define an image that has the requests package installed, so we can query the API.  requests_image = modal.Image.debian_slim().pip_install(\"requests\") Copy  We can now define our main entrypoint, that queries Algolia for the term, and calls post_to_slack on all the results. We specify a schedule in the function decorator, which means that our function will run automatically at the given interval.  @stub.function(image=requests_image) def search_hackernews():     import requests      url = \"http://hn.algolia.com/api/v1/search\"      threshold = datetime.utcnow() - timedelta(days=WINDOW_SIZE_DAYS)      params = {         \"query\": QUERY,         \"numericFilters\": f\"created_at_i>{threshold.timestamp()}\",     }      response = requests.get(url, params, timeout=10).json()     urls = [item[\"url\"] for item in response[\"hits\"] if item.get(\"url\")]      print(f\"Query returned {len(urls)} items.\")      post_to_slack.for_each(urls) Copy Test running  We can now test run our scheduled function as follows: modal run hackernews_alerts.py::stub.search_hackernews  Defining the schedule and deploying  Let’s define a function that will be called by Modal every day  @stub.function(schedule=modal.Period(days=1)) def run_daily():     search_hackernews.remote() Copy  In order to deploy this as a persistent cron job, you can run modal deploy hackernews_alerts.py,  Once the job is deployed, visit the apps page page to see its execution history, logs and other stats.  Hacker News Slackbot Import and define the stub Defining the function and importing the secret Searching Hacker News Test running Defining the schedule and deploying Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 05_scheduling/hackernews_alerts.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/mlc_inference","title":"Llama 2 inference with MLC | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Llama 2 inference with MLC  Machine Learning Compilation (MLC) is high-performance tool for serving LLMs including Llama 2. We will use the mlc_chat package and the pre-compiled Llama 2 binaries to run inference using a Modal GPU.  This example is adapted from this MLC chat collab.  import queue import threading import time from typing import Dict, Generator, List  import modal Copy Imports and global settings  Determine which GPU you want to use.  GPU: str = \"a10g\" Copy  Chose model size. At the time of writing MLC chat only provides compiled binaries for Llama 7b and 13b.  LLAMA_MODEL_SIZE: str = \"13b\" Copy  Define the image and Modal Stub. We use an official NVIDIA CUDA 12.2 image to match MLC CUDA requirements.  mlc_image = (     modal.Image.from_registry(         \"nvidia/cuda:12.2.2-cudnn8-runtime-ubuntu22.04\",         add_python=\"3.11\",     ).run_commands(         \"apt-get update\",         \"apt-get install -y curl git\",         # Install git lfs         \"curl -sSf https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | bash\",         \"apt-get install -y git-lfs\",         \"pip3 install --pre --force-reinstall mlc-ai-nightly-cu122 mlc-chat-nightly-cu122 -f https://mlc.ai/wheels\",     )     # \"These commands will download many prebuilt libraries as well as the chat     # configuration for Llama-2-7b that mlc_chat needs\" [...]     .run_commands(         \"mkdir -p dist/prebuilt\",         \"git clone https://github.com/mlc-ai/binary-mlc-llm-libs.git dist/prebuilt/lib\",         f\"cd dist/prebuilt && git clone https://huggingface.co/mlc-ai/mlc-chat-Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1\",     ) ) stub = modal.Stub(\"mlc-inference\")   LOADING_MESSAGE: str = f\"\"\"                        #%%%%%%%%%%%%(         #%%%%%%%%%%%%#                     ,%##%,         %%      .%#(%/         %%.                    %%.  .%#         (%*   (%*   %%         *%/                  .%#      %%.        .%% %%      (%*         %%                 #%,        ,%%%%%%%%%%%%%/        .%%         (%*                %%         (%*         %%*%(         #%,         %%              (%*         %%         *%(   %%         .%#         #%,             %%         *%/         %%.     *%(         #%,        .%#           /%(         %%         .%#         %%         .%%%%%%%%%%%%%.            (%/      ,%#         #%,           /%(      .%%         (%,              %%    %%.        .%%               %%    (%*         %%               (%*.%#         (%*                 /%/ %%         /%/                 %%%%%%%%%%%%%%                     %%%%%%%%%%%%%%                        LOADING => Llama 2 ({LLAMA_MODEL_SIZE}) [{GPU}]   \"\"\" Copy Define Modal function  The generate function will load MLC chat and the compiled model into memory and run inference on an input prompt. This is a generator, streaming tokens back to the client as they are generated.  @stub.function(gpu=GPU, image=mlc_image) def generate(prompt: str) -> Generator[Dict[str, str], None, None]:     from mlc_chat import ChatModule     from mlc_chat.callback import DeltaCallback      yield {         \"type\": \"loading\",         \"message\": LOADING_MESSAGE + \"\\n\\n\",     }      class QueueCallback(DeltaCallback):         \"\"\"Stream the output of the chat module to client.\"\"\"          def __init__(self, callback_interval: float):             super().__init__()             self.queue: queue.Queue = queue.Queue()             self.stopped = False             self.callback_interval = callback_interval          def delta_callback(self, delta_message: str):             self.stopped = False             self.queue.put(delta_message)          def stopped_callback(self):             self.stopped = True      cm = ChatModule(         model=f\"/dist/prebuilt/mlc-chat-Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1\",         model_lib_path=f\"/dist/prebuilt/lib/Llama-2-{LLAMA_MODEL_SIZE}-chat-hf-q4f16_1-cuda.so\",     )     queue_callback = QueueCallback(callback_interval=1)      # Generate tokens in a background thread so we can yield tokens     # to caller as a generator.     def _generate():         cm.generate(             prompt=prompt,             progress_callback=queue_callback,         )      background_thread = threading.Thread(target=_generate)     background_thread.start()      # Yield as a generator to caller function and spawn     # text-to-speech functions.     while not queue_callback.stopped:         yield {\"type\": \"output\", \"message\": queue_callback.queue.get()} Copy Run model  Create a local Modal entrypoint that calls the generate function. This uses the curses to render tokens as they are streamed back from Modal.  Run this locally with modal run -q mlc_inference.py --prompt \"What is serverless computing?\"  @stub.local_entrypoint() def main(prompt: str):     import curses      def _generate(stdscr):         buffer: List[str] = []          def _buffered_message():             return \"\".join(buffer) + (\"\\n\" * 4)          start = time.time()         for payload in generate.remote_gen(prompt):             message = payload[\"message\"]             if payload[\"type\"] == \"loading\":                 stdscr.clear()                 stdscr.addstr(0, 0, message)                 stdscr.refresh()             else:                 buffer.append(message)                 stdscr.clear()                 stdscr.addstr(0, 0, _buffered_message())                 stdscr.refresh()          n_tokens = len(buffer)         elapsed = time.time() - start         print(             f\"[DONE] {n_tokens} tokens generated in {elapsed:.2f}s ({n_tokens / elapsed:.0f} tok/s). Press any key to exit.\"         )         stdscr.getch()      curses.wrapper(_generate) Copy Llama 2 inference with MLC Imports and global settings Define Modal function Run model Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/mlc_inference.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/controlnet_gradio_demos","title":"Play with the ControlNet demos | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Play with the ControlNet demos  This example allows you to play with all 10 demonstration Gradio apps from the new and amazing ControlNet project. ControlNet provides a minimal interface allowing users to use images to constrain StableDiffusion’s generation process. With ControlNet, users can easily condition the StableDiffusion image generation with different spatial contexts including a depth maps, segmentation maps, scribble drawings, and keypoints!  Imports and config preamble import importlib import os import pathlib from dataclasses import dataclass, field  from fastapi import FastAPI from modal import Image, Secret, Stub, asgi_app Copy  Below are the configuration objects for all 10 demos provided in the original lllyasviel/ControlNet repo. The demos each depend on their own custom pretrained StableDiffusion model, and these models are 5-6GB each. We can only run one demo at a time, so this module avoids downloading the model and ‘detector’ dependencies for all 10 demos and instead uses the demo configuration object to download only what’s necessary for the chosen demo.  Even just limiting our dependencies setup to what’s required for one demo, the resulting container image is huge.  @dataclass(frozen=True) class DemoApp:     \"\"\"Config object defining a ControlNet demo app's specific dependencies.\"\"\"      name: str     model_files: list[str]     detector_files: list[str] = field(default_factory=list)   demos = [     DemoApp(         name=\"canny2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth\"         ],     ),     DemoApp(         name=\"depth2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_depth.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/dpt_hybrid-midas-501f0c75.pt\"         ],     ),     DemoApp(         name=\"fake_scribble2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth\"         ],     ),     DemoApp(         name=\"hed2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_hed.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/network-bsds500.pth\"         ],     ),     DemoApp(         name=\"hough2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_mlsd.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_large_512_fp32.pth\",             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/mlsd_tiny_512_fp32.pth\",         ],     ),     DemoApp(         name=\"normal2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_normal.pth\"         ],     ),     DemoApp(         name=\"pose2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_openpose.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/body_pose_model.pth\",             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/hand_pose_model.pth\",         ],     ),     DemoApp(         name=\"scribble2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"         ],     ),     DemoApp(         name=\"scribble2image_interactive\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_scribble.pth\"         ],     ),     DemoApp(         name=\"seg2image\",         model_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_seg.pth\"         ],         detector_files=[             \"https://huggingface.co/lllyasviel/ControlNet/resolve/main/annotator/ckpts/upernet_global_small.pth\"         ],     ), ] demos_map: dict[str, DemoApp] = {d.name: d for d in demos} Copy Pick a demo, any demo  Simply by changing the DEMO_NAME below, you can change which ControlNet demo app is setup and run by this Modal script.  DEMO_NAME = \"scribble2image\"  # Change this value to change the active demo app. selected_demo = demos_map[DEMO_NAME] Copy Setting up the dependencies  ControlNet requires a lot of dependencies which could be fiddly to setup manually, but Modal’s programmatic container image building Python APIs handle this complexity straightforwardly and automatically.  To run any of the 10 demo apps, we need the following:  a base Python 3 Linux image (we use Debian Slim) a bunch of third party PyPi packages git, so that we can download the ControlNet source code (there’s no controlnet PyPi package) some image process Linux system packages, including ffmpeg and demo specific pre-trained model and detector .pth files  That’s a lot! Fortunately, the code below is already written for you that stitches together a working container image ready to produce remarkable ControlNet images.  Note: a ControlNet model pipeline is now available in Huggingface’s diffusers package. But this does not contain the demo apps.  def download_file(url: str, output_path: pathlib.Path):     import httpx     from tqdm import tqdm      with open(output_path, \"wb\") as download_file:         with httpx.stream(\"GET\", url, follow_redirects=True) as response:             total = int(response.headers[\"Content-Length\"])             with tqdm(                 total=total, unit_scale=True, unit_divisor=1024, unit=\"B\"             ) as progress:                 num_bytes_downloaded = response.num_bytes_downloaded                 for chunk in response.iter_bytes():                     download_file.write(chunk)                     progress.update(                         response.num_bytes_downloaded - num_bytes_downloaded                     )                     num_bytes_downloaded = response.num_bytes_downloaded   def download_demo_files() -> None:     \"\"\"     The ControlNet repo instructs: 'Make sure that SD models are put in \"ControlNet/models\".'     'ControlNet' is just the repo root, so we place in /root/models.      The ControlNet repo also instructs: 'Make sure that... detectors are put in \"ControlNet/annotator/ckpts\".'     'ControlNet' is just the repo root, so we place in /root/annotator/ckpts.     \"\"\"     demo = demos_map[os.environ[\"DEMO_NAME\"]]     models_dir = pathlib.Path(\"/root/models\")     for url in demo.model_files:         filepath = pathlib.Path(url).name         download_file(url=url, output_path=models_dir / filepath)         print(f\"download complete for {filepath}\")      detectors_dir = pathlib.Path(\"/root/annotator/ckpts\")     for url in demo.detector_files:         filepath = pathlib.Path(url).name         download_file(url=url, output_path=detectors_dir / filepath)         print(f\"download complete for {filepath}\")     print(\"🎉 finished baking demo file(s) into image.\")   image = (     Image.debian_slim(python_version=\"3.10\")     .pip_install(         \"gradio==3.16.2\",         \"albumentations==1.3.0\",         \"opencv-contrib-python\",         \"imageio==2.9.0\",         \"imageio-ffmpeg==0.4.2\",         \"pytorch-lightning==1.5.0\",         \"omegaconf==2.1.1\",         \"test-tube>=0.7.5\",         \"streamlit==1.12.1\",         \"einops==0.3.0\",         \"transformers==4.19.2\",         \"webdataset==0.2.5\",         \"kornia==0.6\",         \"open_clip_torch==2.0.2\",         \"invisible-watermark>=0.1.5\",         \"streamlit-drawable-canvas==0.8.0\",         \"torchmetrics==0.6.0\",         \"timm==0.6.12\",         \"addict==2.4.0\",         \"yapf==0.32.0\",         \"prettytable==3.6.0\",         \"safetensors==0.2.7\",         \"basicsr==1.4.2\",         \"tqdm~=4.64.1\",     )     # xformers library offers performance improvement.     .pip_install(\"xformers\", pre=True)     .apt_install(\"git\")     # Here we place the latest ControlNet repository code into /root.     # Because /root is almost empty, but not entirely empty, `git clone` won't work,     # so this `init` then `checkout` workaround is used.     .run_commands(         \"cd /root && git init .\",         \"cd /root && git remote add --fetch origin https://github.com/lllyasviel/ControlNet.git\",         \"cd /root && git checkout main\",     )     .apt_install(\"ffmpeg\", \"libsm6\", \"libxext6\")     .run_function(         download_demo_files, secret=Secret.from_dict({\"DEMO_NAME\": DEMO_NAME})     ) ) stub = Stub(name=\"example-controlnet\", image=image)  web_app = FastAPI() Copy Serving the Gradio web UI  Each ControlNet gradio demo module exposes a block Gradio interface running in queue-mode, which is initialized in module scope on import and served on 0.0.0.0. We want the block interface object, but the queueing and launched webserver aren’t compatible with Modal’s serverless web endpoint interface, so in the import_gradio_app_blocks function we patch out these behaviors.  def import_gradio_app_blocks(demo: DemoApp):     from gradio import blocks      # The ControlNet repo demo scripts are written to be run as     # standalone scripts, and have a lot of code that executes     # in global scope on import, including the launch of a Gradio web server.     # We want Modal to control the Gradio web app serving, so we     # monkeypatch the .launch() function to be a no-op.     blocks.Blocks.launch = lambda self, server_name: print(         \"launch() has been monkeypatched to do nothing.\"     )      # each demo app module is a file like gradio_{name}.py     module_name = f\"gradio_{demo.name}\"     mod = importlib.import_module(module_name)     blocks = mod.block     # disable queueing mode, which is incompatible with our Modal web app setup.     blocks.enable_queue = False     return blocks Copy  Because the ControlNet gradio app’s are so time and compute intensive to cold-start the web app function is limited to running just 1 warm container. This way, while playing with the demos we can pay the cold-start cost once and have all web requests hit the warm container. Spinning up extra containers to handle additional requests would not be efficient given the cold-start time.  @stub.function(     gpu=\"A10G\",     concurrency_limit=1,     keep_warm=1, ) @asgi_app() def run():     from gradio.routes import mount_gradio_app      # mount for execution on Modal     return mount_gradio_app(         app=web_app,         blocks=import_gradio_app_blocks(demo=selected_demo),         path=\"/\",     ) Copy Have fun!  Serve your chosen demo app with modal serve controlnet_gradio_demos.py. If you don’t have any images ready at hand, try one that’s in the 06_gpu_and_ml/controlnet/demo_images/ folder.  StableDiffusion was already impressive enough, but ControlNet’s ability to so accurately and intuitively constrain the image generation process is sure to put a big, dumb grin on your face.  Play with the ControlNet demos Imports and config preamble Pick a demo, any demo Setting up the dependencies Serving the Gradio web UI Have fun! Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal serve 06_gpu_and_ml/controlnet/controlnet_gradio_demos.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/duckdb_nyc_taxi","title":"Use DuckDB to analyze lots of datasets in parallel | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Use DuckDB to analyze lots of datasets in parallel  The Taxi and Limousine Commission of NYC posts datasets with all trips in New York City. They are all Parquet files, which are very well suited for DuckDB which has excellent Parquet support. In fact, DuckDB lets us query remote Parquet data over HTTP which is excellent for what we want to do here.  Running this script should generate a plot like this in just 10-20 seconds, processing a few gigabytes of data:  Basic setup  We need various imports and to define an image with DuckDB installed:  import io import os from datetime import datetime  import modal  stub = modal.Stub(     \"example-duckdb-nyc-taxi\",     image=modal.Image.debian_slim().pip_install(\"matplotlib\", \"duckdb\"), ) Copy DuckDB Modal function  Defining the function that queries the data. This lets us run a SQL query against a remote Parquet file over HTTP Our query is pretty simple: it just aggregates total count numbers by date, but we also have some filters that remove garbage data (days that are outside the range).  @stub.function() def get_data(year, month):     import duckdb      url = f\"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_{year:04d}-{month:02d}.parquet\"     print(\"processing\", url, \"...\")      con = duckdb.connect(database=\":memory:\")     con.execute(\"install httpfs\")  # TODO: bake into the image     con.execute(\"load httpfs\")     q = \"\"\"     with sub as (         select tpep_pickup_datetime::date d, count(1) c         from read_parquet(?)         group by 1     )     select d, c from sub     where date_part('year', d) = ?  -- filter out garbage     and date_part('month', d) = ?   -- same     \"\"\"     con.execute(q, (url, year, month))     return list(con.fetchall()) Copy Plot results  Let’s define a separate function which:  Parallelizes over all files and dispatches calls to the previous function Aggregate the data and plot the result @stub.function() def create_plot():     from matplotlib import pyplot      # Map over all inputs and combine the data     inputs = [         (year, month)         for year in range(2018, 2023)         for month in range(1, 13)         if (year, month) <= (2022, 6)     ]     data: list[list[tuple[datetime, int]]] = [         [] for i in range(7)     ]  # Initialize a list for every weekday     for r in get_data.starmap(inputs):         for d, c in r:             data[d.weekday()].append((d, c))      # Initialize plotting     pyplot.style.use(\"ggplot\")     pyplot.figure(figsize=(16, 9))      # For each weekday, plot     for i, weekday in enumerate(         [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]     ):         data[i].sort()         dates = [d for d, _ in data[i]]         counts = [c for _, c in data[i]]         pyplot.plot(dates, counts, linewidth=3, alpha=0.8, label=weekday)      # Plot annotations     pyplot.title(\"Number of NYC yellow taxi trips by weekday, 2018-2022\")     pyplot.ylabel(\"Number of daily trips\")     pyplot.legend()     pyplot.tight_layout()      # Dump PNG and return     with io.BytesIO() as buf:         pyplot.savefig(buf, format=\"png\", dpi=300)         return buf.getvalue() Copy Entrypoint  Finally, we have some simple entrypoint code that kicks everything off. Note that the plotting function returns raw PNG data that we store locally.  Run this local entrypoint with modal run.  @stub.local_entrypoint() def main():     output_dir = \"/tmp/nyc\"     os.makedirs(output_dir, exist_ok=True)      fn = os.path.join(output_dir, \"nyc_taxi_chart.png\")     png_data = create_plot.remote()     with open(fn, \"wb\") as f:         f.write(png_data)     print(f\"wrote output to {fn}\") Copy Use DuckDB to analyze lots of datasets in parallel Basic setup DuckDB Modal function Plot results Entrypoint Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 10_integrations/duckdb_nyc_taxi.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples","title":"Modal","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler Featured examples Serving Diffusion models  Serve Stable Diffusion XL on Modal with a number of optimizations for blazingly fast inference   View on Github Voice chat with LLMs  Build an end-to-end real-time voice chat app powered by Vicuna   View on Github Stable Diffusion fine-tuning with Dreambooth  Fine-tune Stable Diffusion v1.5 on images of your pet using Dreambooth   View on Github Fast podcast transcriptions  Build an end-to-end podcast transcription app that leverages dozens of containers for super-fast processing   View on Github Document OCR job queue  Use Modal as an infinitely scalable job queue that can service async tasks from a web app   View on Github Parallel processing of Parquet files  Analyze data from the Taxi and Limousine Commission of NYC in parallel   View on Github Question-answering over large text corpuses  Build a question-answering web endpoint that can cite its sources   View on Github Hacker News Slackbot  Use Modal to deploy a cron job that periodically queries Hacker News for new posts, and posts the results to Slack   View on Github Real-time Object Detection  Create a web endpoint that leverages HuggingFace models to do object detection in real-time   View on Github ControlNet playgrounds  Play with all 10 demo Gradio apps from the ControlNet project   View on Github © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/fetch_stock_prices","title":"Fetching stock prices in parallel | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Fetching stock prices in parallel  This is a simple example that uses the Yahoo! Finance API to fetch a bunch of ETFs We do this in parallel, which demonstrates the ability to map over a set of items In this case, we fetch 100 stocks in parallel  You can run this script on the terminal with  modal run 03_scaling_out/fetch_stock_prices.py Copy  If everything goes well, it should plot something like this:  Setup  For this image, we need  httpx and beautifulsoup4 to fetch a list of ETFs from a HTML page yfinance to fetch stock prices from the Yahoo Finance API matplotlib to plot the result import io import os  import modal  stub = modal.Stub(     \"example-fetch-stock-prices\",     image=modal.Image.debian_slim().pip_install(         \"httpx~=0.24.0\",         \"yfinance~=0.2.31\",         \"beautifulsoup4~=4.12.2\",         \"matplotlib~=3.7.1\",     ), ) Copy Fetch a list of tickers  The yfinance package does not have a way to download a list of stocks. To get a list of stocks, we parse the HTML from Yahoo Finance using Beautiful Soup and ask for the top 100 ETFs.  @stub.function() def get_stocks():     import bs4     import httpx      headers = {         \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.122 Safari/537.36\",         \"referer\": \"https://finance.yahoo.com/\",     }     url = \"https://finance.yahoo.com/etfs?count=100&offset=0\"     res = httpx.get(url, headers=headers)     res.raise_for_status()     soup = bs4.BeautifulSoup(res.text, \"html.parser\")     for td in soup.find_all(\"td\", {\"aria-label\": \"Symbol\"}):         for link in td.find_all(\"a\", {\"data-test\": \"quoteLink\"}):             symbol = str(link.next)             print(f\"Found symbol {symbol}\")             yield symbol Copy Fetch stock prices  Now, let’s fetch the stock data. This is the function that we will parallelize. It’s fairly simple and just uses the yfinance package.  @stub.function() def get_prices(symbol):     import yfinance      print(f\"Fetching symbol {symbol}...\")     ticker = yfinance.Ticker(symbol)     data = ticker.history(period=\"1Y\")[\"Close\"]     print(f\"Done fetching symbol {symbol}!\")     return symbol, data.to_dict() Copy Plot the result  Here is our plotting code. We run this in Modal, although you could also run it locally. Note that the plotting code calls the other two functions. Since we plot the data in the cloud, we can’t display it, so we generate a PNG and return the binary content from the function.  @stub.function() def plot_stocks():     from matplotlib import pyplot, ticker      # Setup     pyplot.style.use(\"ggplot\")     fig, ax = pyplot.subplots(figsize=(8, 5))      # Get data     tickers = list(get_stocks.remote_gen())     if not tickers:         raise RuntimeError(\"Retrieved zero stock tickers!\")     data = list(get_prices.map(tickers))     first_date = min((min(prices.keys()) for symbol, prices in data if prices))     last_date = max((max(prices.keys()) for symbol, prices in data if prices))      # Plot every symbol     for symbol, prices in data:         if len(prices) == 0:             continue         dates = list(sorted(prices.keys()))         prices = list(prices[date] for date in dates)         changes = [             100.0 * (price / prices[0] - 1) for price in prices         ]  # Normalize to initial price         if changes[-1] > 20:             # Highlight this line             p = ax.plot(dates, changes, alpha=0.7)             ax.annotate(                 symbol,                 (last_date, changes[-1]),                 ha=\"left\",                 va=\"center\",                 color=p[0].get_color(),                 alpha=0.7,             )         else:             ax.plot(dates, changes, color=\"gray\", alpha=0.2)      # Configure axes and title     ax.yaxis.set_major_formatter(ticker.PercentFormatter())     ax.set_title(f\"Best ETFs {first_date.date()} - {last_date.date()}\")     ax.set_ylabel(f\"% change, {first_date.date()} = 0%\")      # Dump the chart to .png and return the bytes     with io.BytesIO() as buf:         pyplot.savefig(buf, format=\"png\", dpi=300)         return buf.getvalue() Copy Entrypoint  The entrypoint locally runs the app, gets the chart back as a PNG file, and saves it to disk.  OUTPUT_DIR = \"/tmp/\"   @stub.local_entrypoint() def main():     os.makedirs(OUTPUT_DIR, exist_ok=True)     data = plot_stocks.remote()     filename = os.path.join(OUTPUT_DIR, \"stock_prices.png\")     print(f\"saving data to {filename}\")     with open(filename, \"wb\") as f:         f.write(data) Copy Fetching stock prices in parallel Setup Fetch a list of tickers Fetch stock prices Plot the result Entrypoint Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 03_scaling_out/fetch_stock_prices.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/train_and_serve_diffusers_script","title":"Running Diffusers example scripts on Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Running Diffusers example scripts on Modal  The Diffusers library by HuggingFace provides a set of example training scripts that make it easy to experiment with various image fine-tuning techniques. This tutorial will show you how to run a Diffusers example script on Modal.  Select training script  You can see an up-to-date list of all the available examples in the examples subdirectory. It includes, among others, examples for:  Dreambooth Lora Text-to-image Fine-tuning Controlnet Fine-tuning Kandinsky Fine-tuning on Heroicons  In this tutorial, we’ll cover fine-tuning Stable Diffusion on the Heroicons dataset to stylize icons, using the Diffusers text-to-image script. Heroicons are website icons developed by the maker of TailwindCSS. They are open source, but there are only ~300 of them representing common concepts. What if you want icons depicting other concepts not covered by the original 300? Generative AI makes this possible - by using text-to-image models, you can just input your target concept and get a Heroicon of it back!          Fine-tuning results  Here are some of the results of the fine-tuning. As you can see, it’s not perfect - the model sometimes outputs multiple objects when prompted for one, and the outputs aren’t always sufficiently abstract/simple. But it’s very cool that the model is able to visualize even abstract concepts like “international monetary system” in the Heroicon style, and come up with an icon that actually makes sense. You can play around with the fine-tuned model yourself here.              In the HCON style, an icon of a camera In the HCON style, an icon of a golden retriever In the HCON style, an icon of a baby grand piano In the HCON style, an icon of a single ebike         In the HCON style, an icon of barack obama’s head In the HCON style, an icon of a BMW X5, from the front. Please show the entire car. In the HCON style, an icon of a castle In the HCON style, an icon of a single fountain pen         In the HCON style, an icon of a macbook pro computer In the HCON style, an icon of the interior of a library In the HCON style, an icon of a snowflake In the HCON style, an icon of a snowman         In the HCON style, an icon of a german shepherd In the HCON style, an icon of a water bottle In the HCON style, an icon representing a jail cell In the HCON style, an icon representing travel         In the HCON style, an icon that represents the future of AI In the HCON style, an icon representing skiing In the HCON style, an icon representing the international monetary system In the HCON style, an icon representing chemistry Creating the dataset  Most tutorials skip over dataset creation, but since we’re training on a novel dataset, we’ll cover the full process.  Download all the Heroicons from the Heroicons Github repo git clone git@github.com:tailwindlabs/heroicons.git cd optimized/24/outline # we are using the optimized, outline icons. You can also try using the solid icons Copy Postprocess the SVGs  Convert SVGs to PNGs  Most training models are unable to process SVGs, so we convert them first to PNGs.  Add white backgrounds to the PNGs  We also need to add white backgrounds to the PNGs. This is important - transparent backgrounds really confuse the model.  def add_white_background(input_path, output_path):     # Open the image     img = Image.open(input_path)      # Ensure the image has an alpha channel for transparency     img = img.convert('RGBA')      # Create a white background image     bg = Image.new('RGBA', img.size, (255, 255, 255))      # Combine the images     combined = Image.alpha_composite(bg, img)      # Save the result     combined.save(output_path, \"PNG\") Copy Add captions to create a metadata.csv file.  Since the Heroicon filenames match the concept they represent, we can parse them into captions. We also add a prefix to each caption: “In the HCON style, an icon of an <object>.” The purpose of this prefix is to associate a rare keyword, HCONto the particular Heroicon style.  We then create a metadata.csv file, where each row is an image file name with the associated caption. The metadata.csv file should be placed in the same directory as all the training images images, and contain a header row with the string file_name,text   heroicons_training_dir/   arrow.png   bike.png   cruiseShip.png   metadata.csv Copy file_name,text arrow.png,\"In the HCON style, an icon of an arrow\" bike.png,\"In the HCON style, an icon of an arrow\" cruiseShip.png,\"In the HCON style, an icon of an arrow\" Copy Upload the dataset to HuggingFace Hub.  This converts the dataset into an optimized Parquet file.  import os from datasets import load_dataset import huggingface_hub  # login to huggingface hf_key = os.environ[\"HF_TOKEN\"] huggingface_hub.login(hf_key)  dataset = load_dataset(\"imagefolder\", data_dir=\"/lg_white_bg_heroicon_png_img\", split=\"train\")  dataset.push_to_hub(\"yirenlu/heroicons\", private=True) Copy  The final Heroicons dataset on HuggingFace Hub is here.  Set up the dependencies for fine-tuning on Modal  You can put all of the sample code that follows in a single file, for example, train_and_serve_diffusers_script.py. In all the code below, we will be using the train_text_to_image.py script as an example, but you should modify depending on which Diffusers script that makes sense for your use case.  Start by specifying the Python modules that the training will depend on, including the Diffusers library, which contains the actual training script.  import os import sys from dataclasses import dataclass from pathlib import Path  from fastapi import FastAPI from modal import (     Image,     Secret,     Stub,     Volume,     asgi_app,     method, )  GIT_SHA = \"ed616bd8a8740927770eebe017aedb6204c6105f\"  image = (     Image.debian_slim(python_version=\"3.10\")     .pip_install(         \"accelerate==0.19\",         \"datasets~=2.13\",         \"ftfy~=6.1.1\",         \"gradio~=3.10\",         \"smart_open~=6.4.0\",         \"transformers==4.26.0\",         \"safetensors==0.2.8\",         \"torch~=2.1.0\",         \"torchvision~=0.16.0\",         \"triton~=2.1.0\",     )     .pip_install(\"xformers\", pre=True)     .apt_install(\"git\")     # Perform a shallow fetch of just the target `diffusers` commit, checking out     # the commit in the container's current working directory, /root. Then install     # the `diffusers` package.     .run_commands(         \"cd /root && git init .\",         \"cd /root && git remote add origin https://github.com/huggingface/diffusers\",         f\"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}\",         \"cd /root && pip install -e .\",     ) ) Copy Set up Volumes for training data and model output  Modal can’t access your local filesystem, so you should set up a Volume to eventually save the model once training is finished.  web_app = FastAPI() stub = Stub(name=\"example-diffusers-app\")  MODEL_DIR = Path(\"/model\") stub.training_data_volume = Volume.persisted(\"training-data-volume\") stub.model_volume = Volume.persisted(\"output-model-volume\")  VOLUME_CONFIG = {     \"/training_data\": stub.training_data_volume,     \"/model\": stub.model_volume, } Copy Set up config  Each Diffusers example script takes a different set of hyperparameters, so you will need to customize the config depending on the hyperparameters of the script. The code below shows some example parameters.  @dataclass class TrainConfig:     \"\"\"Configuration for the finetuning training.\"\"\"      # identifier for pretrained model on Hugging Face     model_name: str = \"runwayml/stable-diffusion-v1-5\"      resume_from_checkpoint: str = \"latest\"     # HuggingFace Hub dataset     dataset_name = \"yirenlu/heroicons\"      # Hyperparameters/constants from some of the Diffusers examples     # You should modify these to match the hyperparameters of the script you are using.     resolution: int = 512     train_batch_size: int = 1     gradient_accumulation_steps: int = 4     learning_rate: float = 1e-05     lr_scheduler: str = \"constant\"     lr_warmup_steps: int = 0     max_train_steps: int = 100     checkpointing_steps: int = 2000     mixed_precision: str = \"fp16\"     caption_column: str = \"text\"     max_grad_norm: int = 1     validation_prompt: str = \"an icon of a dragon creature\"   @dataclass class AppConfig:     \"\"\"Configuration information for inference.\"\"\"      num_inference_steps: int = 50     guidance_scale: float = 7.5 Copy Set up finetuning dataset  Each of the diffusers training scripts will utilize different argnames to refer to your input finetuning dataset. For example, it might be --instance_data_dir or --dataset_name. You will need to modify the code below to match the argname used by the training script you are using. Generally speaking, these argnames will correspond to either the name of a HuggingFace Hub dataset, or the path of a local directory containing your training dataset. This means that you should either upload your dataset to HuggingFace Hub, or push the dataset to a Volume and then attach that volume to the training function.  Upload to HuggingFace Hub  You can follow the instructions here to upload your dataset to the HuggingFace Hub.  Push dataset to Volume  To push your dataset to the /training_data volume you set up above, you can use modal volume put command to push an entire local directory to a location in the volume. For example, if your dataset is located at /path/to/dataset, you can push it to the volume with the following command:  modal volume put <volume-name> /path/to/dataset /training_data Copy  You can double check that the training data was properly uploaded to the volume by using modal volume ls:  modal volume ls <volume-name> /training_data Copy  You should see the contents of your dataset listed in the output.  Set up stub.function decorator on the training function.  Next, let’s write the stub.function decorator that will be used to launch the training function on Modal. The @stub.function decorator takes several arguments, including:  image - the Docker image that you want to use for training. In this case, we are using the image object that we defined above. gpu - the type of GPU you want to use for training. This argument is optional, but if you don’t specify a GPU, Modal will use a CPU. mounts - the local directories that you want to mount to the Modal container. In this case, we are mounting the local directory where the training images reside. volumes - the Modal volumes that you want to mount to the Modal container. In this case, we are mounting the Volume that we defined above. timeout argument - an integer representing the number of seconds that the training job should run for. This argument is optional, but if you don’t specify a timeout, Modal will use a default timeout of 300 seconds, or 5 minutes. The timeout argument has an upper limit of 24 hours. secrets - the Modal secrets that you want to mount to the Modal container. In this case, we are mounting the HuggingFace API token secret. @stub.function(     image=image,     gpu=\"A100\",  # finetuning is VRAM hungry, so this should be an A100     volumes=VOLUME_CONFIG,     timeout=3600 * 2,  # multiple hours     secrets=[Secret.from_name(\"huggingface-secret\")], ) Copy Define the training function  Now, finally, we define the training function itself. This training function does a bunch of preparatory things, but the core of it is the _exec_subprocess call to accelerate launch that launches the actual Diffusers training script. Depending on which Diffusers script you are using, you will want to modify the script name, and the arguments that are passed to it.  def train():     import huggingface_hub     from accelerate import notebook_launcher     from accelerate.utils import write_basic_config      # change this line to import the training script you want to use     from examples.text_to_image.train_text_to_image import main     from transformers import CLIPTokenizer      # set up TrainConfig     config = TrainConfig()      # set up runner-local image and shared model weight directories     os.makedirs(MODEL_DIR, exist_ok=True)      # set up hugging face accelerate library for fast training     write_basic_config(mixed_precision=\"fp16\")      # authenticate to hugging face so we can download the model weights     hf_key = os.environ[\"HF_TOKEN\"]     huggingface_hub.login(hf_key)      # check whether we can access the model repo     try:         CLIPTokenizer.from_pretrained(config.model_name, subfolder=\"tokenizer\")     except OSError as e:  # handle error raised when license is not accepted         license_error_msg = f\"Unable to load tokenizer. Access to this model requires acceptance of the license on Hugging Face here: https://huggingface.co/{config.model_name}.\"         raise Exception(license_error_msg) from e      def launch_training():         sys.argv = [             \"examples/text_to_image/train_text_to_image.py\",  # potentially modify             f\"--pretrained_model_name_or_path={config.model_name}\",             f\"--dataset_name={config.dataset_name}\",             \"--use_ema\",             f\"--output_dir={MODEL_DIR}\",             f\"--resolution={config.resolution}\",             \"--center_crop\",             \"--random_flip\",             f\"--gradient_accumulation_steps={config.gradient_accumulation_steps}\",             \"--gradient_checkpointing\",             f\"--train_batch_size={config.train_batch_size}\",             f\"--learning_rate={config.learning_rate}\",             f\"--lr_scheduler={config.lr_scheduler}\",             f\"--max_train_steps={config.max_train_steps}\",             f\"--lr_warmup_steps={config.lr_warmup_steps}\",             f\"--checkpointing_steps={config.checkpointing_steps}\",         ]          main()      # run training -- see huggingface accelerate docs for details     print(\"launching fine-tuning training script\")      notebook_launcher(launch_training, num_processes=1)     # The trained model artefacts have been output to the volume mounted at `MODEL_DIR`.     # To persist these artefacts for use in future inference function calls, we 'commit' the changes     # to the volume.     stub.model_volume.commit()   @stub.local_entrypoint() def run():     train.remote() Copy Run training function  To run this training function:  modal run train_and_serve_diffusers_script.py Copy Set up inference function  Depending on which Diffusers training script you are using, you may need to use an alternative pipeline to StableDiffusionPipeline. The READMEs of the example training scripts will generally provide instructions for which inference pipeline to use. For example, if you are fine-tuning Kandinsky, it tells you to use AutoPipelineForText2Image instead of StableDiffusionPipeline.  @stub.cls(     image=image,     gpu=\"A100\",     volumes=VOLUME_CONFIG,  # mount the location where your model weights were saved to ) class Model:     def __enter__(self):         import torch         from diffusers import DDIMScheduler, StableDiffusionPipeline          # Reload the modal.Volume to ensure the latest state is accessible.         stub.volume.reload()          # set up a hugging face inference pipeline using our model         ddim = DDIMScheduler.from_pretrained(MODEL_DIR, subfolder=\"scheduler\")         # potentially use different pipeline         pipe = StableDiffusionPipeline.from_pretrained(             MODEL_DIR,             scheduler=ddim,             torch_dtype=torch.float16,             safety_checker=None,         ).to(\"cuda\")         pipe.enable_xformers_memory_efficient_attention()         self.pipe = pipe      @method()     def inference(self, text, config):         image = self.pipe(             text,             num_inference_steps=config.num_inference_steps,             guidance_scale=config.guidance_scale,         ).images[0]          return image Copy Set up Gradio app  Finally, we set up a Gradio app that will allow you to interact with your model. This will be mounted to the Modal container, and will be accessible at the URL of your Modal deployment. You can refer to the Gradio docs for more information on how to customize the app.  @stub.function(     image=image,     concurrency_limit=3, ) @asgi_app() def fastapi_app():     import gradio as gr     from gradio.routes import mount_gradio_app      # Call to the GPU inference function on Modal.     def go(text):         return Model().inference.remote(text, config)      # set up AppConfig     config = AppConfig()      HCON_prefix = \"In the HCON style, an icon of\"      example_prompts = [         f\"{HCON_prefix} a movie ticket\",         f\"{HCON_prefix} barack obama\",         f\"{HCON_prefix} a castle\",         f\"{HCON_prefix} a german shepherd\",     ]      modal_docs_url = \"https://modal.com/docs/guide\"     modal_example_url = f\"{modal_docs_url}/ex/diffusers\"      description = f\"\"\"Describe a concept that you would like drawn as a Heroicon icon. Try the examples below for inspiration.  ### Learn how to make your own [here]({modal_example_url}).     \"\"\"      # add a gradio UI around inference     interface = gr.Interface(         fn=go,         inputs=\"text\",         outputs=gr.Image(shape=(512, 512)),         title=\"Generate custom heroicons\",         examples=example_prompts,         description=description,         css=\"/assets/index.css\",         allow_flagging=\"never\",     )      # mount for execution on Modal     return mount_gradio_app(         app=web_app,         blocks=interface,         path=\"/\",     ) Copy Run Gradio app  Finally, we run the Gradio app. This will launch the Gradio app on Modal.  modal serve train_and_serve_diffusers_script.py Copy Running Diffusers example scripts on Modal Select training script Fine-tuning on Heroicons Fine-tuning results Creating the dataset Set up the dependencies for fine-tuning on Modal Set up Volumes for training data and model output Set up config Set up finetuning dataset Upload to HuggingFace Hub Push dataset to Volume Set up stub.function decorator on the training function. Define the training function Run training function Set up inference function Set up Gradio app Run Gradio app Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/diffusers/train_and_serve_diffusers_script.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/webcam","title":"Machine learning model inference endpoint that uses the webcam | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Machine learning model inference endpoint that uses the webcam  This example creates a web endpoint that uses a Huggingface model for object detection.  The web endpoint takes an image from their webcam, and sends it to a Modal web endpoint. The Modal web endpoint in turn calls a Modal function that runs the actual model.  If you run this, it will look something like this:  Live demo  Take a look at the deployed app.  A couple of caveats:  This is not optimized for latency: every prediction takes about 1s, and there’s an additional overhead on the first prediction since the containers have to be started and the model initialized. This doesn’t work on iPhone unfortunately due to some issues with HTML5 webcam components Code  Starting with imports:  import base64 import io from pathlib import Path  from fastapi import FastAPI, Request, Response from fastapi.staticfiles import StaticFiles from modal import Image, Mount, Stub, asgi_app, build, method Copy  We need to install transformers which is a package Huggingface uses for all their models, but also Pillow which lets us work with images from Python, and a system font for drawing.  This example uses the facebook/detr-resnet-50 pre-trained model, which is downloaded once at image build time using the @build hook and saved into the image. ‘Baking’ models into the modal.Image at build time provided the fastest cold start.  model_repo_id = \"facebook/detr-resnet-50\"   stub = Stub(\"example-webcam-object-detection\") image = (     Image.debian_slim()     .pip_install(         \"huggingface-hub==0.16.4\",         \"Pillow\",         \"timm\",         \"transformers\",     )     .apt_install(\"fonts-freefont-ttf\") ) Copy Prediction function  The object detection function has a few different features worth mentioning:  There’s a container initialization step in the __enter__ method, which runs on every container start. This lets us load the model only once per container, so that it’s reused for subsequent function calls. Above we stored the model in the container image. This lets us download the model only when the image is (re)built, and not everytime the function is called. We’re running it on multiple CPUs for extra performance  Note that the function takes an image and returns a new image. The input image is from the webcam The output image is an image with all the bounding boxes and labels on them, with an alpha channel so that most of the image is transparent so that the web interface can render it on top of the webcam view.  with image.imports():     import torch     from huggingface_hub import snapshot_download     from PIL import Image, ImageColor, ImageDraw, ImageFont     from transformers import DetrForObjectDetection, DetrImageProcessor   @stub.cls(     cpu=4,     image=image, ) class ObjectDetection:     @build()     def download_model(self):         snapshot_download(repo_id=model_repo_id, cache_dir=\"/cache\")      def __enter__(self):         self.feature_extractor = DetrImageProcessor.from_pretrained(             model_repo_id,             cache_dir=\"/cache\",         )         self.model = DetrForObjectDetection.from_pretrained(             model_repo_id,             cache_dir=\"/cache\",         )      @method()     def detect(self, img_data_in):         # Based on https://huggingface.co/spaces/nateraw/detr-object-detection/blob/main/app.py         # Read png from input         image = Image.open(io.BytesIO(img_data_in)).convert(\"RGB\")          # Make prediction         inputs = self.feature_extractor(image, return_tensors=\"pt\")         outputs = self.model(**inputs)         img_size = torch.tensor([tuple(reversed(image.size))])         processed_outputs = (             self.feature_extractor.post_process_object_detection(                 outputs=outputs,                 target_sizes=img_size,                 threshold=0,             )         )         output_dict = processed_outputs[0]          # Grab boxes         keep = output_dict[\"scores\"] > 0.7         boxes = output_dict[\"boxes\"][keep].tolist()         scores = output_dict[\"scores\"][keep].tolist()         labels = output_dict[\"labels\"][keep].tolist()          # Plot bounding boxes         colors = list(ImageColor.colormap.values())         font = ImageFont.truetype(             \"/usr/share/fonts/truetype/freefont/FreeMono.ttf\", 18         )         output_image = Image.new(\"RGBA\", (image.width, image.height))         output_image_draw = ImageDraw.Draw(output_image)         for _score, box, label in zip(scores, boxes, labels):             color = colors[label % len(colors)]             text = self.model.config.id2label[label]             box = tuple(map(int, box))             output_image_draw.rectangle(box, outline=color)             output_image_draw.text(                 box[:2], text, font=font, fill=color, width=3             )          # Return PNG as bytes         with io.BytesIO() as output_buf:             output_image.save(output_buf, format=\"PNG\")             return output_buf.getvalue() Copy Defining the web interface  To keep things clean, we define the web endpoints separate from the prediction function. This will introduce a tiny bit of extra latency (every web request triggers a Modal function call which will call another Modal function) but in practice the overhead is much smaller than the overhead of running the prediction function etc.  We also serve a static html page which contains some tiny bit of Javascript to capture the webcam feed and send it to Modal.  web_app = FastAPI() static_path = Path(__file__).with_name(\"webcam\").resolve() Copy  The endpoint for the prediction function takes an image as a data URI and returns another image, also as a data URI:  @web_app.post(\"/predict\") async def predict(request: Request):     # Takes a webcam image as a datauri, returns a bounding box image as a datauri     body = await request.body()     img_data_in = base64.b64decode(body.split(b\",\")[1])  # read data-uri     img_data_out = ObjectDetection().detect.remote(img_data_in)     output_data = b\"data:image/png;base64,\" + base64.b64encode(img_data_out)     return Response(content=output_data) Copy Exposing the web server  Let’s take the Fast API app and expose it to Modal.  @stub.function(     mounts=[Mount.from_local_dir(static_path, remote_path=\"/assets\")], ) @asgi_app() def fastapi_app():     web_app.mount(\"/\", StaticFiles(directory=\"/assets\", html=True))     return web_app Copy Running this locally  You can run this as an ephemeral app, by running  modal serve webcam.py Copy Machine learning model inference endpoint that uses the webcam Live demo Code Prediction function Defining the web interface Exposing the web server Running this locally Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal serve 06_gpu_and_ml/obj_detection_webcam/webcam.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/serve_streamlit","title":"Run and share Streamlit apps | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Run and share Streamlit apps  This example shows you how to run a Streamlit app with modal serve, and then deploy it as a serverless web app.  This example is structured as two files:  This module, which defines the Modal objects (name the script serve_streamlit.py locally). app.py, which is any Streamlit script to be mounted into the Modal function (download script). import pathlib  import modal Copy Define container dependencies  The app.py script imports three third-party packages, so we include these in the example’s image definition. We also install asgiproxy to proxy the Streamlit server.  image = (     modal.Image.debian_slim()     .apt_install(\"git\")     .pip_install(\"streamlit\", \"numpy\", \"pandas\")     # Use fork until https://github.com/valohai/asgiproxy/pull/11 is merged.     .pip_install(\"git+https://github.com/modal-labs/asgiproxy.git\") )  stub = modal.Stub(name=\"example-modal-streamlit\", image=image) Copy Mounting the app.py script  We can just mount the app.py script inside the container at a pre-defined path using a Modal Mount.  streamlit_script_local_path = pathlib.Path(__file__).parent / \"app.py\" streamlit_script_remote_path = pathlib.Path(\"/root/app.py\")  if not streamlit_script_local_path.exists():     raise RuntimeError(         \"app.py not found! Place the script with your streamlit app in the same directory.\"     )  streamlit_script_mount = modal.Mount.from_local_file(     streamlit_script_local_path,     streamlit_script_remote_path, ) Copy Spawning the Streamlit server  Inside the container, we will run the Streamlit server in a background subprocess using subprocess.Popen. Here we define spawn_server() to do this and then poll until the server is ready to accept connections.  HOST = \"127.0.0.1\" PORT = \"8000\"   def spawn_server():     import socket     import subprocess      process = subprocess.Popen(         [             \"streamlit\",             \"run\",             str(streamlit_script_remote_path),             \"--browser.serverAddress\",             HOST,             \"--server.port\",             PORT,             \"--browser.serverPort\",             PORT,             \"--server.enableCORS\",             \"false\",         ]     )      # Poll until webserver accepts connections before running inputs.     while True:         try:             socket.create_connection((HOST, int(PORT)), timeout=1).close()             print(\"Webserver ready!\")             return process         except (socket.timeout, ConnectionRefusedError):             # Check if launcher webserving process has exited.             # If so, a connection can never be made.             retcode = process.poll()             if retcode is not None:                 raise RuntimeError(                     f\"launcher exited unexpectedly with code {retcode}\"                 ) Copy Wrap it in an ASGI app  Finally, Modal can only serve apps that speak the ASGI or WSGI protocols. Since the Streamlit server is neither, we run a separate ASGI app that proxies requests to the Streamlit server using the asgiproxy package. Note that at this point asgiproxy has a bug with websocket handling, so we are using a fork with the fix for this.  @stub.function(     # Allows 100 concurrent requests per container.     allow_concurrent_inputs=100,     mounts=[streamlit_script_mount], ) @modal.asgi_app() def run():     from asgiproxy.config import BaseURLProxyConfigMixin, ProxyConfig     from asgiproxy.context import ProxyContext     from asgiproxy.simple_proxy import make_simple_proxy_app      spawn_server()      config = type(         \"Config\",         (BaseURLProxyConfigMixin, ProxyConfig),         {             \"upstream_base_url\": f\"http://{HOST}:{PORT}\",             \"rewrite_host_header\": f\"{HOST}:{PORT}\",         },     )()     proxy_context = ProxyContext(config)     return make_simple_proxy_app(proxy_context) Copy Iterate and Deploy  While you’re iterating on your screamlit app, you can run it “ephemerally” with modal serve. This will run a local process that watches your files and updates the app if anything changes.  modal serve serve_streamlit.py Copy  Once you’re happy with your changes, you can deploy your application with  modal deploy serve_streamlit.py Copy  If successful, this will print a URL for your app, that you can navigate to from your browser 🎉 .  Run and share Streamlit apps Define container dependencies Mounting the app.py script Spawning the Streamlit server Wrap it in an ASGI app Iterate and Deploy Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 10_integrations/streamlit/serve_streamlit.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/llm-voice-chat","title":"QuilLLMan: Voice Chat with LLMs | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler QuilLLMan: Voice Chat with LLMs  Vicuna is the latest in a series of open-source chatbots that approach the quality of proprietary models like GPT-4, but in addition can be self-hosted at a fraction of the cost. We’ve enjoyed playing around with Vicuna enough at Modal HQ that we decided we wanted to have it available at all times in the form of a voice chat app.  So, we built QuiLLMan, a complete chat app that transcribes audio in real-time using Whisper, streams back a response from a language model, and synthesizes this response as natural-sounding speech.  Everything (including the React frontend and backend API) is deployed serverlessly on Modal, and you can play around with the live demo here. This post provides a high-level walkthrough of the repo. We’re looking to add more models and features to this as time goes on, and contributions are welcome!  Code overview  Traditionally, building a serverless web application with a backend API and three different ML services, each of these running in its own custom container and autoscaling independently, would require a lot of work. But with Modal, it’s as simple as writing 4 different classes and running a CLI command.  Our project structure looks like this:  Language model service Transcription service Text-to-speech service FastAPI server React frontend  Let’s go through each of these components in more detail.  Language model  We use GPTQ-for-LLaMa, an implementation of GPTQ, to quantize our model to 4 bits for faster inference. This repository needs to be built from scratch against CUDA runtime. Fortunately, Modal makes it easy to express a complex image definition like this one as a series of functions:  vicuna_image = (     modal.Image.from_registry(\"nvidia/cuda:12.2.0-devel-ubuntu20.04\", add_python=\"3.8\")     .apt_install(\"git\", \"gcc\", \"build-essential\")     .run_commands(         \"git clone https://github.com/thisserand/FastChat.git\",         \"cd FastChat && pip install -e .\",     )     .run_commands(         # FastChat hard-codes a path for GPTQ, so this needs to be cloned inside repositories.         \"git clone https://github.com/oobabooga/GPTQ-for-LLaMa.git -b cuda /FastChat/repositories/GPTQ-for-LLaMa\",         \"cd /FastChat/repositories/GPTQ-for-LLaMa && python setup_cuda.py install\",         gpu=\"any\",     ) ) Copy  Above, we use from_registry to select the official CUDA container as the base image, install python and build requirements, clone the FastChat repo, and finally build GPTQ-for-LLaMa. Note that in order for the compilation step to work, a CUDA environment is required; adding gpu=\"any\" lets us run that step on a GPU machine.  The generate function itself constructs a prompt using the current input, previous history and a prompt template. Then, it simply yields tokens as they are produced. Python generators just work out-of-the-box in Modal, so building streaming interactions is easy.  Although we’re going to call this model from our backend API, it’s useful to test it directly as well. To do this, we define a local_entrypoint:  @stub.local_entrypoint() def main(input: str):     model = Vicuna()     for val in model.generate.remote(input):         print(val, end=\"\", flush=True) Copy  Now, we can run the model with a prompt of our choice from the terminal:  modal run -q src.llm_vicuna --input \"How do antihistamines work?\" Copy Transcription  In this file we define a Modal class that uses OpenAI’s Whisper to transcribe audio in real-time. The helper function load_audio downsamples the audio to 16kHz (required by Whisper) using ffmpeg.  We’re using an A10G GPU for transcriptions, which lets us transcribe most segments in under 2 seconds.  Text-to-speech  The text-to-speech service is adapted from tortoise-tts-modal-api, a Modal deployment of Tortoise TTS. Take a look at those repos if you’re interested in understanding how the code works, or for a full list of parameters and voices you can use.  FastAPI server  Our backend is a FastAPI app. Modal provides an @asgi_app decorator that lets us serve this app on the internet without any extra effort.  Of the 4 endpoints in the file, POST /generate is the most interesting. It calls Vicuna.generate and streams the text results back. When a sentence is completed, it also calls Tortoise.speak asynchronously to generate audio, and return a handle to the function call. This handle can be used to poll for the audio later (take a look at our job queue example for an explanation of this pattern). If Tortoise is not enabled, we return the sentence directly so that the frontend can use the browser’s built-in text-to-speech.  In order to send these different types of messages over the same stream, each is sent as a serialized JSON consisting of a type and payload. The ASCII record separator character (\\x1e) is used to delimit the messages, since it cannot appear in JSON.      def gen_serialized():         for i in gen():             yield json.dumps(i) + \"\\x1e\"      return StreamingResponse(         gen_serialized(),         media_type=\"text/event-stream\",     ) Copy  In addition, the function checks if the body contains a noop flag. This is used to warm the containers when the user first loads the page, so that the models can be loaded into memory ahead of time.  The other endpoints are more straightforward:  POST /transcribe: Calls Whisper.transcribe and returns the results directly. GET /audio/{call_id}: Polls to check if a Tortoise.speak call ID generated above has completed. If yes, it returns the audio data. If not, it returns a 202 status code to indicate that the request should be retried again. DELETE /audio/{call_id}: Cancels a Tortoise.speak call ID generated above. Useful if we want to stop generating audio for a given user. React frontend  We use the Web Audio API to record snippets of audio from the user’s microphone. The file src/frontend/processor.js defines an AudioWorkletProcessor that distinguishes between speech and silence, and emits events for speech segments so we can transcribe them.  Pending text-to-speech syntheses are stored in a queue. For the next item in the queue, we use the GET /audio/{call_id} endpoint to poll for the audio data.  Finally, the frontend maintains a state machine to manage the state of the conversation and transcription progress. This is implemented with the help of the incredible XState library.  const chatMachine = createMachine(   {     initial: \"botDone\",     states: {       botGenerating: {         on: {           GENERATION_DONE: { target: \"botDone\", actions: \"resetTranscript\" },         },       },       botDone: { ... },       userTalking: {  ... },       userSilent: { ... },     },     ...   }, Copy Steal this example  The code for this entire example is available on GitHub. Follow the instructions in the README for how to run or deploy it yourself on Modal.  QuilLLMan: Voice Chat with LLMs Code overview Language model Transcription Text-to-speech FastAPI server React frontend Steal this example © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/db_to_sheet","title":"Write to Google Sheets | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Write to Google Sheets  In this tutorial, we’ll show how to use Modal to schedule a daily update of a dataset from an analytics database to Google Sheets.  Entering credentials  We begin by setting up some credentials that we’ll need in order to access our database and output spreadsheet. To do that in a secure manner, we log in to our Modal account on the web and go to the Secrets section.  Database  First we will enter our database credentials. The easiest way to do this is to click New secret and select the Postgres compatible secret preset and fill in the requested information. Then we press Next and name our secret “analytics-database” and click Create.  Google Sheets/GCP  We’ll now add another Secret for Google Sheets access through Google Cloud Platform. Click New secret and select the Google Sheets preset.  In order to access the Google Sheets API, we’ll need to create a Service Account in Google Cloud Platform. You can skip this step if you already have a Service Account json file.  Sign up to Google Cloud Platform or log in if you haven’t (https://cloud.google.com/). Go to https://console.cloud.google.com/. In the navigation pane on the left, go to IAM & Admin > Service Accounts. Click the + CREATE SERVICE ACCOUNT button. Give the service account a suitable name, like “sheet-access-bot”. Click Done. You don’t have to grant it any specific access privileges at this time. Click your new service account in the list view that appears and navigate to the Keys section. Click Add key and choose Create new key. Use the JSON key type and confirm by clicking Create. A json key file should be downloaded to your computer at this point. Copy the contents of that file and use it as the value for the SERVICE_ACCOUNT_JSON field in your new secret.  We’ll name this other secret “gsheets”.  Now you can access the values of your secrets from modal functions that you annotate with the corresponding EnvDict includes, e.g.:  import os  import modal  stub = modal.Stub(\"example-db-to-sheet\")   @stub.function(secret=modal.Secret.from_name(\"postgres-secret\")) def my_func():     # automatically filled from the specified secret     print(\"Host is \" + os.environ[\"PGHOST\"]) Copy  In order to connect to the database, we’ll use the psycopg2 Python package. To make it available to your Modal function you need to supply it with an image argument that tells Modal how to build the container image that contains that package. We’ll base it off of the Image.debian_slim base image that’s built into modal, and make sure to install the required binary packages as well as the psycopg2 package itself:  pg_image = (     modal.Image.debian_slim().apt_install(\"libpq-dev\").pip_install(\"psycopg2\") ) Copy  Since the default keynames for a Postgres compatible secret correspond to the environment variables that psycopg2 looks for, you can now easily connect to the database even without explicit credentials in your code. We’ll create a simple function that queries the city for each user in our dummy users table:  @stub.function(     image=pg_image,     secret=modal.Secret.from_name(\"postgres-secret\"), ) def get_db_rows():     import psycopg2      conn = psycopg2.connect()  # no explicit credentials needed     cur = conn.cursor()     cur.execute(\"SELECT city FROM users\")     return [row[0] for row in cur.fetchall()] Copy  Note that we import psycopg2 inside our function instead of the global scope. This allows us to run this Modal function even from an environment where psycopg2 is not installed. We can test run this function using the modal run shell command: modal run db_to_sheet.py::stub.get_db_rows.  Applying Python logic  For each city in our source data we’ll make an online lookup of the current weather using the http://openweathermap.org API. To do this, we’ll add the API key to another modal secret. We’ll use a custom secret called “weather” with the key OPENWEATHER_API_KEY containing our API key for OpenWeatherMap.  requests_image = modal.Image.debian_slim().pip_install(\"requests\")   @stub.function(     image=requests_image,     secret=modal.Secret.from_name(\"weather-secret\"), ) def city_weather(city):     import requests      url = \"https://api.openweathermap.org/data/2.5/weather\"     params = {\"q\": city, \"appid\": os.environ[\"OPENWEATHER_API_KEY\"]}     response = requests.get(url, params=params)     weather_label = response.json()[\"weather\"][0][\"main\"]     return weather_label Copy  We’ll make use of Modal’s built-in function.map method to create our report. function.map makes it really easy to parallelise work by executing a function for a larger sequence of input data. For this example we’ll make a simple count of rows per weather type, using Python’s standard library collections.Counter.  from collections import Counter   @stub.function() def create_report(cities):     # run city_weather for each city in parallel     user_weather = city_weather.map(cities)     users_by_weather = Counter(user_weather).items()     return users_by_weather Copy  Let’s try to run this! To make it simple to trigger the function with some predefined input data, we create a “local entrypoint” main that can be easily triggered from the command line:  @stub.local_entrypoint() def main():     cities = [         \"Stockholm,,Sweden\",         \"New York,NY,USA\",         \"Tokyo,,Japan\",     ]     print(create_report.remote(cities)) Copy  Running the local entrypoint using modal run db_to_sheet.py should print something like: dict_items([('Clouds', 3)]). Note that since this file only has a single stub, and the stub has only one local entrypoint we only have to specify the file to run - the function/entrypoint is inferred.  In this case the logic is quite simple, but in a real world context you could have applied a machine learning model or any other tool you could build into a container to transform the data.  Sending output to a Google Sheet  We’ll set up a new Google Sheet to send our report to. Using the “Sharing” dialog in Google Sheets, we make sure to share the document to the service account’s email address (the value of the client_email field in the json file) and make the service account an editor of the document.  The URL of a Google Sheet is something like: https://docs.google.com/spreadsheets/d/1wOktal......IJR77jD8Do.  We copy the part of the URL that comes after /d/ - that is the key of the document which we’ll refer to in our code. We’ll make use of the pygsheets python package to authenticate with Google Sheets and then update the spreadsheet with information from the report we just created:  pygsheets_image = modal.Image.debian_slim().pip_install(\"pygsheets\")   @stub.function(     image=pygsheets_image,     secret=modal.Secret.from_name(\"gsheets-secret\"), ) def update_sheet_report(rows):     import pygsheets      gc = pygsheets.authorize(service_account_env_var=\"SERVICE_ACCOUNT_JSON\")     document_key = \"1RqQrJ6Ikf611adKunm8tmL1mKzHLjNwLWm_T7mfXSYA\"     sh = gc.open_by_key(document_key)     worksheet = sh.sheet1     worksheet.clear(\"A2\")      worksheet.update_values(\"A2\", [list(row) for row in rows]) Copy  At this point, we have everything we need in order to run the full program. We can put it all together in another Modal function, and add a schedule argument so it runs every day automatically:  @stub.function(schedule=modal.Cron(\"0 0 * * *\")) def db_to_sheet():     rows = get_db_rows.remote()     report = create_report.remote(rows)     update_sheet_report.remote(report)     print(\"Updated sheet with new weather distribution\")     for weather, count in report:         print(f\"{weather}: {count}\") Copy  This entire stub can now be deployed using modal deploy db_to_sheet.py. The apps page shows our cron job’s execution history and lets you navigate to each invocation’s logs. To trigger a manual run from your local code during development, you can also trigger this function using the cli: modal run db_to_sheet.py::stub.db_to_sheet  Note that all of the @stub.function() annotated functions above run remotely in isolated containers that are specified per function, but they are called as seamlessly as using regular Python functions. This is a simple showcase of how you can mix and match functions that use different environments and have them feed into each other or even call each other as if they were all functions in the same local program.  Write to Google Sheets Entering credentials Database Google Sheets/GCP Applying Python logic Sending output to a Google Sheet Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 04_secrets/db_to_sheet.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/doc_ocr_webapp","title":"Document OCR web app | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Document OCR web app  This tutorial shows you how to use Modal to deploy a fully serverless React + FastAPI application. We’re going to build a simple “Receipt Parser” web app that submits OCR transcription tasks to a separate Modal app defined in the Job Queue tutorial, polls until the task is completed, and displays the results. Try it out for yourself here.  Basic setup  Let’s get the imports out of the way and define a Stub.  from pathlib import Path  import fastapi import fastapi.staticfiles from modal import Function, Mount, Stub, asgi_app  stub = Stub(\"example-doc-ocr-webapp\") Copy  Modal works with any ASGI or WSGI web framework. Here, we choose to use FastAPI.  web_app = fastapi.FastAPI() Copy Define endpoints  We need two endpoints: one to accept an image and submit it to the Modal job queue, and another to poll for the results of the job.  In parse, we’re going to submit tasks to the function defined in the Job Queue tutorial, so we import it first using Function.lookup.  We call .spawn() on the function handle we imported above, to kick off our function without blocking on the results. spawn returns a unique ID for the function call, that we can use later to poll for its result.  @web_app.post(\"/parse\") async def parse(request: fastapi.Request):     parse_receipt = Function.lookup(\"example-doc-ocr-jobs\", \"parse_receipt\")      form = await request.form()     receipt = await form[\"receipt\"].read()  # type: ignore     call = parse_receipt.spawn(receipt)     return {\"call_id\": call.object_id} Copy  /result uses the provided call_id to instantiate a modal.FunctionCall object, and attempt to get its result. If the call hasn’t finished yet, we return a 202 status code, which indicates that the server is still working on the job.  @web_app.get(\"/result/{call_id}\") async def poll_results(call_id: str):     from modal.functions import FunctionCall      function_call = FunctionCall.from_id(call_id)     try:         result = function_call.get(timeout=0)     except TimeoutError:         return fastapi.responses.JSONResponse(content=\"\", status_code=202)      return result Copy  Finally, we mount the static files for our front-end. We’ve made a simple React app that hits the two endpoints defined above. To package these files with our app, first we get the local assets path, and then create a modal Mount that mounts this directory at /assets inside our container. Then, we instruct FastAPI to serve this static file directory at our root path.  assets_path = Path(__file__).parent / \"doc_ocr_frontend\"   @stub.function(     mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")] ) @asgi_app() def wrapper():     web_app.mount(         \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)     )      return web_app Copy Running  You can run this as an ephemeral app, by running the command  modal serve doc_ocr_webapp.py Copy Deploy  That’s all! To deploy your application, run  modal deploy doc_ocr_webapp.py Copy  If successful, this will print a URL for your app, that you can navigate to from your browser 🎉 .  Developing  If desired, instead of deploying, we can serve our app ephemerally. In this case, Modal watches all the mounted files, and updates the app if anything changes.  Document OCR web app Basic setup Define endpoints Running Deploy Developing Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 09_job_queues/doc_ocr_webapp.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/hello_world","title":"Hello, world! | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Hello, world!  This is a trivial example of a Modal function, but it illustrates a few features:  You can print things to stdout and stderr. You can return data. You can map over a function. Import Modal and define the app  Let’s start with the top level imports. You need to import Modal and define the app. A stub is an object that defines everything that will be run.  import sys  import modal  stub = modal.Stub(\"example-hello-world\") Copy Defining a function  Here we define a Modal function using the modal.function decorator. The body of the function will automatically be run remotely. This particular function is pretty silly: it just prints “hello” and “world” alternatingly to standard out and standard error.  @stub.function() def f(i):     if i % 2 == 0:         print(\"hello\", i)     else:         print(\"world\", i, file=sys.stderr)      return i * i Copy Running it  Finally, let’s actually invoke it. We put this invocation code inside a @stub.local_entrypoint(). This is because this module will be imported in the cloud, and we don’t want this code to be executed a second time in the cloud.  Run modal run hello_world.py and the @stub.local_entrypoint() decorator will handle starting the Modal app and then executing the wrapped function body.  Inside the main() function body, we are calling the function f in three ways:  1 As a simple local call, f(1000) 2. As a simple remote call f.remote(1000) 3. By mapping over the integers 0..19  @stub.local_entrypoint() def main():     # Call the function locally.     print(f.local(1000))      # Call the function remotely.     print(f.remote(1000))      # Parallel map.     total = 0     for ret in f.map(range(20)):         total += ret      print(total) Copy What happens?  When you do .remote on function f, Modal will execute f in the cloud, not locally on your computer. It will take the code, put it inside a container, run it, and stream all the output back to your local computer.  Try doing one of these things next.  Change the code and run again  For instance, change the print statement in the function f. You can see that the latest code is always run.  Modal’s goal is to make running code in the cloud feel like you’re running code locally. You don’t need to run any commands to rebuild, push containers, or go to a web UI to download logs.  Map over a larger dataset  Change the map range from 20 to some large number. You can see that Modal will create and run more containers in parallel.  The function f is obviously silly and doesn’t do much, but you could imagine something more significant, like:  Training a machine learning model Transcoding media Backtesting a trading algorithm.  Modal lets you parallelize that operation trivially by running hundreds or thousands of containers in the cloud.  Hello, world! Import Modal and define the app Defining a function Running it What happens? Change the code and run again Map over a larger dataset Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 01_getting_started/hello_world.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/dreambooth_app","title":"Pet Art Dreambooth with Hugging Face and Gradio | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Pet Art Dreambooth with Hugging Face and Gradio  This example finetunes the Stable Diffusion v1.5 model on images of a pet (by default, a puppy named Qwerty) using a technique called textual inversion from the “Dreambooth” paper. Effectively, it teaches a general image generation model a new “proper noun”, allowing for the personalized generation of art and photos. It then makes the model shareable with others using the Gradio.app web interface framework.  It demonstrates a simple, productive, and cost-effective pathway to building on large pretrained models by using Modal’s building blocks, like GPU-accelerated Modal Functions, volumes for caching, and Modal webhooks.  And with some light customization, you can use it to generate images of your pet!  Setting up the dependencies  We can start from a base image and specify all of our dependencies.  import os from dataclasses import dataclass from pathlib import Path  from fastapi import FastAPI from modal import (     Image,     Mount,     Stub,     Volume,     asgi_app,     method, )  web_app = FastAPI() assets_path = Path(__file__).parent / \"assets\" stub = Stub(name=\"example-dreambooth-app\") Copy  Commit in diffusers to checkout train_dreambooth.py from.  GIT_SHA = \"ed616bd8a8740927770eebe017aedb6204c6105f\"  image = (     Image.debian_slim(python_version=\"3.10\")     .pip_install(         \"accelerate==0.19\",         \"datasets~=2.13\",         \"ftfy~=6.1\",         \"gradio~=3.10\",         \"smart_open~=6.4\",         \"transformers==4.28\",         \"torch~=2.1\",         \"torchvision~=0.16\",         \"triton~=2.1\",     )     .pip_install(\"xformers\", pre=True)     .apt_install(\"git\")     # Perform a shallow fetch of just the target `diffusers` commit, checking out     # the commit in the container's current working directory, /root. Then install     # the `diffusers` package.     .run_commands(         \"cd /root && git init .\",         \"cd /root && git remote add origin https://github.com/huggingface/diffusers\",         f\"cd /root && git fetch --depth=1 origin {GIT_SHA} && git checkout {GIT_SHA}\",         \"cd /root && pip install -e .\",     ) ) Copy  A persisted modal.Volume will store model artefacts across Modal app runs. This is crucial as finetuning runs are separate from the Gradio app we run as a webhook.  volume = Volume.persisted(\"dreambooth-finetuning-volume\") MODEL_DIR = \"/model\" Copy Config  All configs get their own dataclasses to avoid scattering special/magic values throughout code. You can read more about how the values in TrainConfig are chosen and adjusted in this blog post on Hugging Face. To run training on images of your own pet, upload the images to separate URLs and edit the contents of the file at TrainConfig.instance_example_urls_file to point to them.  @dataclass class SharedConfig:     \"\"\"Configuration information shared across project components.\"\"\"      # The instance name is the \"proper noun\" we're teaching the model     instance_name: str = \"Qwerty\"     # That proper noun is usually a member of some class (person, bird),     # and sharing that information with the model helps it generalize better.     class_name: str = \"Golden Retriever\"   @dataclass class TrainConfig(SharedConfig):     \"\"\"Configuration for the finetuning step.\"\"\"      # training prompt looks like `{PREFIX} {INSTANCE_NAME} the {CLASS_NAME} {POSTFIX}`     prefix: str = \"a photo of\"     postfix: str = \"\"      # locator for plaintext file with urls for images of target instance     instance_example_urls_file: str = str(         Path(__file__).parent / \"instance_example_urls.txt\"     )      # identifier for pretrained model on Hugging Face     model_name: str = \"runwayml/stable-diffusion-v1-5\"      # Hyperparameters/constants from the huggingface training example     resolution: int = 512     train_batch_size: int = 1     gradient_accumulation_steps: int = 1     learning_rate: float = 2e-6     lr_scheduler: str = \"constant\"     lr_warmup_steps: int = 0     max_train_steps: int = 600     checkpointing_steps: int = 1000   @dataclass class AppConfig(SharedConfig):     \"\"\"Configuration information for inference.\"\"\"      num_inference_steps: int = 50     guidance_scale: float = 7.5 Copy Get finetuning dataset  Part of the magic of Dreambooth is that we only need 4-10 images for finetuning. So we can fetch just a few images, stored on consumer platforms like Imgur or Google Drive — no need for expensive data collection or data engineering.  def load_images(image_urls: list[str]) -> Path:     import PIL.Image     from smart_open import open      img_path = Path(\"/img\")      img_path.mkdir(parents=True, exist_ok=True)     for ii, url in enumerate(image_urls):         with open(url, \"rb\") as f:             image = PIL.Image.open(f)             image.save(img_path / f\"{ii}.png\")     print(\"Images loaded.\")      return img_path Copy Finetuning a text-to-image model  This model is trained to do a sort of “reverse ekphrasis”: it attempts to recreate a visual work of art or image from only its description.  We can use a trained model to synthesize wholly new images by combining the concepts it has learned from the training data.  We use a pretrained model, version 1.5 of the Stable Diffusion model. In this example, we “finetune” SD v1.5, making only small adjustments to the weights, in order to just teach it a new word: the name of our pet.  The result is a model that can generate novel images of our pet: as an astronaut in space, as painted by Van Gogh or Bastiat, etc.  Finetuning with Hugging Face 🧨 Diffusers and Accelerate  The model weights, libraries, and training script are all provided by 🤗 Hugging Face.  You can kick off a training job with the command modal run dreambooth_app.py::stub.train. It should take about ten minutes.  Tip: if the results you’re seeing don’t match the prompt too well, and instead produce an image of your subject again, the model has likely overfit. In this case, repeat training with a lower value of max_train_steps. On the other hand, if the results don’t look like your subject, you might need to increase max_train_steps.  @stub.function(     image=image,     gpu=\"A100\",  # fine-tuning is VRAM-heavy and requires an A100 GPU     volumes={MODEL_DIR: volume},  # stores fine-tuned model     timeout=1800,  # 30 minutes ) def train(instance_example_urls):     import subprocess      from accelerate.utils import write_basic_config     from transformers import CLIPTokenizer      # set up TrainConfig     config = TrainConfig()      # set up runner-local image and shared model weight directories     img_path = load_images(instance_example_urls)     os.makedirs(MODEL_DIR, exist_ok=True)      # set up hugging face accelerate library for fast training     write_basic_config(mixed_precision=\"fp16\")      # check whether we can access to model repo     try:         CLIPTokenizer.from_pretrained(config.model_name, subfolder=\"tokenizer\")     except OSError as e:  # handle error raised when license is not accepted         license_error_msg = f\"Unable to load tokenizer. Access to this model requires acceptance of the license on Hugging Face here: https://huggingface.co/{config.model_name}.\"         raise Exception(license_error_msg) from e      # define the training prompt     instance_phrase = f\"{config.instance_name} {config.class_name}\"     prompt = f\"{config.prefix} {instance_phrase} {config.postfix}\".strip()      def _exec_subprocess(cmd: list[str]):         \"\"\"Executes subprocess and prints log to terminal while subprocess is running.\"\"\"         process = subprocess.Popen(             cmd,             stdout=subprocess.PIPE,             stderr=subprocess.STDOUT,         )         with process.stdout as pipe:             for line in iter(pipe.readline, b\"\"):                 line_str = line.decode()                 print(f\"{line_str}\", end=\"\")          if exitcode := process.wait() != 0:             raise subprocess.CalledProcessError(exitcode, \"\\n\".join(cmd))      # run training -- see huggingface accelerate docs for details     print(\"launching dreambooth training script\")     _exec_subprocess(         [             \"accelerate\",             \"launch\",             \"examples/dreambooth/train_dreambooth.py\",             \"--train_text_encoder\",  # needs at least 16GB of GPU RAM.             f\"--pretrained_model_name_or_path={config.model_name}\",             f\"--instance_data_dir={img_path}\",             f\"--output_dir={MODEL_DIR}\",             f\"--instance_prompt='{prompt}'\",             f\"--resolution={config.resolution}\",             f\"--train_batch_size={config.train_batch_size}\",             f\"--gradient_accumulation_steps={config.gradient_accumulation_steps}\",             f\"--learning_rate={config.learning_rate}\",             f\"--lr_scheduler={config.lr_scheduler}\",             f\"--lr_warmup_steps={config.lr_warmup_steps}\",             f\"--max_train_steps={config.max_train_steps}\",             f\"--checkpointing_steps={config.checkpointing_steps}\",         ]     )     # The trained model artefacts have been output to the volume mounted at `MODEL_DIR`.     # To persist these artefacts for use in future inference function calls, we 'commit' the changes     # to the volume.     volume.commit() Copy The inference function.  To generate images from prompts using our fine-tuned model, we define a function called inference. In order to initialize the model just once on container startup, we use Modal’s container lifecycle feature, which requires the function to be part of a class. The modal.Volume is mounted at MODEL_DIR, so that the fine-tuned model created by train is then available to inference.  @stub.cls(     image=image,     gpu=\"A100\",     volumes={MODEL_DIR: volume}, ) class Model:     def __enter__(self):         import torch         from diffusers import DDIMScheduler, StableDiffusionPipeline          # Reload the modal.Volume to ensure the latest state is accessible.         volume.reload()          # set up a hugging face inference pipeline using our model         ddim = DDIMScheduler.from_pretrained(MODEL_DIR, subfolder=\"scheduler\")         pipe = StableDiffusionPipeline.from_pretrained(             MODEL_DIR,             scheduler=ddim,             torch_dtype=torch.float16,             safety_checker=None,         ).to(\"cuda\")         pipe.enable_xformers_memory_efficient_attention()         self.pipe = pipe      @method()     def inference(self, text, config):         image = self.pipe(             text,             num_inference_steps=config.num_inference_steps,             guidance_scale=config.guidance_scale,         ).images[0]          return image Copy Wrap the trained model in Gradio’s web UI  Gradio.app makes it super easy to expose a model’s functionality in an easy-to-use, responsive web interface.  This model is a text-to-image generator, so we set up an interface that includes a user-entry text box and a frame for displaying images.  We also provide some example text inputs to help guide users and to kick-start their creative juices.  You can deploy the app on Modal forever with the command modal deploy dreambooth_app.py.  @stub.function(     image=image,     concurrency_limit=3,     mounts=[Mount.from_local_dir(assets_path, remote_path=\"/assets\")], ) @asgi_app() def fastapi_app():     import gradio as gr     from gradio.routes import mount_gradio_app      # Call to the GPU inference function on Modal.     def go(text):         return Model().inference.remote(text, config)      # set up AppConfig     config = AppConfig()      instance_phrase = f\"{config.instance_name} the {config.class_name}\"      example_prompts = [         f\"{instance_phrase}\",         f\"a painting of {instance_phrase.title()} With A Pearl Earring, by Vermeer\",         f\"oil painting of {instance_phrase} flying through space as an astronaut\",         f\"a painting of {instance_phrase} in cyberpunk city. character design by cory loftis. volumetric light, detailed, rendered in octane\",         f\"drawing of {instance_phrase} high quality, cartoon, path traced, by studio ghibli and don bluth\",     ]      modal_docs_url = \"https://modal.com/docs/guide\"     modal_example_url = f\"{modal_docs_url}/ex/dreambooth_app\"      description = f\"\"\"Describe what they are doing or how a particular artist or style would depict them. Be fantastical! Try the examples below for inspiration.  ### Learn how to make your own [here]({modal_example_url}).     \"\"\"      # add a gradio UI around inference     interface = gr.Interface(         fn=go,         inputs=\"text\",         outputs=gr.Image(shape=(512, 512)),         title=f\"Generate images of {instance_phrase}.\",         description=description,         examples=example_prompts,         css=\"/assets/index.css\",         allow_flagging=\"never\",     )      # mount for execution on Modal     return mount_gradio_app(         app=web_app,         blocks=interface,         path=\"/\",     ) Copy Running this on the command line  You can use the modal command-line interface to interact with this code, in particular training the model and running the interactive Gradio service  modal run dreambooth_app.py will train the model modal serve dreambooth_app.py will serve the Gradio interface at a temporarily location. modal shell dreambooth_app.py is a convenient helper to open a bash shell in our image (for debugging)  Remember, once you’ve trained your own fine-tuned model, you can deploy it using modal deploy dreambooth_app.py.  This app is already deployed on Modal and you can try it out at https://modal-labs-example-dreambooth-app-fastapi-app.modal.run  @stub.local_entrypoint() def run():     with open(TrainConfig().instance_example_urls_file) as f:         instance_example_urls = [line.strip() for line in f.readlines()]     train.remote(instance_example_urls) Copy Pet Art Dreambooth with Hugging Face and Gradio Setting up the dependencies Config Get finetuning dataset Finetuning a text-to-image model Finetuning with Hugging Face 🧨 Diffusers and Accelerate The inference function. Wrap the trained model in Gradio’s web UI Running this on the command line Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/dreambooth/dreambooth_app.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/llm-finetuning","title":"Fine-tune an LLM in minutes (ft. Llama 2, CodeLlama, Mistral, etc.) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler Fine-tune an LLM in minutes (ft. Llama 2, CodeLlama, Mistral, etc.) View on GitHub  Tired of prompt engineering? Fine-tuning helps you get more out of a pretrained LLM by adjusting the model weights to better fit a specific task. This operational guide will help you take a base model and fine-tune it on your own dataset (API docs, conversation transcripts, etc.) in the matter of minutes.  The repository comes ready to use as-is with all the recommended, start-of-the-art optimizations for fast training results:  Parameter-Efficient Fine-Tuning (PEFT) via LoRA adapters for faster convergence Flash Attention for fast and memory-efficient attention during training (note: only works with certain hardware, like A100s) Gradient checkpointing to reduce VRAM footprint, fit larger batches and get higher training throughput Distributed training via DeepSpeed so training scales optimally with multiple GPUs  The heavy lifting is done by the axolotl library. For the purposes of this guide, we’ll fine-tune CodeLlama 7B to generate SQL queries, but the code is easy to tweak for many base models, datasets, and training configurations.  Best of all, using Modal for training means you never have to worry about infrastructure headaches like building images, provisioning GPUs, and managing cloud storage. If a training script runs on Modal, it’s repeatable and scalable enough to ship to production right away.  Prerequisites  To follow along, make sure that you have completed the following:  Set up Modal account:  pip install modal python3 -m modal setup Copy  Create a HuggingFace secret in your workspace (only HF_TOKEN is needed, which you can get if you go into your Hugging Face settings > API tokens)  Clone the repository and navigate to the directory:  git clone https://github.com/modal-labs/llm-finetuning.git cd llm-finetuning Copy  Some models like Llama 2 also require that you apply for access, which you can do on the Hugging Face page (granted instantly).  Quickstart  We created a simple GUI using Gradio that makes it easy to train and test models using this repository with a single click. All you need to do for customization is paste your desired training config (YAML) and dataset (JSONL) as plain text into the UI.  See Using the GUI for details on getting started with the Gradio interface.  Code overview  The source directory contains a training script to launch a training job in the cloud with your config/dataset (config.yml and my_data.jsonl, unless otherwise specified), as well as an inference engine for testing your training results.  We use Modal’s built-in cloud storage system to share data across all functions in the app. In particular, we mount a persisting volume at /pretrained inside the container to store our pretrained models (so we only need to load them once) and another persisting volume at /runs to store our training config, dataset, and results for each run (for easier reproducibility and management).  There are two main ways to train:  Use the GUI to familiarize with the system (recommended for new fine-tuners!) Use CLI commands (recommended for power users) Training  The training script contains three Modal functions that run in the cloud:  launch prepares a new folder in the /runs volume with the training config and data for a new training job. It also ensures the base model is downloaded from HuggingFace. train takes a prepared run folder in the volume and performs the training job using the config and data. merge merges the trained adapter with the base model (as a CPU job).  By default, when you make local changes to either config.yml or my_data.jsonl, they will be used for your next training run. You can also specify which local config and data files to use with the --config and --dataset flags. See Making it your own for more details on customizing your dataset and config.  To kickstart a training job with the CLI, use:  modal run --detach src.train Copy  --detach lets the app continue running even if your client disconnects.  The training run folder name will be in the command output (e.g. axo-2023-11-24-17-26-66e8). You can check if your fine-tuned model is stored properly in this folder using modal volume ls or the File Explorer in our Gradio GUI.  Serving your fine-tuned model  Once a training run has completed, run inference to compare the model before/after training.  Inference.completion can spawn a vLLM inference container for any pre-trained or fine-tuned model from a previous training job.  You can serve a model for inference using the following command, specifying which training run folder to load the model from with the –run-folder flag (run folder name is in the training log output):  modal run -q src.inference --run-folder /runs/axo-2023-11-24-17-26-66e8 Copy  We use vLLM to speed up our inference up to 24x.  Using the GUI  The Gradio GUI makes it easy to run each of the cloud functions with a single click. The interface is also helpful for viewing the files in your mounted volume.  To use the GUI, first deploy the training backend with all the business logic (launch, train, and completion in __init__.py):  modal deploy src Copy  If you would like to change the number of GPUs in your training config (2 80GB A100s), you should set them during this deployment step. For example, if you wanted to use 4 40GB A100s instead:  NUM_GPUS=4 GPU_MEM=40 modal deploy src Copy  Then run the GUI as an ephemeral app:  modal run src.gui Copy  Within a couple seconds of running the app, you should see a *.modal.host link in the command output. Open the URL in your browser to use the app.  In the Gradio app, you should see two tabs for launching training runs and testing out trained models:     Train  Paste your desired config and dataset as text directly into the UI (formatted as YAML for the config and JSONL for the dataset, see training section for more details), then click “Launch training job.” You can inspect the files stored in your runs volume, including your training checkpoints and results, using the FileExplorer.  Inference  After a training run is completed, you can test out your fine-tuned model. Use the dropdown to switch between various training run results.  Making it your own  Training on your own dataset, using a different base model, and activating another SOTA technique is as easy as modifying a couple files.  Dataset  Bringing your own dataset is as simple as creating a JSONL file — Axolotl supports many dataset formats (see more).  We recommend adding your custom dataset as a JSONL file in the src directory (or pasting the text directly into my_data.jsonl box if using the GUI) and making the appropriate modifications to your config, as explained below.  Config  All of your training parameters and options are customizable in a single config file. We recommend duplicating one of the example_configs to src/config.yml (or directly pasting into the UI if using the GUI) and modifying as you need. See an overview of Axolotl’s config options here.  The most important options to consider are:  Model  base_model: codellama/CodeLlama-7b-Instruct-hf Copy  Dataset (by default we upload a local .jsonl file from the src folder in completion format, but you can see all dataset options here)  datasets: - path: my_data.jsonl    ds_type: json    type: completion Copy  LoRA  adapter: lora # for qlora, or leave blank for full finetune lora_r: 8 lora_alpha: 16 lora_dropout: 0.05 lora_target_modules:   - q_proj   - v_proj Copy  Multi-GPU training  We recommend DeepSpeed for multi-GPU training, which is easy to set up. Axolotl provides several default deepspeed JSON configurations and Modal makes it easy to attach multiple GPUs of any type in code, so all you need to do is specify which of these configs you’d like to use.  In your config.yml:  deepspeed: /root/axolotl/deepspeed/zero3.json Copy  In train.py:  import os  N_GPUS = int(os.environ.get(\"N_GPUS\", 2)) GPU_MEM = int(os.environ.get(\"GPU_MEM\", 80)) GPU_CONFIG = modal.gpu.A100(count=N_GPUS, memory=GPU_MEM)  # you can also change this in code to use A10Gs or T4s Copy  Logging with Weights and Biases  To track your training runs with Weights and Biases:  Create a Weights and Biases secret in your Modal dashboard, if not set up already (only the WANDB_API_KEY is needed, which you can get if you log into your Weights and Biases account and go to the Authorize page) Add the Weights and Biases secret to your app, so initializing your stub in common.py should look like: from modal import Stub, Secret  stub = Stub(\"my_app\", secrets=[Secret.from_name(\"huggingface\"), Secret.from_name(\"my-wandb-secret\")]) Copy Add your wandb config to your config.yml: wandb_project: mistral-7b-samsum wandb_watch: gradients Copy  Once you have your trained model, you can easily deploy it to production for serverless inference via Modal’s web endpoint feature (see example here). Modal will handle all the auto-scaling for you, so that you only pay for the compute you use!  Fine-tune an LLM in minutes (ft. Llama 2, CodeLlama, Mistral, etc.) Prerequisites Quickstart Code overview Training Serving your fine-tuned model Using the GUI Making it your own Dataset Config © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/vllm_mixtral","title":"Fast inference with vLLM (Mixtral 8x7B) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Fast inference with vLLM (Mixtral 8x7B)  In this example, we show how to run basic inference, using vLLM to take advantage of PagedAttention, which speeds up sequential inferences with optimized key-value caching.  vLLM also supports a use case as a FastAPI server which we will explore in a future guide. This example walks through setting up an environment that works with vLLM for basic inference.  We are running the Mixtral 8x7B Instruct model here, which is a mixture-of-experts model finetuned for conversation. You can expect 3 minute second cold starts For a single request, the throughput is about 11 tokens/second, but there are upcoming vLLM optimizations to improve this. The larger the batch of prompts, the higher the throughput (up to about 300 tokens/second). For example, with the 60 prompts below, we can produce 30k tokens in 100 seconds.  Setup  First we import the components we need from modal.  import os import time  from modal import Image, Stub, gpu, method  MODEL_DIR = \"/model\" BASE_MODEL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" GPU_CONFIG = gpu.A100(memory=80, count=2) Copy Define a container image  We want to create a Modal image which has the model weights pre-saved to a directory. The benefit of this is that the container no longer has to re-download the model from Huggingface - instead, it will take advantage of Modal’s internal filesystem for faster cold starts.  Download the weights  We can download the model to a particular directory using the HuggingFace utility function snapshot_download.  Tip: avoid using global variables in this function. Changes to code outside this function will not be detected and the download step will not re-run.  def download_model_to_folder():     from huggingface_hub import snapshot_download     from transformers.utils import move_cache      os.makedirs(MODEL_DIR, exist_ok=True)      snapshot_download(         BASE_MODEL,         local_dir=MODEL_DIR,         ignore_patterns=\"*.pt\",  # Using safetensors     )     move_cache() Copy Image definition  We’ll start from a Dockerhub image recommended by vLLM, and use run_function to run the function defined above to ensure the weights of the model are saved within the container image.  vllm_image = (     Image.from_registry(         \"nvidia/cuda:12.1.0-base-ubuntu22.04\", add_python=\"3.10\"     )     .pip_install(\"vllm==0.2.5\", \"huggingface_hub==0.19.4\", \"hf-transfer==0.1.4\")     .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})     .run_function(download_model_to_folder, timeout=60 * 20) )  stub = Stub(\"example-vllm-mixtral\") Copy The model class  The inference function is best represented with Modal’s class syntax and the __enter__ method. This enables us to load the model into memory just once every time a container starts up, and keep it cached on the GPU for each subsequent invocation of the function.  The vLLM library allows the code to remain quite clean. There are, however, some outstanding issues and performance improvements that we patch here, such as multi-GPU setup and suboptimal Ray CPU pinning.  @stub.cls(     gpu=GPU_CONFIG,     timeout=60 * 10,     container_idle_timeout=60 * 10,     allow_concurrent_inputs=10,     image=vllm_image, ) class Model:     def __enter__(self):         from vllm.engine.arg_utils import AsyncEngineArgs         from vllm.engine.async_llm_engine import AsyncLLMEngine          if GPU_CONFIG.count > 1:             # Patch issue from https://github.com/vllm-project/vllm/issues/1116             import ray              ray.shutdown()             ray.init(num_gpus=GPU_CONFIG.count)          engine_args = AsyncEngineArgs(             model=MODEL_DIR,             tensor_parallel_size=GPU_CONFIG.count,             gpu_memory_utilization=0.90,         )          self.engine = AsyncLLMEngine.from_engine_args(engine_args)         self.template = \"<s> [INST] {user} [/INST] \"          # Performance improvement from https://github.com/vllm-project/vllm/issues/2073#issuecomment-1853422529         if GPU_CONFIG.count > 1:             import subprocess              RAY_CORE_PIN_OVERRIDE = \"cpuid=0 ; for pid in $(ps xo '%p %c' | grep ray:: | awk '{print $1;}') ; do taskset -cp $cpuid $pid ; cpuid=$(($cpuid + 1)) ; done\"             subprocess.call(RAY_CORE_PIN_OVERRIDE, shell=True)      @method()     async def completion_stream(self, user_question):         from vllm import SamplingParams         from vllm.utils import random_uuid          sampling_params = SamplingParams(             temperature=0.75,             max_tokens=1024,             repetition_penalty=1.1,         )          t0 = time.time()         request_id = random_uuid()         result_generator = self.engine.generate(             self.template.format(user=user_question),             sampling_params,             request_id,         )         index, num_tokens = 0, 0         async for output in result_generator:             if (                 output.outputs[0].text                 and \"\\ufffd\" == output.outputs[0].text[-1]             ):                 continue             text_delta = output.outputs[0].text[index:]             index = len(output.outputs[0].text)             num_tokens = len(output.outputs[0].token_ids)              yield text_delta          print(f\"Generated {num_tokens} tokens in {time.time() - t0:.2f}s\") Copy Run the model  We define a local_entrypoint to call our remote function sequentially for a list of inputs. You can run this locally with modal run -q vllm_mixtral.py. The q flag enables the text to stream in your local terminal.  @stub.local_entrypoint() def main():     model = Model()     questions = [         \"Implement a Python function to compute the Fibonacci numbers.\",         \"What is the fable involving a fox and grapes?\",         \"What were the major contributing factors to the fall of the Roman Empire?\",         \"Describe the city of the future, considering advances in technology, environmental changes, and societal shifts.\",         \"What is the product of 9 and 8?\",         \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",     ]     for question in questions:         print(\"Sending new request:\", question)         for text in model.completion_stream.remote_gen(question):             print(text, end=\"\", flush=True) Copy Deploy and invoke the model  Once we deploy this model with modal deploy text_generation_inference.py, we can invoke inference from other apps, sharing the same pool of GPU containers with all other apps we might need.  $ python >>> import modal >>> f = modal.Function.lookup(\"example-tgi-Mixtral-8x7B-Instruct-v0.1\", \"Model.generate\") >>> f.remote(\"What is the story about the fox and grapes?\") 'The story about the fox and grapes ... Copy Coupling a frontend web application  We can stream inference from a FastAPI backend, also deployed on Modal.  You can try our deployment here.  from pathlib import Path  from modal import Mount, asgi_app  frontend_path = Path(__file__).parent / \"llm-frontend\"   @stub.function(     mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],     keep_warm=1,     allow_concurrent_inputs=20,     timeout=60 * 10, ) @asgi_app(label=\"vllm-mixtral\") def app():     import json      import fastapi     import fastapi.staticfiles     from fastapi.responses import StreamingResponse      web_app = fastapi.FastAPI()      @web_app.get(\"/stats\")     async def stats():         stats = await Model().completion_stream.get_current_stats.aio()         return {             \"backlog\": stats.backlog,             \"num_total_runners\": stats.num_total_runners,             \"model\": BASE_MODEL + \" (vLLM)\",         }      @web_app.get(\"/completion/{question}\")     async def completion(question: str):         from urllib.parse import unquote          async def generate():             async for text in Model().completion_stream.remote_gen.aio(                 unquote(question)             ):                 yield f\"data: {json.dumps(dict(text=text), ensure_ascii=False)}\\n\\n\"          return StreamingResponse(generate(), media_type=\"text/event-stream\")      web_app.mount(         \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)     )     return web_app Copy Fast inference with vLLM (Mixtral 8x7B) Setup Define a container image Download the weights Image definition The model class Run the model Deploy and invoke the model Coupling a frontend web application Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/vllm_mixtral.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide","title":"Introduction to Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Introduction to Modal  Modal lets you run code in the cloud without having to think about infrastructure.  Features Run any code remotely within seconds. Define container environments in code (or use one of our pre-built backends). Scale up horizontally to thousands of containers. Deploy and monitor persistent cron jobs. Attach GPUs with a single line of code. Serve your functions as web endpoints. Use powerful primitives like distributed dictionaries and queues. Getting started  The nicest thing about all of this is that you don’t have to set up any infrastructure. Just:  Create an account at modal.com Install the modal Python package Set up a token  …and you can start running jobs right away.  Modal is currently Python-only, but we may support other languages in the future.  How does it work?  Modal takes your code, puts it in a container, and executes it in the cloud.  Where does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.  Introduction to Modal Features Getting started How does it work? See it in action Hello, world!  A simple web scraper  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/misc-ml-examples","title":"Miscellaneous AI examples | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler Miscellaneous AI examples  Looking for how to make a popular machine learning library work with Modal? There’s a guide for that:  Vision model training and deployment with FastAI, WandB and Gradio Classifier training with TensorFlow and TensorBoard Batch inference with a Huggingface model Real-time object detection with webcam input Generate synthetic data with Jsonformer Stable Diffusion with AITemplate Stable Diffusion with ONNX Runtime Stable Diffusion with A1111 Run LLaMA with Machine Learning Compilation (MLC) Run OpenLLaMA-7B on an A100 Run Falcon-40B on an A100 with bitsandbytes quantization Run Falcon-40B on an A100 with GPTQ quantization Fine-tune Flan-T5 and monitor with TensorBoard Miscellaneous AI examples © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/text_generation_inference","title":"Hosting any LLaMA 2 model with Text Generation Inference (TGI) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Hosting any LLaMA 2 model with Text Generation Inference (TGI)  In this example, we show how to run an optimized inference server using Text Generation Inference (TGI) with performance advantages over standard text generation pipelines including:  continuous batching, so multiple generations can take place at the same time on a single container PagedAttention, an optimization that increases throughput.  This example deployment, accessible here, can serve LLaMA 2 70B with 70 second cold starts, up to 200 tokens/s of throughput and per-token latency of 55ms.  Setup  First we import the components we need from modal.  import os import subprocess from pathlib import Path  from modal import Image, Mount, Secret, Stub, asgi_app, gpu, method Copy  Next, we set which model to serve, taking care to specify the GPU configuration required to fit the model into VRAM, and the quantization method (bitsandbytes or gptq) if desired. Note that quantization does degrade token generation performance significantly.  Any model supported by TGI can be chosen here.  GPU_CONFIG = gpu.A100(memory=80, count=2) MODEL_ID = \"meta-llama/Llama-2-70b-chat-hf\" REVISION = \"36d9a7388cc80e5f4b3e9701ca2f250d21a96c30\" Copy  Add [\"--quantize\", \"gptq\"] for TheBloke GPTQ models.  LAUNCH_FLAGS = [     \"--model-id\",     MODEL_ID,     \"--port\",     \"8000\",     \"--revision\",     REVISION, ] Copy Define a container image  We want to create a Modal image which has the Huggingface model cache pre-populated. The benefit of this is that the container no longer has to re-download the model from Huggingface - instead, it will take advantage of Modal’s internal filesystem for faster cold starts. On the largest 70B model, the 135GB model can be loaded in as little as 70 seconds.  Download the weights  We can use the included utilities to download the model weights (and convert to safetensors, if necessary) as part of the image build.  def download_model():     subprocess.run(         [             \"text-generation-server\",             \"download-weights\",             MODEL_ID,             \"--revision\",             REVISION,         ],         env={             **os.environ,             \"HUGGING_FACE_HUB_TOKEN\": os.environ[\"HF_TOKEN\"],         },         check=True,     ) Copy Image definition  We’ll start from a Dockerhub image recommended by TGI, and override the default ENTRYPOINT for Modal to run its own which enables seamless serverless deployments.  Next we run the download step to pre-populate the image with our model weights.  For this step to work on a gated model such as LLaMA 2, the HF_TOKEN environment variable must be set (reference).  After creating a HuggingFace access token, head to the secrets page to create a Modal secret.  Finally, we install the text-generation client to interface with TGI’s Rust webserver over localhost.  stub = Stub(\"example-tgi-\" + MODEL_ID.split(\"/\")[-1])  tgi_image = (     Image.from_registry(\"ghcr.io/huggingface/text-generation-inference:1.0.3\")     .dockerfile_commands(\"ENTRYPOINT []\")     .run_function(download_model, secret=Secret.from_name(\"huggingface-secret\"))     .pip_install(\"text-generation\") ) Copy The model class  The inference function is best represented with Modal’s class syntax. The class syntax is a special representation for a Modal function which splits logic into two parts:  the __enter__ method, which runs once per container when it starts up, and the @method() function, which runs per inference request.  This means the model is loaded into the GPUs, and the backend for TGI is launched just once when each container starts, and this state is cached for each subsequent invocation of the function. Note that on start-up, we must wait for the Rust webserver to accept connections before considering the container ready.  Here, we also  specify the secret so the HUGGING_FACE_HUB_TOKEN environment variable is set specify how many A100s we need per container specify that each container is allowed to handle up to 10 inputs (i.e. requests) simultaneously keep idle containers for 10 minutes before spinning down lift the timeout of each request. @stub.cls(     secret=Secret.from_name(\"huggingface-secret\"),     gpu=GPU_CONFIG,     allow_concurrent_inputs=10,     container_idle_timeout=60 * 10,     timeout=60 * 60,     image=tgi_image, ) class Model:     def __enter__(self):         import socket         import time          from text_generation import AsyncClient          self.launcher = subprocess.Popen(             [\"text-generation-launcher\"] + LAUNCH_FLAGS,             env={                 **os.environ,                 \"HUGGING_FACE_HUB_TOKEN\": os.environ[\"HF_TOKEN\"],             },         )         self.client = AsyncClient(\"http://127.0.0.1:8000\", timeout=60)         self.template = \"\"\"<s>[INST] <<SYS>> {system} <</SYS>>  {user} [/INST] \"\"\"          # Poll until webserver at 127.0.0.1:8000 accepts connections before running inputs.         def webserver_ready():             try:                 socket.create_connection((\"127.0.0.1\", 8000), timeout=1).close()                 return True             except (socket.timeout, ConnectionRefusedError):                 # Check if launcher webserving process has exited.                 # If so, a connection can never be made.                 retcode = self.launcher.poll()                 if retcode is not None:                     raise RuntimeError(                         f\"launcher exited unexpectedly with code {retcode}\"                     )                 return False          while not webserver_ready():             time.sleep(1.0)          print(\"Webserver ready!\")      def __exit__(self, _exc_type, _exc_value, _traceback):         self.launcher.terminate()      @method()     async def generate(self, question: str):         prompt = self.template.format(system=\"\", user=question)         result = await self.client.generate(prompt, max_new_tokens=1024)          return result.generated_text      @method()     async def generate_stream(self, question: str):         prompt = self.template.format(system=\"\", user=question)          async for response in self.client.generate_stream(             prompt, max_new_tokens=1024         ):             if not response.token.special:                 yield response.token.text Copy Run the model  We define a local_entrypoint to invoke our remote function. You can run this script locally with modal run text_generation_inference.py.  @stub.local_entrypoint() def main():     print(         Model().generate.remote(             \"Implement a Python function to compute the Fibonacci numbers.\"         )     ) Copy Serve the model  Once we deploy this model with modal deploy text_generation_inference.py, we can serve it behind an ASGI app front-end. The front-end code (a single file of Alpine.js) is available here.  You can try our deployment here.  frontend_path = Path(__file__).parent / \"llm-frontend\"   @stub.function(     mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],     keep_warm=1,     allow_concurrent_inputs=10,     timeout=60 * 10, ) @asgi_app(label=\"tgi-app\") def app():     import json      import fastapi     import fastapi.staticfiles     from fastapi.responses import StreamingResponse      web_app = fastapi.FastAPI()      @web_app.get(\"/stats\")     async def stats():         stats = await Model().generate_stream.get_current_stats.aio()         return {             \"backlog\": stats.backlog,             \"num_total_runners\": stats.num_total_runners,             \"model\": MODEL_ID,         }      @web_app.get(\"/completion/{question}\")     async def completion(question: str):         from urllib.parse import unquote          async def generate():             async for text in Model().generate_stream.remote_gen.aio(                 unquote(question)             ):                 yield f\"data: {json.dumps(dict(text=text), ensure_ascii=False)}\\n\\n\"          return StreamingResponse(generate(), media_type=\"text/event-stream\")      web_app.mount(         \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)     )     return web_app Copy Invoke the model from other apps  Once the model is deployed, we can invoke inference from other apps, sharing the same pool of GPU containers with all other apps we might need.  $ python >>> import modal >>> f = modal.Function.lookup(\"example-tgi-Llama-2-70b-chat-hf\", \"Model.generate\") >>> f.remote(\"What is the story about the fox and grapes?\") 'The story about the fox and grapes ... Copy Hosting any LLaMA 2 model with Text Generation Inference (TGI) Setup Define a container image Download the weights Image definition The model class Run the model Serve the model Invoke the model from other apps Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/text_generation_inference.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/doc_ocr_jobs","title":"Document OCR job queue | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Document OCR job queue  This tutorial shows you how to use Modal as an infinitely scalable job queue that can service async tasks from a web app. For the purpose of this tutorial, we’ve also built a React + FastAPI web app on Modal that works together with it, but note that you don’t need a web app running on Modal to use this pattern. You can submit async tasks to Modal from any Python application (for example, a regular Django app running on Kubernetes).  Our job queue will handle a single task: running OCR transcription for images. We’ll make use of a pre-trained Document Understanding model using the donut package to accomplish this. Try it out for yourself here.  Define a Stub  Let’s first import modal and define a Stub. Later, we’ll use the name provided for our Stub to find it from our web app, and submit tasks to it.  import urllib.request  import modal  stub = modal.Stub(\"example-doc-ocr-jobs\") Copy Model cache  donut downloads the weights for pre-trained models to a local directory, if those weights don’t already exist. To decrease start-up time, we want this download to happen just once, even across separate function invocations. To accomplish this, we use the Image.run_function method, which allows us to run some code at image build time to save the model weights into the image.  CACHE_PATH = \"/root/model_cache\" MODEL_NAME = \"naver-clova-ix/donut-base-finetuned-cord-v2\"   def download_model_weights() -> None:     from huggingface_hub import snapshot_download      snapshot_download(repo_id=MODEL_NAME, cache_dir=CACHE_PATH)   image = (     modal.Image.debian_slim(python_version=\"3.9\")     .pip_install(         \"donut-python==1.0.7\",         \"huggingface-hub==0.16.4\",         \"transformers==4.21.3\",         \"timm==0.5.4\",     )     .run_function(download_model_weights) ) Copy Handler function  Now let’s define our handler function. Using the @stub.function() decorator, we set up a Modal Function that uses GPUs, runs on a custom container image, and automatically retries failures up to 3 times.  @stub.function(     gpu=\"any\",     image=image,     retries=3, ) def parse_receipt(image: bytes):     import io      import torch     from donut import DonutModel     from PIL import Image      # Use donut fine-tuned on an OCR dataset.     task_prompt = \"<s_cord-v2>\"     pretrained_model = DonutModel.from_pretrained(         MODEL_NAME,         cache_dir=CACHE_PATH,     )      # Initialize model.     pretrained_model.half()     device = torch.device(\"cuda\")     pretrained_model.to(device)      # Run inference.     input_img = Image.open(io.BytesIO(image))     output = pretrained_model.inference(image=input_img, prompt=task_prompt)[         \"predictions\"     ][0]     print(\"Result: \", output)      return output Copy Deploy  Now that we have a function, we can publish it by deploying the app:  modal deploy doc_ocr_jobs.py Copy  Once it’s published, we can look up this function from another Python process and submit tasks to it:  fn = modal.Function.lookup(\"example-doc-ocr-jobs\", \"parse_receipt\") fn.spawn(my_image) Copy  Modal will auto-scale to handle all the tasks queued, and then scale back down to 0 when there’s no work left. To see how you could use this from a Python web app, take a look at the receipt parser frontend tutorial.  Run manually  We can also trigger parse_receipt manually for easier debugging: modal run doc_ocr_jobs::stub.main To try it out, you can find some example receipts here.  @stub.local_entrypoint() def main():     from pathlib import Path      receipt_filename = Path(__file__).parent / \"receipt.png\"     if receipt_filename.exists():         with open(receipt_filename, \"rb\") as f:             image = f.read()     else:         image = urllib.request.urlopen(             \"https://nwlc.org/wp-content/uploads/2022/01/Brandys-walmart-receipt-8.webp\"         ).read()     print(parse_receipt.remote(image)) Copy Document OCR job queue Define a Stub Model cache Handler function Deploy Run manually Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 09_job_queues/doc_ocr_jobs.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/algolia_indexer","title":"Algolia docsearch crawler | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Algolia docsearch crawler  This tutorial shows you how to use Modal to run the Algolia docsearch crawler to index your website and make it searchable. This is not just example code - we run the same code in production to power search on this page (Ctrl+K to try it out!).  Basic setup  Let’s get the imports out of the way.  import json import os import subprocess  from modal import Image, Secret, Stub, web_endpoint Copy  Modal lets you use and extend existing Docker images, as long as they have python and pip available. We’ll use the official crawler image built by Algolia, with a small adjustment: since this image has python symlinked to python3.6 and Modal is not compatible with Python 3.6, we install Python 3.8 and symlink that as the python executable instead.  algolia_image = Image.from_registry(     \"algolia/docsearch-scraper\",     add_python=\"3.8\",     setup_dockerfile_commands=[\"ENTRYPOINT []\"], )  stub = Stub(\"example-algolia-indexer\") Copy Configure the crawler  Now, let’s configure the crawler with the website we want to index, and which CSS selectors we want to scrape. Complete documentation for crawler configuration is available here.  CONFIG = {     \"index_name\": \"modal_docs\",     \"start_urls\": [         {\"url\": \"https://modal.com/docs/guide\", \"page_rank\": 5},         {\"url\": \"https://modal.com/docs/examples\", \"page_rank\": 3},         {\"url\": \"https://modal.com/docs/reference\", \"page_rank\": 1},     ],     \"selectors\": {         \"lvl0\": \"article h1\",         \"lvl1\": \"article h1\",         \"lvl2\": \"article h2\",         \"lvl3\": \"article h3\",         \"lvl4\": \"article h4\",         \"text\": \"article p,article ol,article ul,article pre\",     }, } Copy Create an API key  If you don’t already have one, sign up for an account on Algolia. Set up a project and create an API key with write access to your index, and with the ACL permissions addObject, editSettings and deleteIndex. Now, create a secret on the Modal Secrets page with the API_KEY and APPLICATION_ID you just created. You can name this anything you want, we named it algolia-secret.  The actual function  We want to trigger our crawler from our CI/CD pipeline, so we’re serving it as a web endpoint that can be triggered by a GET request during deploy. You could also consider running the crawler on a schedule.  The Algolia crawler is written for Python 3.6 and needs to run in the pipenv created for it, so we’re invoking it using a subprocess.  @stub.function(     image=algolia_image,     secrets=[Secret.from_name(\"algolia-secret\")], ) def crawl():     # Installed with a 3.6 venv; Python 3.6 is unsupported by Modal, so use a subprocess instead.     subprocess.run(         [\"pipenv\", \"run\", \"python\", \"-m\", \"src.index\"],         env={**os.environ, \"CONFIG\": json.dumps(CONFIG)},     ) Copy  We want to be able to trigger this function through a webhook.  @stub.function() @web_endpoint() def crawl_webhook():     crawl.remote()     return \"Finished indexing docs\" Copy Deploy the indexer  That’s all the code we need! To deploy your application, run  modal deploy algolia_indexer.py Copy  If successful, this will print a URL for your new webhook, that you can hit using curl or a browser. Logs from webhook invocations can be found from the apps page.  The indexed contents can be found at https://www.algolia.com/apps/APP_ID/explorer/browse/, for your APP_ID. Once you’re happy with the results, you can set up the docsearch package with your website, and create a search component that uses this index.  Entrypoint for development  To make it easier to test this, we also have an entrypoint for when you run modal run algolia_indexer.py  @stub.local_entrypoint() def run():     crawl.remote() Copy Algolia docsearch crawler Basic setup Configure the crawler Create an API key The actual function Deploy the indexer Entrypoint for development Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 10_integrations/algolia_indexer.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/covid_datasette","title":"Publish interactive datasets with Datasette | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Publish interactive datasets with Datasette  This example shows how to serve a Datasette application on Modal. The published dataset is COVID-19 case data from Johns Hopkins University which is refreshed daily. Try it out for yourself at modal-labs-example-covid-datasette-app.modal.run/covid-19.  Some Modal features it uses:  Volumes: a persisted volume lets us store and grow the published dataset over time. Scheduled functions: the underlying dataset is refreshed daily, so we schedule a function to run daily. Web endpoints: exposes the Datasette application for web browser interaction and API requests. Basic setup  Let’s get started writing code. For the Modal container image we need a few Python packages, including GitPython, which we’ll use to download the dataset.  import asyncio import pathlib import shutil import subprocess from datetime import datetime from urllib.request import urlretrieve  from modal import Image, Period, Stub, Volume, asgi_app  stub = Stub(\"example-covid-datasette\") datasette_image = (     Image.debian_slim()     .pip_install(\"datasette~=0.63.2\", \"sqlite-utils\")     .apt_install(\"unzip\") ) Copy Persistent dataset storage  To separate database creation and maintenance from serving, we’ll need the underlying database file to be stored persistently. To achieve this we use a Volume.  volume = Volume.persisted(\"example-covid-datasette-cache-vol\")  VOLUME_DIR = \"/cache-vol\" REPORTS_DIR = pathlib.Path(VOLUME_DIR, \"COVID-19\") DB_PATH = pathlib.Path(VOLUME_DIR, \"covid-19.db\") Copy Getting a dataset  Johns Hopkins has been publishing up-to-date COVID-19 pandemic data on GitHub since early February 2020, and as of late September 2022 daily reporting is still rolling in. Their dataset is what this example will use to show off Modal and Datasette’s capabilities.  The full git repository size for the dataset is over 6GB, but we only need to shallow clone around 300MB.  @stub.function(     image=datasette_image,     volumes={VOLUME_DIR: volume},     retries=2, ) def download_dataset(cache=True):     if REPORTS_DIR.exists() and cache:         print(f\"Dataset already present and {cache=}. Skipping download.\")         return     elif REPORTS_DIR.exists():         print(\"Cleaning dataset before re-downloading...\")         shutil.rmtree(REPORTS_DIR)      print(\"Downloading dataset...\")     urlretrieve(         \"https://github.com/CSSEGISandData/COVID-19/archive/refs/heads/master.zip\",         \"/tmp/covid-19.zip\",     )      print(\"Unpacking archive...\")     prefix = \"COVID-19-master/csse_covid_19_data/csse_covid_19_daily_reports\"     subprocess.run(         f\"unzip /tmp/covid-19.zip {prefix}/* -d {REPORTS_DIR}\", shell=True     )     subprocess.run(f\"mv {REPORTS_DIR / prefix}/* {REPORTS_DIR}\", shell=True)      print(\"Committing the volume...\")     volume.commit()      print(\"Finished downloading dataset.\") Copy Data munging  This dataset is no swamp, but a bit of data cleaning is still in order. The following two functions read a handful of .csv files and clean the data, before inserting it into SQLite.  def load_daily_reports():     volume.reload()     daily_reports = list(REPORTS_DIR.glob(\"*.csv\"))     if not daily_reports:         raise RuntimeError(             f\"Could not find any daily reports in {REPORTS_DIR}.\"         )     for filepath in daily_reports:         yield from load_report(filepath)   def load_report(filepath):     import csv      mm, dd, yyyy = filepath.stem.split(\"-\")     with filepath.open() as fp:         for row in csv.DictReader(fp):             province_or_state = (                 row.get(\"\\ufeffProvince/State\")                 or row.get(\"Province/State\")                 or row.get(\"Province_State\")                 or None             )             country_or_region = row.get(\"Country_Region\") or row.get(                 \"Country/Region\"             )             yield {                 \"day\": f\"{yyyy}-{mm}-{dd}\",                 \"country_or_region\": country_or_region.strip()                 if country_or_region                 else None,                 \"province_or_state\": province_or_state.strip()                 if province_or_state                 else None,                 \"confirmed\": int(float(row[\"Confirmed\"] or 0)),                 \"deaths\": int(float(row[\"Deaths\"] or 0)),                 \"recovered\": int(float(row[\"Recovered\"] or 0)),                 \"active\": int(row[\"Active\"]) if row.get(\"Active\") else None,                 \"last_update\": row.get(\"Last Update\")                 or row.get(\"Last_Update\")                 or None,             } Copy Inserting into SQLite  With the CSV processing out of the way, we’re ready to create an SQLite DB and feed data into it. Importantly, the prep_db function mounts the same volume used by download_dataset(), and rows are batch inserted with progress logged after each batch, as the full COVID-19 has millions of rows and does take some time to be fully inserted.  A more sophisticated implementation would only load new data instead of performing a full refresh, but we’re keeping things simple for this example!  def chunks(it, size):     import itertools      return iter(lambda: tuple(itertools.islice(it, size)), ())   @stub.function(     image=datasette_image,     volumes={VOLUME_DIR: volume},     timeout=900, ) def prep_db():     import sqlite_utils      print(\"Loading daily reports...\")     records = load_daily_reports()      DB_PATH.parent.mkdir(parents=True, exist_ok=True)     db = sqlite_utils.Database(DB_PATH)     table = db[\"johns_hopkins_csse_daily_reports\"]      batch_size = 100_000     for i, batch in enumerate(chunks(records, size=batch_size)):         truncate = True if i == 0 else False         table.insert_all(batch, batch_size=batch_size, truncate=truncate)         print(f\"Inserted {len(batch)} rows into DB.\")      table.create_index([\"day\"], if_not_exists=True)     table.create_index([\"province_or_state\"], if_not_exists=True)     table.create_index([\"country_or_region\"], if_not_exists=True)      db.close()      print(\"Syncing DB with volume.\")     volume.commit() Copy Keep it fresh  Johns Hopkins commits new data to the dataset repository every day, so we set up a scheduled function to automatically refresh the database every 24 hours.  @stub.function(schedule=Period(hours=24), timeout=1000) def refresh_db():     print(f\"Running scheduled refresh at {datetime.now()}\")     download_dataset.remote(cache=False)     prep_db.remote() Copy Web endpoint  Hooking up the SQLite database to a Modal webhook is as simple as it gets. The Modal @asgi_app decorator wraps a few lines of code: one import and a few lines to instantiate the Datasette instance and return its app server.  @stub.function(     image=datasette_image,     volumes={VOLUME_DIR: volume},     allow_concurrent_inputs=16, ) @asgi_app() def app():     from datasette.app import Datasette      ds = Datasette(files=[DB_PATH], settings={\"sql_time_limit_ms\": 10000})     asyncio.run(ds.invoke_startup())     return ds.app() Copy Publishing to the web  Run this script using modal run covid_datasette.py and it will create the database.  You can then use modal serve covid_datasette.py to create a short-lived web URL that exists until you terminate the script.  When publishing the interactive Datasette app you’ll want to create a persistent URL. Just run modal deploy covid_datasette.py.  @stub.local_entrypoint() def run():     print(\"Downloading COVID-19 dataset...\")     download_dataset.remote()     print(\"Prepping SQLite DB...\")     prep_db.remote() Copy  You can explore the data at the deployed web endpoint.  Publish interactive datasets with Datasette Basic setup Persistent dataset storage Getting a dataset Data munging Inserting into SQLite Keep it fresh Web endpoint Publishing to the web Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 10_integrations/covid_datasette.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/potus_speech_qanda","title":"Question-answering with LangChain | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Question-answering with LangChain  In this example we create a large-language-model (LLM) powered question answering web endpoint and CLI. Only a single document is used as the knowledge-base of the application, the 2022 USA State of the Union address by President Joe Biden. However, this same application structure could be extended to do question-answering over all State of the Union speeches, or other large text corpuses.  It’s the LangChain library that makes this all so easy. This demo is only around 100 lines of code!  Defining dependencies  The example uses three PyPi packages to make scraping easy, and three to build and run the question-answering functionality. These are installed into a Debian Slim base image using the pip_install function.  Because OpenAI’s API is used, we also specify the openai-secret Modal Secret, which contains an OpenAI API key.  A docsearch global variable is also declared to facilitate caching a slow operation in the code below.  from pathlib import Path  from modal import Image, Secret, Stub, web_endpoint  image = Image.debian_slim().pip_install(     # scraping pkgs     \"beautifulsoup4~=4.11.1\",     \"httpx~=0.23.3\",     \"lxml~=4.9.2\",     # langchain pkgs     \"faiss-cpu~=1.7.3\",     \"langchain~=0.0.138\",     \"openai~=0.27.4\",     \"tiktoken==0.3.0\", ) stub = Stub(     name=\"example-langchain-qanda\",     image=image,     secrets=[Secret.from_name(\"openai-secret\")], ) docsearch = None  # embedding index that's relatively expensive to compute, so caching with global var. Copy Scraping the speech from whitehouse.gov  It’s super easy to scrape the transcipt of Biden’s speech using httpx and BeautifulSoup. This speech is just one document and it’s relatively short, but it’s enough to demonstrate the question-answering capability of the LLM chain.  def scrape_state_of_the_union() -> str:     import httpx     from bs4 import BeautifulSoup      url = \"https://www.whitehouse.gov/state-of-the-union-2022/\"      # fetch article; simulate desktop browser     headers = {         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_2) AppleWebKit/601.3.9 (KHTML, like Gecko) Version/9.0.2 Safari/601.3.9\"     }     response = httpx.get(url, headers=headers)     soup = BeautifulSoup(response.text, \"lxml\")      # get all text paragraphs & construct string of article text     speech_text = \"\"     speech_section = soup.find_all(         \"div\", {\"class\": \"sotu-annotations__content\"}     )     if speech_section:         paragraph_tags = speech_section[0].find_all(\"p\")         speech_text = \"\".join([p.get_text() for p in paragraph_tags])      return speech_text.replace(\"\\t\", \"\") Copy Constructing the Q&A chain  At a high-level, this LLM chain will be able to answer questions asked about Biden’s speech and provide references to which parts of the speech contain the evidence for given answers.  The chain combines a text-embedding index over parts of Biden’s speech with OpenAI’s GPT-3 LLM. The index is used to select the most likely relevant parts of the speech given the question, and these are used to build a specialized prompt for the OpenAI language model.  For more information on this, see LangChain’s “Question Answering” notebook.  def retrieve_sources(sources_refs: str, texts: list[str]) -> list[str]:     \"\"\"     Map back from the references given by the LLM's output to the original text parts.     \"\"\"     clean_indices = [         r.replace(\"-pl\", \"\").strip() for r in sources_refs.split(\",\")     ]     numeric_indices = (int(r) if r.isnumeric() else None for r in clean_indices)     return [         texts[i] if i is not None else \"INVALID SOURCE\" for i in numeric_indices     ]   def qanda_langchain(query: str) -> tuple[str, list[str]]:     from langchain.chains.qa_with_sources import load_qa_with_sources_chain     from langchain.embeddings.openai import OpenAIEmbeddings     from langchain.llms import OpenAI     from langchain.text_splitter import CharacterTextSplitter     from langchain.vectorstores.faiss import FAISS      # Support caching speech text on disk.     speech_file_path = Path(\"state-of-the-union.txt\")      if speech_file_path.exists():         state_of_the_union = speech_file_path.read_text()     else:         print(\"scraping the 2022 State of the Union speech\")         state_of_the_union = scrape_state_of_the_union()         speech_file_path.write_text(state_of_the_union)      # We cannot send the entire speech to the model because OpenAI's model     # has a maximum limit on input tokens. So we split up the speech     # into smaller chunks.     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)     print(\"splitting speech into text chunks\")     texts = text_splitter.split_text(state_of_the_union)      # Embedding-based query<->text similarity comparison is used to select     # a small subset of the speech text chunks.     # Generating the `docsearch` index is too slow to re-run on every request,     # so we do rudimentary caching using a global variable.     global docsearch      if not docsearch:         # New OpenAI accounts have a very low rate-limit for their first 48 hrs.         # It's too low to embed even just this single Biden speech.         # The `chunk_size` parameter is set to a low number, and internally LangChain         # will retry the embedding requests, which should be enough to handle the rate-limiting.         #         # Ref: https://platform.openai.com/docs/guides/rate-limits/overview.         print(\"generating docsearch indexer\")         docsearch = FAISS.from_texts(             texts,             OpenAIEmbeddings(chunk_size=5),             metadatas=[{\"source\": i} for i in range(len(texts))],         )      print(\"selecting text parts by similarity to query\")     docs = docsearch.similarity_search(query)      chain = load_qa_with_sources_chain(         OpenAI(model_name=\"gpt-3.5-turbo-instruct\", temperature=0),         chain_type=\"stuff\",     )     print(\"running query against Q&A chain.\\n\")     result = chain(         {\"input_documents\": docs, \"question\": query}, return_only_outputs=True     )     output: str = result[\"output_text\"]     parts = output.split(\"SOURCES: \")     if len(parts) == 2:         answer, sources_refs = parts         sources = retrieve_sources(sources_refs, texts)     elif len(parts) == 1:         answer = parts[0]         sources = []     else:         raise RuntimeError(             f\"Expected to receive an answer with a single 'SOURCES' block, got:\\n{output}\"         )     return answer.strip(), sources Copy Modal Functions  With our application’s functionality implemented we can hook it into Modal. As said above, we’re implementing a web endpoint, web, and a CLI command, cli.  @stub.function() @web_endpoint(method=\"GET\") def web(query: str, show_sources: bool = False):     answer, sources = qanda_langchain(query)     if show_sources:         return {             \"answer\": answer,             \"sources\": sources,         }     else:         return {             \"answer\": answer,         }   @stub.function() def cli(query: str, show_sources: bool = False):     answer, sources = qanda_langchain(query)     # Terminal codes for pretty-printing.     bold, end = \"\\033[1m\", \"\\033[0m\"      print(f\"🦜 {bold}ANSWER:{end}\")     print(answer)     if show_sources:         print(f\"🔗 {bold}SOURCES:{end}\")         for text in sources:             print(text)             print(\"----\") Copy Test run the CLI modal run potus_speech_qanda.py --query \"What did the president say about Justice Breyer\" 🦜 ANSWER: The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy. Copy  To see the text of the sources the model chain used to provide the answer, set the --show-sources flag.  modal run potus_speech_qanda.py \\    --query \"How many oil barrels were released from reserves\" \\    --show-sources=True Copy Test run the web endpoint  Modal makes it trivially easy to ship LangChain chains to the web. We can test drive this app’s web endpoint by running modal serve potus_speech_qanda.py and then hitting the endpoint with curl:  curl --get \\   --data-urlencode \"query=What did the president say about Justice Breyer\" \\   https://modal-labs--example-langchain-qanda-web.modal.run Copy {   \"answer\": \"The president thanked Justice Breyer for his service and mentioned his legacy of excellence. He also nominated Ketanji Brown Jackson to continue in Justice Breyer's legacy.\" } Copy Question-answering with LangChain Defining dependencies Scraping the speech from whitehouse.gov Constructing the Q&A chain Modal Functions Test run the CLI Test run the web endpoint Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/langchains/potus_speech_qanda.py --query 'How many oil barrels were released from reserves' Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/blender_video","title":"Render a video with Blender on GPUs | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Render a video with Blender on GPUs  This example shows how you can render an animated 3D scene using Blender’s Python interface. We use Modal’s GPU workers for this.  Basic setup import os import tempfile  import modal Copy  The S3 locations of the assets we want to render, and the frame ranges.  SCENE_FILENAME = (     \"https://modal-public-assets.s3.amazonaws.com/living_room_cam.blend\" ) MATERIALS_FILENAME = (     \"https://modal-public-assets.s3.amazonaws.com/living_room_final.mtl\" )  START_FRAME = 32 END_FRAME = 34 Copy Defining the image  Blender requires a very custom image in order to run properly. In order to save you some time, we have precompiled the Python packages and stored them in a Dockerhub image.  dockerfile_commands = [     \"RUN export DEBIAN_FRONTEND=noninteractive && \"     \"chown root:root /var /etc /usr /var/lib /var/log / && \"  # needed for some weird systemd error     '    echo \"deb http://deb.debian.org/debian testing main contrib non-free\" > /etc/apt/sources.list.d/testing.list && '     \"    apt update && \"     \"    apt install -yq --no-install-recommends libcrypt1 && \"     \"    apt install -yq --no-install-recommends\"     \"        libgomp1 \"     \"        xorg \"     \"        openbox \"     \"        xvfb \"     \"        libxxf86vm1 \"     \"        libxfixes3 \"     \"        libgl1\",     \"COPY --from=akshatb42/bpy:2.93-gpu\"     \"     /usr/local/lib/python3.9/dist-packages/\"     \"     /usr/local/lib/python3.9/site-packages/\",     \"RUN apt install -yq curl\",     f\"RUN curl -L -o scene.blend -C - '{SCENE_FILENAME}'\",     f\"RUN curl -L -o scene.mtl -C - '{MATERIALS_FILENAME}'\", ] stub = modal.Stub(     \"example-blender-video\",     image=modal.Image.debian_slim(python_version=\"3.9\").dockerfile_commands(         dockerfile_commands     ), ) Copy Setting things up in the containers  We need various global configuration that we want to happen inside the containers (but not locally), such as enabling the GPU device. To do this, we use the stub.image.run.inside() context manager.  with stub.image.imports():     import bpy      # NOTE: Blender segfaults if you try to do this after the other imports.     bpy.ops.wm.open_mainfile(filepath=\"/scene.blend\")     bpy.data.scenes[\"Scene\"].camera = bpy.data.objects.get(\"Camera.001\")      bpy.data.scenes[0].render.engine = \"CYCLES\"      # Set the device_type     bpy.context.preferences.addons[         \"cycles\"     ].preferences.compute_device_type = \"CUDA\"      # Set the device and feature set     bpy.context.scene.cycles.device = \"GPU\"      bpy.context.preferences.addons[\"cycles\"].preferences.get_devices()      for d in bpy.context.preferences.addons[\"cycles\"].preferences.devices:         d[\"use\"] = 1  # Using all devices, include GPU and CPU      print(         \"Has active device:\",         bpy.context.preferences.addons[             \"cycles\"         ].preferences.has_active_device(),     )      bpy.data.scenes[0].render.tile_x = 64     bpy.data.scenes[0].render.tile_y = 64     bpy.data.scenes[0].cycles.samples = 200 Copy Use a GPU from a Modal function  Now, let’s define the function that renders each frame in parallel. Note the gpu=\"any\" argument which tells Modal to use GPU workers.  @stub.function(gpu=\"t4\") def render_frame(i):     print(f\"Using frame {i}\")      scn = bpy.context.scene     scn.render.resolution_x = 400     scn.render.resolution_y = 400     scn.render.resolution_percentage = 100     scn.frame_set(i)      with tempfile.NamedTemporaryFile(suffix=\".png\") as tf:         scn.render.filepath = tf.name         # Render still frame         bpy.ops.render.render(write_still=True)         with open(tf.name, \"rb\") as image:             img_bytes = bytearray(image.read())             return i, img_bytes Copy Entrypoint  The code that gets run locally. Note that it doesn’t require Blender present to run it. In order to render in parallel, we use the .map method on the render_frame function. This spins up as many workers as are needed—as many as one for each frame, doing everything in parallel.  OUTPUT_DIR = \"/tmp/render\"   @stub.local_entrypoint() def main():     os.makedirs(OUTPUT_DIR, exist_ok=True)      # Render the frames in parallel using modal, and write them to disk.     for idx, frame in render_frame.map(range(START_FRAME, END_FRAME + 1)):         with open(os.path.join(OUTPUT_DIR, f\"scene_{idx:03}.png\"), \"wb\") as f:             f.write(frame)      # Stitch together frames into a gif.     import glob      from PIL import Image      img, *imgs = [         Image.open(f)         for f in sorted(glob.glob(os.path.join(OUTPUT_DIR, \"scene*.png\")))     ]     img.save(         fp=os.path.join(OUTPUT_DIR, \"scene.gif\"),         format=\"GIF\",         append_images=imgs,         save_all=True,         duration=200,         loop=0,     ) Copy Render a video with Blender on GPUs Basic setup Defining the image Setting things up in the containers Use a GPU from a Modal function Entrypoint Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/blender/blender_video.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/whisper-transcriber","title":"Example: Parallel podcast transcription using Whisper | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler Example: Parallel podcast transcription using Whisper  OpenAI’s late-September 2022 release of the Whisper speech recognition model was another eye-widening milestone in the rapidly improving field of deep learning, and like others we jumped to try Whisper on podcasts.  The result is the Modal Podcast Transcriber!  This example application is more feature-packed than others, and doesn’t fit in a single page of code and commentary. So instead of progressing through the example’s code linearly, this post provides a higher-level walkthrough of how Modal is used to do fast, on-demand podcast episode transcription for whichever podcast you’d like.  Hour-long episodes transcribed in just 1 minute  The focal point of this demonstration app is that it does serverless CPU transcription across dozens of containers at the click of a button, completing hour-long audio files in just 1 minute.  We use a podcast metadata API to allow users to transcribe an arbitrary episode from whatever niche podcast a user desires — how about The Pen Addict, a podcast dedicated to stationary?.  The video below shows the 45-minute long first episode of Serial season 2 get transcribed in 62 seconds.  What’s extra cool is that each transcription segment has links back to auto-play the original audio. If you read a transcription and wonder, did they really say that? Click the timestamp on the right and find out!  (🎧 Enable sound for this video)  Try it yourself  If you’re itching to see this in action, here are links to begin transcribing the three most popular podcasts on Spotify right now:  Case 63 by Gimlet Media The Joe Rogan Experience The Psychology of your 20s Tech-stack overview  The entire application is hosted serverlessly on Modal and consists of these main components:  A React + Vite single page application (SPA) deployed as static files into a Modal web endpoint. A Modal web endpoint running FastAPI The Podchaser API provides podcast search and episode metadata retrieval. It’s hooked into our code with a Modal Secret. A Modal async job queue, described in more detail below.  All of this is deployed with one command and costs $0.00 when it’s not transcribing podcasts or serving HTTP requests.  Speed-boosting Whisper with parallelism  Modal’s dead-simple parallelism primitives are the key to doing the transcription so quickly. Even with a GPU, transcribing a full episode serially was taking around 10 minutes.  But by pulling in ffmpeg with a simple .pip_install(\"ffmpeg-python\") addition to our Modal Image, we could exploit the natural silences of the podcast medium to partition episodes into hundreds of short segments. Each segment is transcribed by Whisper in its own container task with 2 physical CPU cores, and when all are done we stitch the segments back together with only a minimal loss in transcription quality. This approach actually accords quite well with Whisper’s model architecture:  “The Whisper architecture is a simple end-to-end approach, implemented as an encoder-decoder Transformer. Input audio is split into 30-second chunks, converted into a log-Mel spectrogram, and then passed into an encoder.”  ―Introducing Whisper  Run this app on Modal  All source code for this example can be found on GitHub. The README.md includes instructions on setting up the frontend build and getting authenticated with the Podchaser API. Happy transcribing!  Example: Parallel podcast transcription using Whisper Hour-long episodes transcribed in just 1 minute Try it yourself Tech-stack overview Speed-boosting Whisper with parallelism Run this app on Modal © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/stable_diffusion_xl","title":"Stable Diffusion XL 1.0 | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Stable Diffusion XL 1.0  This example is similar to the Stable Diffusion CLI example, but it generates images from the larger SDXL 1.0 model. Specifically, it runs the first set of steps with the base model, followed by the refiner model.  Try out the live demo here! The first generation may include a cold-start, which takes around 20 seconds. The inference speed depends on the GPU and step count (for reference, an A100 runs 40 steps in 8 seconds).  Basic setup import io from pathlib import Path  from modal import Image, Mount, Stub, asgi_app, build, enter, gpu, method Copy Define a container image  To take advantage of Modal’s blazing fast cold-start times, we’ll need to download our model weights inside our container image with a download function. We ignore binaries, ONNX weights and 32-bit weights.  Tip: avoid using global variables in this function to ensure the download step detects model changes and triggers a rebuild.  sdxl_image = (     Image.debian_slim()     .apt_install(         \"libglib2.0-0\", \"libsm6\", \"libxrender1\", \"libxext6\", \"ffmpeg\", \"libgl1\"     )     .pip_install(         \"diffusers~=0.19\",         \"invisible_watermark~=0.1\",         \"transformers~=4.31\",         \"accelerate~=0.21\",         \"safetensors~=0.3\",     ) )  stub = Stub(\"stable-diffusion-xl\")  with sdxl_image.imports():     import fastapi.staticfiles     import torch     from diffusers import DiffusionPipeline     from fastapi import FastAPI     from fastapi.responses import Response     from huggingface_hub import snapshot_download Copy Load model and run inference  The container lifecycle __enter__ function loads the model at startup. Then, we evaluate it in the run_inference function.  To avoid excessive cold-starts, we set the idle timeout to 240 seconds, meaning once a GPU has loaded the model it will stay online for 4 minutes before spinning down. This can be adjusted for cost/experience trade-offs.  @stub.cls(gpu=gpu.A10G(), container_idle_timeout=240, image=sdxl_image) class Model:     @build()     def build(self):         ignore = [             \"*.bin\",             \"*.onnx_data\",             \"*/diffusion_pytorch_model.safetensors\",         ]         snapshot_download(             \"stabilityai/stable-diffusion-xl-base-1.0\", ignore_patterns=ignore         )         snapshot_download(             \"stabilityai/stable-diffusion-xl-refiner-1.0\",             ignore_patterns=ignore,         )      @enter()     def enter(self):         load_options = dict(             torch_dtype=torch.float16,             use_safetensors=True,             variant=\"fp16\",             device_map=\"auto\",         )          # Load base model         self.base = DiffusionPipeline.from_pretrained(             \"stabilityai/stable-diffusion-xl-base-1.0\", **load_options         )          # Load refiner model         self.refiner = DiffusionPipeline.from_pretrained(             \"stabilityai/stable-diffusion-xl-refiner-1.0\",             text_encoder_2=self.base.text_encoder_2,             vae=self.base.vae,             **load_options,         )          # Compiling the model graph is JIT so this will increase inference time for the first run         # but speed up subsequent runs. Uncomment to enable.         # self.base.unet = torch.compile(self.base.unet, mode=\"reduce-overhead\", fullgraph=True)         # self.refiner.unet = torch.compile(self.refiner.unet, mode=\"reduce-overhead\", fullgraph=True)      @method()     def inference(self, prompt, n_steps=24, high_noise_frac=0.8):         negative_prompt = \"disfigured, ugly, deformed\"         image = self.base(             prompt=prompt,             negative_prompt=negative_prompt,             num_inference_steps=n_steps,             denoising_end=high_noise_frac,             output_type=\"latent\",         ).images         image = self.refiner(             prompt=prompt,             negative_prompt=negative_prompt,             num_inference_steps=n_steps,             denoising_start=high_noise_frac,             image=image,         ).images[0]          byte_stream = io.BytesIO()         image.save(byte_stream, format=\"PNG\")         image_bytes = byte_stream.getvalue()          return image_bytes Copy  And this is our entrypoint; where the CLI is invoked. Explore CLI options with: modal run stable_diffusion_xl.py --prompt 'An astronaut riding a green horse'  @stub.local_entrypoint() def main(prompt: str):     image_bytes = Model().inference.remote(prompt)      dir = Path(\"/tmp/stable-diffusion-xl\")     if not dir.exists():         dir.mkdir(exist_ok=True, parents=True)      output_path = dir / \"output.png\"     print(f\"Saving it to {output_path}\")     with open(output_path, \"wb\") as f:         f.write(image_bytes) Copy A user interface  Here we ship a simple web application that exposes a front-end (written in Alpine.js) for our backend deployment.  The Model class will serve multiple users from a its own shared pool of warm GPU containers automatically.  We can deploy this with modal deploy stable_diffusion_xl.py.  frontend_path = Path(__file__).parent / \"frontend\"   @stub.function(     mounts=[Mount.from_local_dir(frontend_path, remote_path=\"/assets\")],     allow_concurrent_inputs=20, ) @asgi_app() def app():     web_app = FastAPI()      @web_app.get(\"/infer/{prompt}\")     async def infer(prompt: str):         image_bytes = Model().inference.remote(prompt)          return Response(image_bytes, media_type=\"image/png\")      web_app.mount(         \"/\", fastapi.staticfiles.StaticFiles(directory=\"/assets\", html=True)     )      return web_app Copy Stable Diffusion XL 1.0 Basic setup Define a container image Load model and run inference A user interface Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/stable_diffusion/stable_diffusion_xl.py --prompt 'An astronaut riding a green horse' Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/web-scraper","title":"A simple web scraper | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler A simple web scraper  In this guide we’ll introduce you to Modal by writing a simple web scraper. We’ll explain the foundations of a Modal application step by step.  Set up your first Modal app  Modal apps are orchestrated as Python scripts, but can theoretically run anything you can run in a container. To get you started, make sure to install the latest Modal Python package and set up an API token (the first two steps of the Getting started page).  Finding links  First, we create an empty Python file scrape.py. This file will contain our application code. Lets write some basic Python code to fetch the contents of a web page and print the links (href attributes) it finds in the document:  import re import sys import urllib.request   def get_links(url):     response = urllib.request.urlopen(url)     html = response.read().decode(\"utf8\")     links = []     for match in re.finditer('href=\"(.*?)\"', html):         links.append(match.group(1))     return links   if __name__ == \"__main__\":     links = get_links(sys.argv[1])     print(links) Copy  Now obviously this is just pure standard library Python code, and you can run it on your machine:  $ python scrape.py http://example.com ['https://www.iana.org/domains/example'] Copy Running it in Modal  To make the get_links function run in Modal instead of your local machine, all you need to do is  Import modal Create a modal.Stub instance Add a @stub.function() annotation to your function Replace the if __name__ == \"__main__\": block with a function decorated with @stub.local_entrypoint() Call get_links using get_links.remote import re import urllib.request import modal  stub = modal.Stub(name=\"link-scraper\")   @stub.function() def get_links(url):     ...   @stub.local_entrypoint() def main(url):     links = get_links.remote(url)     print(links) Copy  You can now run this with the Modal CLI, using modal run instead of python. This time, you’ll see additional progress indicators while the script is running:  $ modal run scrape.py --url http://example.com ✓ Initialized. ✓ Created objects. ['https://www.iana.org/domains/example'] ✓ App completed. Copy Custom containers  In the code above we make use of the Python standard library urllib library. This works great for static web pages, but many pages these days use javascript to dynamically load content, which wouldn’t appear in the loaded html file. Let’s use the Playwright package to instead launch a headless Chromium browser which can interpret any javascript that might be on the page.  We can pass custom container images (defined using modal.Image) to the @stub.function() decorator. We’ll make use of the modal.Image.debian_slim pre-bundled image add the shell commands to install Playwright and its dependencies:  playwright_image = modal.Image.debian_slim(python_version=\"3.10\").run_commands(     \"apt-get update\",     \"apt-get install -y software-properties-common\",     \"apt-add-repository non-free\",     \"apt-add-repository contrib\",     \"pip install playwright==1.30.0\",     \"playwright install-deps chromium\",     \"playwright install chromium\", ) Copy  Note that we don’t have to install Playwright or Chromium on our development machine since this will all run in Modal. We can now modify our get_links function to make use of the new tools:  @stub.function(image=playwright_image) async def get_links(cur_url: str):     from playwright.async_api import async_playwright      async with async_playwright() as p:         browser = await p.chromium.launch()         page = await browser.new_page()         await page.goto(cur_url)         links = await page.eval_on_selector_all(\"a[href]\", \"elements => elements.map(element => element.href)\")         await browser.close()      print(\"Links\", links)     return links Copy  Since Playwright has a nice async interface, we’ll redeclare our get_links function as async (Modal works with both sync and async functions).  The first time you run the function after making this change, you’ll notice that the output first shows the progress of building the custom image you specified, after which your function runs like before. This image is then cached so that on subsequent runs of the function it will not be rebuilt as long as the image definition is the same.  Scaling out  So far, our script only fetches the links for a single page. What if we want to scrape a large list of links in parallel?  We can do this easily with Modal, because of some magic: the function we wrapped with the @stub.function() decorator is no longer an ordinary function, but a Modal Function object. This means it comes with a map property built in, that lets us run this function for all inputs in parallel, scaling up to as many workers as needed.  Let’s change our code to scrape all urls we feed to it in parallel:  @stub.local_entrypoint() def main():     urls = [\"http://modal.com\", \"http://github.com\"]     for links in get_links.map(urls):         for link in links:             print(link) Copy Schedules and deployments  Let’s say we want to log the scraped links daily. We move the print loop into its own Modal function and annotate it with a modal.Period(days=1) schedule - indicating we want to run it once per day. Since the scheduled function will not run from our command line, we also add a hard-coded list of links to crawl for now. In a more realistic setting we could read this from a database or other accessible data source.  @stub.function(schedule=modal.Period(days=1)) def daily_scrape():     urls = [\"http://modal.com\", \"http://github.com\"]     for links in get_links.map(urls):         for link in links:             print(link) Copy  To deploy this as a permanent app, run the command  modal deploy scrape.py Copy  Running this command deploys this function and then closes immediately. We can see the deployment and all of its runs, including the printed links, on the Modal Apps page. Rerunning the script will redeploy the code with any changes you have made - overwriting an existing deploy with the same name (“link-scraper” in this case).  Integrations and Secrets  Instead of looking at the links in the run logs of our deployments, let’s say we wanted to post them to our #scraped-links Slack channel. To do this, we can make use of the Slack API and the slack-sdk PyPI package.  The Slack SDK WebClient requires an API token to get access to our Slack Workspace, and since it’s bad practice to hardcode credentials into application code we make use of Modal’s Secrets. Secrets are snippets of data that will be injected as environment variables in the containers running your functions.  The easiest way to create Secrets is to go to the Secrets section of modal.com. You can both create a free-form secret with any environment variables, or make use of presets for common services. We’ll use the Slack preset and after filling in the necessary information we are presented with a snippet of code that can be used to post to Slack using our credentials:  import os slack_sdk_image = modal.Image.debian_slim().pip_install(\"slack-sdk\")   @stub.function(image=slack_sdk_image, secret=modal.Secret.from_name(\"my-slack-secret\")) def bot_token_msg(channel, message):     import slack_sdk     client = slack_sdk.WebClient(token=os.environ[\"SLACK_BOT_TOKEN\"])     client.chat_postMessage(channel=channel, text=message) Copy  Copy that code as-is, then amend the daily_scrape function to call bot_token_msg.  @stub.function(schedule=modal.Period(days=1)) def daily_scrape():     urls = [\"http://modal.com\", \"http://github.com\"]     for links in get_links.map(urls):         for link in links:             bot_token_msg.remote(\"scraped-links\", link) Copy  Note that we are freely making function calls across completely different container images, as if they were regular Python functions in the same program.  We rerun the script which overwrites the old deploy with our updated code, and now we get a daily feed of our scraped links in our Slack channel 🎉  Summary  We have shown how you can use Modal to develop distributed Python data applications using custom containers. Through simple constructs we were able to add parallel execution. With the change of a single line of code were were able to go from experimental development code to a deployed application. The full code of this example can be found here. We hope this overview gives you a glimpse of what you are able to build using Modal.  A simple web scraper Set up your first Modal app Finding links Running it in Modal Custom containers Scaling out Schedules and deployments Integrations and Secrets Summary © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/examples/vllm_inference","title":"Fast inference with vLLM (Mistral 7B) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Featured Getting started Hello, world Simple web scraper Large language models (LLMs) Featured: Mixtral 8x7B (vLLM) Inference: TGI Inference: vLLM Inference: MLC Inference: Voice Chat with LLMs Fine-tuning: Training an LLM Fine-tuning: Replace your CEO with an LLM Diffusion models Generate: Stable Diffusion XL 1.0 Generate: Stable Diffusion XL Turbo Image-to-image Generate: ControlNet demos Generate: MusicGen for a Discord bot Fine-tuning: Diffusion models with diffusers Fine-tuning: Pet Art with Dreambooth Parallel processing and job scheduling Podcast transcription using Whisper Face detection on YouTube videos Hacker News Slackbot Document OCR job queue Document OCR web app Connecting to other APIs Google Sheets Langchain: Question-answering Miscellaneous Hosting popular libraries DuckDB: Analyze NYC taxi data in parallel Blender: Distributed 3D rendering Streamlit: Run and deploy Streamlit apps SQLite: Publish explorable data with Datasette Y! Finance: Stock prices in parallel Algolia: Docsearch crawler View on GitHub Fast inference with vLLM (Mistral 7B)  In this example, we show how to run basic inference, using vLLM to take advantage of PagedAttention, which speeds up sequential inferences with optimized key-value caching.  vLLM also supports a use case as a FastAPI server which we will explore in a future guide. This example walks through setting up an environment that works with vLLM for basic inference.  We are running the Mistral 7B Instruct model here, which is an instruct fine-tuned version of Mistral’s 7B model best fit for conversation. You can expect 20 second cold starts and well over 100 tokens/second. The larger the batch of prompts, the higher the throughput. For example, with the 60 prompts below, we can produce 19k tokens in 15 seconds, which is around 1.25k tokens/second.  To run any of the other supported models, simply replace the model name in the download step. You may also need to enable trust_remote_code for MPT models (see comment below)..  Setup  First we import the components we need from modal.  import os  from modal import Image, Secret, Stub, method  MODEL_DIR = \"/model\" BASE_MODEL = \"mistralai/Mistral-7B-Instruct-v0.1\" Copy Define a container image  We want to create a Modal image which has the model weights pre-saved to a directory. The benefit of this is that the container no longer has to re-download the model from Huggingface - instead, it will take advantage of Modal’s internal filesystem for faster cold starts.  Download the weights  Make sure you have created a HuggingFace access token. To access the token in a Modal function, we can create a secret on the secrets page. Now the token will be available via the environment variable named HF_TOKEN. Functions that inject this secret will have access to the environment variable.  We can download the model to a particular directory using the HuggingFace utility function snapshot_download.  Tip: avoid using global variables in this function. Changes to code outside this function will not be detected and the download step will not re-run.  def download_model_to_folder():     from huggingface_hub import snapshot_download     from transformers.utils import move_cache      os.makedirs(MODEL_DIR, exist_ok=True)      snapshot_download(         BASE_MODEL,         local_dir=MODEL_DIR,         token=os.environ[\"HF_TOKEN\"],     )     move_cache() Copy Image definition  We’ll start from a recommended Dockerhub image and install vLLM. Then we’ll use run_function to run the function defined above to ensure the weights of the model are saved within the container image.  image = (     Image.from_registry(         \"nvidia/cuda:12.1.0-base-ubuntu22.04\", add_python=\"3.10\"     )     .pip_install(\"vllm==0.2.5\", \"huggingface_hub==0.19.4\", \"hf-transfer==0.1.4\")     # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.     .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})     .run_function(         download_model_to_folder,         secret=Secret.from_name(\"huggingface-secret\"),         timeout=60 * 20,     ) )  stub = Stub(\"example-vllm-inference\", image=image) Copy The model class  The inference function is best represented with Modal’s class syntax and the __enter__ method. This enables us to load the model into memory just once every time a container starts up, and keep it cached on the GPU for each subsequent invocation of the function.  The vLLM library allows the code to remain quite clean.  @stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface-secret\")) class Model:     def __enter__(self):         from vllm import LLM          # Load the model. Tip: MPT models may require `trust_remote_code=true`.         self.llm = LLM(MODEL_DIR)         self.template = \"\"\"<s>[INST] <<SYS>> {system} <</SYS>>  {user} [/INST] \"\"\"      @method()     def generate(self, user_questions):         from vllm import SamplingParams          prompts = [             self.template.format(system=\"\", user=q) for q in user_questions         ]          sampling_params = SamplingParams(             temperature=0.75,             top_p=1,             max_tokens=800,             presence_penalty=1.15,         )         result = self.llm.generate(prompts, sampling_params)         num_tokens = 0         for output in result:             num_tokens += len(output.outputs[0].token_ids)             print(output.prompt, output.outputs[0].text, \"\\n\\n\", sep=\"\")         print(f\"Generated {num_tokens} tokens\") Copy Run the model  We define a local_entrypoint to call our remote function sequentially for a list of inputs. You can run this locally with modal run vllm_inference.py.  @stub.local_entrypoint() def main():     model = Model()     questions = [         # Coding questions         \"Implement a Python function to compute the Fibonacci numbers.\",         \"Write a Rust function that performs binary exponentiation.\",         \"How do I allocate memory in C?\",         \"What are the differences between Javascript and Python?\",         \"How do I find invalid indices in Postgres?\",         \"How can you implement a LRU (Least Recently Used) cache in Python?\",         \"What approach would you use to detect and prevent race conditions in a multithreaded application?\",         \"Can you explain how a decision tree algorithm works in machine learning?\",         \"How would you design a simple key-value store database from scratch?\",         \"How do you handle deadlock situations in concurrent programming?\",         \"What is the logic behind the A* search algorithm, and where is it used?\",         \"How can you design an efficient autocomplete system?\",         \"What approach would you take to design a secure session management system in a web application?\",         \"How would you handle collision in a hash table?\",         \"How can you implement a load balancer for a distributed system?\",         # Literature         \"What is the fable involving a fox and grapes?\",         \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",         \"Who does Harry turn into a balloon?\",         \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",         \"Describe a day in the life of a secret agent who's also a full-time parent.\",         \"Create a story about a detective who can communicate with animals.\",         \"What is the most unusual thing about living in a city floating in the clouds?\",         \"In a world where dreams are shared, what happens when a nightmare invades a peaceful dream?\",         \"Describe the adventure of a lifetime for a group of friends who found a map leading to a parallel universe.\",         \"Tell a story about a musician who discovers that their music has magical powers.\",         \"In a world where people age backwards, describe the life of a 5-year-old man.\",         \"Create a tale about a painter whose artwork comes to life every night.\",         \"What happens when a poet's verses start to predict future events?\",         \"Imagine a world where books can talk. How does a librarian handle them?\",         \"Tell a story about an astronaut who discovered a planet populated by plants.\",         \"Describe the journey of a letter traveling through the most sophisticated postal service ever.\",         \"Write a tale about a chef whose food can evoke memories from the eater's past.\",         # History         \"What were the major contributing factors to the fall of the Roman Empire?\",         \"How did the invention of the printing press revolutionize European society?\",         \"What are the effects of quantitative easing?\",         \"How did the Greek philosophers influence economic thought in the ancient world?\",         \"What were the economic and philosophical factors that led to the fall of the Soviet Union?\",         \"How did decolonization in the 20th century change the geopolitical map?\",         \"What was the influence of the Khmer Empire on Southeast Asia's history and culture?\",         # Thoughtfulness         \"Describe the city of the future, considering advances in technology, environmental changes, and societal shifts.\",         \"In a dystopian future where water is the most valuable commodity, how would society function?\",         \"If a scientist discovers immortality, how could this impact society, economy, and the environment?\",         \"What could be the potential implications of contact with an advanced alien civilization?\",         # Math         \"What is the product of 9 and 8?\",         \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",         \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",         \"Think through this step by step. Calculate the sum of an arithmetic series with first term 3, last term 35, and total terms 11.\",         \"Think through this step by step. What is the area of a triangle with vertices at the points (1,2), (3,-4), and (-2,5)?\",         \"Think through this step by step. Solve the following system of linear equations: 3x + 2y = 14, 5x - y = 15.\",         # Facts         \"Who was Emperor Norton I, and what was his significance in San Francisco's history?\",         \"What is the Voynich manuscript, and why has it perplexed scholars for centuries?\",         \"What was Project A119 and what were its objectives?\",         \"What is the 'Dyatlov Pass incident' and why does it remain a mystery?\",         \"What is the 'Emu War' that took place in Australia in the 1930s?\",         \"What is the 'Phantom Time Hypothesis' proposed by Heribert Illig?\",         \"Who was the 'Green Children of Woolpit' as per 12th-century English legend?\",         \"What are 'zombie stars' in the context of astronomy?\",         \"Who were the 'Dog-Headed Saint' and the 'Lion-Faced Saint' in medieval Christian traditions?\",         \"What is the story of the 'Globsters', unidentified organic masses washed up on the shores?\",     ]     model.generate.remote(questions) Copy Fast inference with vLLM (Mistral 7B) Setup Define a container image Download the weights Image definition The model class Run the model Try this on Modal!  You can run this example on Modal in 60 seconds.  Create account to run  After creating a free account, install the Modal Python package, and create an API token.  $ pip install modal $ modal setup Copy  Clone the modal-examples repository and run:  $ git clone https://github.com/modal-labs/modal-examples $ cd modal-examples $ modal run 06_gpu_and_ml/vllm_inference.py Copy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/streaming-endpoints","title":"Streaming endpoints | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Streaming endpoints  Modal web endpoints support streaming responses using FastAPI’s StreamingResponse class. This class accepts asynchronous generators, synchronous generators, or any Python object that implements the iterator protocol, and can be used with Modal Functions!  Simple example  This simple example combines Modal’s @web_endpoint decorator with a StreamingResponse object to produce a real-time SSE response.  import time from fastapi.responses import StreamingResponse  from modal import Stub, web_endpoint  stub = Stub()  def fake_event_streamer():     for i in range(10):         yield f\"data: some data {i}\\n\\n\".encode()         time.sleep(0.5)   @stub.function() @web_endpoint() def stream_me():     return StreamingResponse(         fake_event_streamer(), media_type=\"text/event-stream\"     ) Copy  If you serve this web endpoint and hit it with curl, you will see the ten SSE events progressively appear in your terminal over a ~5 second period.  curl --no-buffer https://modal-labs--example-streaming-stream-me.modal.run Copy  The MIME type of text/event-stream is important in this example, as it tells the downstream web server to return responses immediately, rather than buffering them in byte chunks (which is more efficient for compression).  You can still return other content types like large files in streams, but they are not guaranteed to arrive as real-time events.  Streaming responses with .remote  A Modal Function wrapping a generator function body can have its response passed directly into a StreamingResponse. This is particularly useful if you want to do some GPU processing in one Modal Function that is called by a CPU-based web endpoint Modal Function.  from modal import Stub, web_endpoint  stub = Stub()  @stub.function(gpu=\"any\") def fake_video_render():     for i in range(10):         yield f\"data: finished processing some data from GPU {i}\\n\\n\".encode()         time.sleep(1)   @stub.function() @web_endpoint() def hook():     return StreamingResponse(         fake_video_render.remote(), media_type=\"text/event-stream\"     ) Copy Streaming responses with .map and .starmap  You can also combine Modal Function parallelization with streaming responses, enabling applications to service a request by farming out to dozens of containers and iteratively returning result chunks to the client.  from modal import Stub, web_endpoint  stub = Stub()  @stub.function() def map_me(i):     return f\"segment {i}\\n\"   @stub.function() @web_endpoint() def mapped():     return StreamingResponse(map_me.map(range(10)), media_type=\"text/plain\") Copy  This snippet will spread the ten map_me(i) executions across containers, and return each string response part as it completes. By default the results will be ordered, but if this isn’t necessary pass order_outputs=False as keyword argument to the .map call.  Asynchronous streaming  The example above uses a synchronous generator, which automatically runs on its own thread, but in asynchronous applications, a loop over a .map or .starmap call can block the event loop. This will stop the StreamingResponse from returning response parts iteratively to the client.  To avoid this, you can use the .aio() method to convert a synchronous .map into its async version. Also, other blocking calls should be offloaded to a separate thread with asyncio.to_thread(). For example:  @stub.function(gpu=\"any\") @web_endpoint() async def transcribe_video(request):     segments = await asyncio.to_thread(split_video, request)     return StreamingResponse(wrapper(segments), media_type=\"text/event-stream\")   # Notice that this is an async generator. async def wrapper(segments):     async for partial_result in transcribe_video.map.aio(segments):         yield \"data: \" + partial_result + \"\\n\\n\" Copy Further examples Complete code the for the simple examples given above is available in our modal-examples Github repository. An example of streaming ChatGPT responses over HTTP An end-to-end example of streaming Youtube video transcriptions with OpenAI’s whisper model. Streaming endpoints Simple example Streaming responses with .remote Streaming responses with .map and .starmap Asynchronous streaming Further examples See it in action LLM Voice Chat  Text Generation Inference  MLC model  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/project-structure","title":"File and project structure | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints File and project structure Apps spanning multiple files  If you have a project spanning multiple files, you can still use a single Modal Stub to create Modal resources across all of them.  Assume we have a package named pkg with files a.py and b.py that contain functions we want to deploy:  pkg/ ├── __init__.py ├── a.py └── b.py Copy  First create a separate file with your shared Modal resources, such as your stub definition and images:  # pkg/common.py stub = modal.Stub()  image_1 = modal.Image.debian_slim().pip_install(...) image_2 = modal.Image.debian_slim().pip_install(...) Copy  Then, import these definitions from each of your existing project files and decorate any functions you need to deploy:  # pkg/a.py from .common import stub, image_1  @stub.fuction(image=image_1) def f():     ... Copy # pkg/b.py from .common import stub, image_2  @stub.fuction(image=image_2) def g():     ... Copy  Finally, to deploy all of these resources together, make a single deployment file that imports all of the app resources that should be created in one place:  # pkg/deploy.py from .a import f from .b import g Copy  Now you can deploy your app by running modal deploy pkg.deploy from above the pkg directory. Your deployed Modal app will have both f and g.  The final file structure now looks like this:  pkg/ ├── __init__.py ├── common.py ├── a.py ├── b.py └── deploy.py Copy  Tip: you can also make __init__.py your deployment file, which makes deploying a package slightly more convenient. With this, you can deploy your entire project using just modal deploy pkg.  File and project structure Apps spanning multiple files © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/dicts-and-queues","title":"Dicts and queues | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Dicts and queues  Modal provides a variety of distributed objects to enable seamless interactivity and data transfer across different components of a distributed system. Two key objects are dicts and queues, both of which serve specific roles in facilitating communication and data management in your applications.  Modal Dicts  A Dict in Modal provides distributed key-value storage. Much like a standard Python dictionary, it lets you store and retrieve values using keys. However, unlike a regular dictionary, a dict in Modal is shared across all containers of an application and can be accessed and manipulated concurrently from any of them.  import modal  stub = modal.Stub() stub.my_dict = modal.Dict.new()   @stub.local_entrypoint() def main():     stub.my_dict[\"key\"] = \"value\"  # setting a value     value = stub.my_dict[\"key\"]    # getting a value Copy  Dicts in Modal are persisted, which means that the data in the dictionary is stored and can be retrieved later, even after the application is redeployed. They can also be accessed from other Modal functions.  You can store Python values of any type within Dicts, since they’re serialized using cloudpickle.  Modal Queues  A Queue in Modal is a distributed queue-like object. It allows you to add and retrieve items in a first-in-first-out (FIFO) manner. Queues are particularly useful when you want to handle tasks or process data asynchronously, or when you need to pass messages between different components of your distributed system.  import modal  stub = modal.Stub() stub.my_queue = modal.Queue.new()   @stub.local_entrypoint() def main():     stub.my_queue.put(\"some object\")  # adding a value     value = stub.my_queue.get()       # retrieving a value Copy  Similar to Dicts, Queues are also persisted and support values of any type.  Asynchronous calls  Both Dicts and Queues are synchronous by default, but they support asynchronous interaction with the .aio function suffix.  @stub.local_entrypoint() async def main():     await stub.my_queue.put.aio(100)     assert await stub.my_queue.get.aio() == 100      await stub.my_dict.put.aio(\"hello\", 400)     assert await stub.my_dict.get.aio(\"hello\") == 400 Copy  Note that .put and .get are aliases for the overloaded indexing operators on Dicts, and you need them name for asynchronous calls.  Please see the docs on asynchronous functions for more information.  Example: Dict and Queue Interaction  To illustrate how dicts and queues can interact together in a simple distributed system, consider the following example program that crawls the web, starting from wikipedia.org and traversing links to many sites in breadth-first order. The Queue stores pages to crawl, while the Dict is used as a kill switch to stop execution of tasks immediately upon completion.  import queue import sys from datetime import datetime  from modal import Dict, Image, Queue, Stub   stub = Stub() stub.image = Image.debian_slim().pip_install(\"requests\", \"beautifulsoup4\")  stub.signal = Dict.new()  # Used to signal the app to stop stub.queue = Queue.new()  # Stream of URLs that have been crawled   def extract_links(url: str) -> list[str]:     \"\"\"Extract links from a given URL.\"\"\"     import requests     import urllib.parse     from bs4 import BeautifulSoup      resp = requests.get(url, timeout=10)     resp.raise_for_status()     soup = BeautifulSoup(resp.text, \"html.parser\")     links = []     for link in soup.find_all(\"a\"):         links.append(urllib.parse.urljoin(url, link.get(\"href\")))     return links   @stub.function() def crawl_pages(urls: set[str]) -> None:     for url in urls:         if \"stop\" in stub.signal:             return         try:             s = datetime.now()             links = extract_links(url)             print(f\"Crawled: {url} in {datetime.now() - s}, with {len(links)} links\")             stub.queue.put_many(links)         except Exception as exc:             print(f\"Failed to crawl: {url} with error {exc}, skipping...\", file=sys.stderr)   @stub.local_entrypoint() def main():     start_time = datetime.now()      # Initialize queue with a starting URL     stub.queue.put(\"https://www.wikipedia.org/\")      # Crawl until the queue is empty, or reaching some number of URLs     visited = set()     max_urls = 50000     while True:         try:             next_urls = stub.queue.get_many(2000, timeout=5)         except queue.Empty:             break         new_urls = set(next_urls) - visited         visited |= new_urls         if len(visited) < max_urls:             crawl_pages.spawn(new_urls)         else:             stub.signal[\"stop\"] = True      elapsed = (datetime.now() - start_time).total_seconds()     print(f\"Crawled {len(visited)} URLs in {elapsed:.2f} seconds\") Copy  Starting from Wikipedia, this spawns several dozen containers (auto-scaled on demand) to crawl over 200,000 URLs in 40 seconds.  Data durability  Dict and Queue objects are backed by an in-memory database, and thus are not resilient to database server restarts. Dict keys are also subject to expiration, as described by the modal.Dict reference page.  Please get in touch if you need durability for Dict or Queue objects.  Dicts and queues Modal Dicts Modal Queues Asynchronous calls Example: Dict and Queue Interaction Data durability © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/cron","title":"Scheduling remote cron jobs | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Scheduling remote cron jobs  A common requirement is to perform some task at a given time every day or week automatically. Modal facilitates this through function schedules.  Basic scheduling  Let’s say we have a Python module heavy.py with a function, perform_heavy_computation().  # heavy.py def perform_heavy_computation():     ...  @stub.local_entrypoint() def main():     perform_heavy_computation.remote() Copy  To schedule this function to run once per day, we create a Modal Stub and attach our function to it with the @stub.function decorator and a schedule parameter:  # heavy.py import modal  stub = modal.Stub()  @stub.function(schedule=modal.Period(days=1)) def perform_heavy_computation():     ... Copy  To activate the schedule, deploy your app, either through the CLI:  modal deploy --name daily_heavy heavy.py Copy  Or programmatically:   if __name__ == \"__main__\":     modal.runner.deploy_stub(stub) Copy  When you make changes to your function, just rerun the deploy command to overwrite the old deployment.  Monitoring your scheduled runs  To see past execution logs for the scheduled function, go to the Apps section on the Modal web site.  Schedules currently cannot be paused. Instead the schedule should be removed and the app redeployed. Schedules can be started manually on the app’s dashboard page, using the “run now” button.  Schedule types  There are two kinds of base schedule values - modal.Period and modal.Cron.  modal.Period lets you specify an interval between function calls, e.g. Period(days=1) or Period(hours=5):   # runs once every 5 hours @stub.function(schedule=modal.Period(hours=5)) def perform_heavy_computation():     ... Copy  modal.Cron gives you finer control using cron syntax:  # runs at 8 am (UTC) every Monday @stub.function(schedule=modal.Cron(\"0 8 * * 1\")) def perform_heavy_computation():     ... Copy  For more details, see the API reference for Period, Cron and Function  Scheduling remote cron jobs Basic scheduling Monitoring your scheduled runs Schedule types See it in action Hacker News Slackbot  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/trigger-deployed-functions","title":"Invoking deployed functions | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Invoking deployed functions  Modal lets you take a function created by a deployment and call it from other contexts.  There are two ways of invoking deployed functions. If the invoking client is running Python, then the same Modal client library used to write Modal code can be used. HTTPS is used if the invoking client is not running Python and therefore cannot import the Modal client library.  Invoking with Python  Some use cases for Python invocation include:  An existing Python web server (eg. Django, Flask) wants to invoke Modal functions. You have split your product or system into multiple Modal applications that deploy independently and call each other. Function lookup and invocation basics  Let’s say you have a script my_shared_app.py and this script defines a Modal app with a function that computes the square of a number:  import modal  stub = modal.Stub(\"my-shared-app\")   @stub.function() def square(x: int):     return x ** 2 Copy  You can deploy this app to create a persistent deployment:  % modal deploy shared_app.py ✓ Initialized. ✓ Created objects. ├── 🔨 Created square. ├── 🔨 Mounted /Users/erikbern/modal/shared_app.py. ✓ App deployed! 🎉  View Deployment: https://modal.com/apps/erikbern/my-shared-app Copy  Let’s try to run this function from a different context. For instance, let’s fire up the Python interactive interpreter:  % python Python 3.9.5 (default, May  4 2021, 03:29:30) [Clang 12.0.0 (clang-1200.0.32.27)] on darwin Type \"help\", \"copyright\", \"credits\" or \"license\" for more information. >>> import modal >>> f = modal.Function.lookup(\"my-shared-app\", \"square\") >>> f.remote(42) 1764 >>> Copy  This works exactly the same as a regular modal Function object. For example, you can .map() over functions invoked this way too:  >>> f = modal.Function.lookup(\"my-shared-app\", \"square\") >>> f.map([1, 2, 3, 4, 5]) [1, 4, 9, 16, 25] Copy  Importantly, this does not need to use stub.run(), which would create a new ephemeral app unnecessarily.  Lookup of lifecycle functions  Lifecycle functions are defined on classes, and looked up by the concatenation of the class name and function name, separated by a period (.).  stub = modal.Stub(\"my-shared-app\")  @stub.cls() class MyLifecycleClass:     @enter()     def enter(self):         self.var = \"hello world\"      @modal.method()     def foo(self):         return self.var Copy >>> f = modal.Function.lookup(\"my-shared-app\", \"MyLifecycleClass.foo\") >>> f.remote() 'hello world' Copy Asynchronous invocation  In certain contexts, a Modal client will need to trigger Modal functions without waiting on the result. This is done by spawning functions and receiving a FunctionCall as a handle to the triggered execution.  The following is an example of a Flask web server (running outside Modal) which accepts model training jobs to be executed within Modal. Instead of the HTTP POST request waiting on a training job to complete, which would be infeasible, the relevant Modal function is spawned and the FunctionCall object is stored for later polling of execution status.  from uuid import uuid4 from flask import Flask, jsonify, request  app = Flask(__name__) pending_jobs = {}  ...  @app.route(\"/jobs\", methods = [\"POST\"]) def create_job():     fn = modal.Function.lookup(\"example\", \"train_model\")     job_id = str(uuid4())     function_call = predict_fn.spawn(         job_id=job_id,         params=request.json,     )     pending_jobs[job_id] = function_call     return {         \"job_id\": job_id,         \"status\": \"pending\",     } Copy Importing a Modal function between Modal apps  You can also import one function defined in an app from another app:  import modal  stub = modal.Stub(\"another-app\") stub.square = modal.Function.from_name(\"my-shared-app\", \"square\")   @stub.function() def cube(x):     return x * stub.square.remote(x)   @stub.local_entrypoint() def main():     assert cube.remote(42) == 74088 Copy Comparison with HTTPS  Compared with HTTPS invocation, Python invocation has the following benefits:  Avoids the need to create web endpoint functions. Avoids handling serialization of request and response data between Modal and your client. Uses the Modal client library’s built-in authentication. Web endpoints are public to the entire internet, whereas function lookup only exposes your code to you (and your org). You can work with shared Modal functions as if they are normal Python functions, which might be more convenient. Invoking with HTTPS  Any non-Python application client can interact with deployed Modal applications via web endpoint functions.  Anything able to make HTTPS requests can trigger a Modal web endpoint function. Note that all deployed web endpoint functions have a stable HTTPS URL.  Some use cases for HTTPS invocation include:  Calling Modal functions from a web browser client running Javascript Calling Modal functions from non-Python backend services (Java, Go, Ruby, NodeJS, etc) Calling Modal functions using UNIX tools (curl, wget)  However, if the client of your Modal deployment is running Python, it’s better to use the Modal client library to invoke your Modal code.  For more detail on setting up functions for invocation over HTTP see the web endpoints guide.  Invoking deployed functions Invoking with Python Function lookup and invocation basics Lookup of lifecycle functions Asynchronous invocation Importing a Modal function between Modal apps Comparison with HTTPS Invoking with HTTPS © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide#how-does-it-work","title":"Introduction to Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Introduction to Modal  Modal lets you run code in the cloud without having to think about infrastructure.  Features Run any code remotely within seconds. Define container environments in code (or use one of our pre-built backends). Scale up horizontally to thousands of containers. Deploy and monitor persistent cron jobs. Attach GPUs with a single line of code. Serve your functions as web endpoints. Use powerful primitives like distributed dictionaries and queues. Getting started  The nicest thing about all of this is that you don’t have to set up any infrastructure. Just:  Create an account at modal.com Install the modal Python package Set up a token  …and you can start running jobs right away.  Modal is currently Python-only, but we may support other languages in the future.  How does it work?  Modal takes your code, puts it in a container, and executes it in the cloud.  Where does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.  Introduction to Modal Features Getting started How does it work? See it in action Hello, world!  A simple web scraper  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/apps","title":"Apps, stubs, and entrypoints | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Apps, stubs, and entrypoints  Every object in Modal is attached to a Stub. This includes things like functions, secrets, and images. A Stub is a description of how to construct a Modal application. When you run or deploy a Stub, it creates an ephemeral or a deployed App, respectively.  You can view a list of all currently running Apps on the apps page.  Ephemeral apps  An ephemeral app is created when you use the modal run CLI command, or the stub.run method. This creates a temporary app that only exists for the duration of your script.  Ephemeral apps are stopped automatically when the calling program exits, or when the server detects that the client is no longer connected (use --detach in order to keep the app running even after the client exits).  Deployed apps  A deployed app is created using the modal deploy CLI command. The app is persisted indefinitely until you delete it from the web UI. Functions in a deployed app that have an attached schedule will be run on a schedule. Otherwise, you can invoke them manually using web endpoints or Python.  Deployed apps are named via the Stub constructor. Re-deploying an existing App (based on the name) will update it in place.  Entrypoints for ephemeral apps  The code that runs first when you modal run an app is called the entrypoint.  You can register a local entrypoint using the @stub.local_entrypoint() decorator. You can also use a regular Modal function as an entrypoint, in which case only the code in global scope is executed locally.  Argument parsing  If your entrypoint function take arguments with primitive types, modal run automatically parses them as CLI options. For example, the following function can be called with modal run script.py --foo 1 --bar \"hello\":  # script.py  @stub.local_entrypoint() def main(foo: int, bar: str):     some_modal_function.call(foo, bar) Copy Manually specifying an entrypoint  If there is only one local_entrypoint registered, modal run script.py will automatically use it. If you have no entrypoint specified, and just one decorated Modal function, that will be used as a remote entrypoint instead. Otherwise, you can direct modal run to use a specific entrypoint.  For example, if you have a function decorated with @stub.function() in your file:  # script.py  @stub.function() def f():     print(\"Hello world!\")   @stub.function() def g():     print(\"Goodbye world!\")   @stub.local_entrypoint() def main():     f.remote() Copy  Running modal run script.py will execute the main function locally, which would call the f function remotely. However you can instead run modal run script.py::stub.f or modal run script.py::stub.g to execute f or g directly.  Apps, stubs, and entrypoints Ephemeral apps Deployed apps Entrypoints for ephemeral apps Argument parsing Manually specifying an entrypoint © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide#getting-started","title":"Introduction to Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Introduction to Modal  Modal lets you run code in the cloud without having to think about infrastructure.  Features Run any code remotely within seconds. Define container environments in code (or use one of our pre-built backends). Scale up horizontally to thousands of containers. Deploy and monitor persistent cron jobs. Attach GPUs with a single line of code. Serve your functions as web endpoints. Use powerful primitives like distributed dictionaries and queues. Getting started  The nicest thing about all of this is that you don’t have to set up any infrastructure. Just:  Create an account at modal.com Install the modal Python package Set up a token  …and you can start running jobs right away.  Modal is currently Python-only, but we may support other languages in the future.  How does it work?  Modal takes your code, puts it in a container, and executes it in the cloud.  Where does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.  Introduction to Modal Features Getting started How does it work? See it in action Hello, world!  A simple web scraper  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/checkpointing","title":"Checkpointing (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Checkpointing (beta)  Checkpointing improves cold-boot performance by creating a memory checkpoint of your function and then restoring the memory checkpoint when you need it.  Checkpointing happens after your function’s import sequence. During import, your all reads many files from the file system. Some of these files are very large, for instance torch is hundreds of MB. We create a memory checkpoint of your function after it is done importing packages—but before calling for inputs. We save that checkpoint as a file. Then, every time your function is invoked, we restore your function memory. Your function will skip all file reads associated with its imports in favor of a single checkpoint file. The result is increased cold boot performance: functions with checkpointing enabled start 2-3x faster compared to not using checkpoints.  You don’t need to modify your function to take advantage of checkpointing in most cases (see below). This is a beta feature. Let us know in Modal Slack if you find any issues.  Enabling checkpointing  Checkpointing is currently in beta and is available as a flag in the function decorator. You can enable it as follows:  # checkpoint_example.py import modal  stub = modal.Stub(\"checkpointing-example-simple\")  @stub.function(checkpointing_enabled=True) def my_func():     print(\"hello\") Copy  Then deploy the function with modal deploy.  Keep the following in mind when using checkpointing:  Modal will take a memory checkpoint after your function runs the first few times, typically 2-5 times. Creating checkpoints adds latency to a function start time, so expect your function to be slower to start during the first invocations. Subsequent runs are much faster. Only functions from deployed apps are checkpointable. Ephemeral apps are not checkpointable at this time (coming soon). Checkpointing imports() state  In addition to imported packages, you can also checkpoint arbitrary memory objects. This is useful when you need to load model weights into memory, read large files, etc. For example:  # checkpoint_example_imports.py import sys import modal  image = modal.Image.debian_slim().pip_install(\"torch\") stub = modal.Stub(\"checkpointing-example-run-inside\", image=image)  def memory_size(tensor):     memory_size = sys.getsizeof(large_tensor.untyped_storage())     memory_size_mb = memory_size / (1024 * 1024)     return memory_size_mb  with stub.image.imports():     import torch      # Create a 100MB PyTorch Tensor     # 100MB = 100 * 1024 * 1024 bytes      num_elements = 100 * 1024 * 1024 // 4  # using 32-bit (4 byte) floats     large_tensor = torch.empty(num_elements, dtype=torch.float32)  @stub.function(checkpointing_enabled=True) def my_func():     size = memory_size(large_tensor)     return size Copy  Deploy this program with modal deploy checkpoint_example_imports.py, then invoke it in a different program (see Trigger deployed functions). For example:  # checkpoint_example_imports_invoke.py import modal  f = modal.Function.lookup(\"checkpointing-example-run-inside\", \"my_func\") size = f.remote() print(f\"large PyTorch Tensor with size => {size}mb\") Copy python checkpoint_example_imports_invoke.py Copy  Compare start up results in the web UI with checkpointing enabled and disabled for a clearer understanding of results.  GPU functions  Checkpointing also works with GPUs. This is especially useful for improving cold boot performance for programs that need to model weights from files. One can load model weights into CPU memory then create a memory checkpoint. When a function starts, you then move the model and weights to the GPU. Moving weights is much faster than reading files, greatly improving cold boot times.  Here’s a simple example using bert-large-uncased and transformers.  # checkpoint_example_gpu.py # # Make sure to have numpy installed. # pip install numpy import modal import time  def _download_model():     transformers.AutoTokenizer.from_pretrained('bert-large-uncased')     transformers.BertModel.from_pretrained('bert-large-uncased')   image = (     modal.Image.debian_slim()         .pip_install(\"transformers\", \"torch\")         .run_function(_download_model) ) stub = modal.Stub(\"checkpointing-example-gpu\", image=image)  with stub.image.imports():     import transformers      # Load bert-large-uncased into CPU memory.     # We will create a memory checkpoint of the model     # in memory.     start = time.time()     tokenizer = transformers.AutoTokenizer.from_pretrained('bert-large-uncased')     model = transformers.BertModel.from_pretrained('bert-large-uncased')     model.to(\"cpu\")     end = time.time()      print(f\"loading model took => {end - start}\")   @stub.function(gpu=modal.gpu.A10G(), checkpointing_enabled=True) def my_func(sentence:str='Hello World!'):      # Move model from CPU memory to the GPU. Moving is much faster     # than reading model files from file system.     start = time.time()     inputs = tokenizer(sentence, return_tensors=\"pt\").to(\"cuda:0\")     model.to(\"cuda:0\")     end = time.time()     print(f\"moving model to gpu took => {end - start}\")      outputs = model(**inputs)     return outputs[0].detach().cpu().numpy() Copy  Now deploy your function with modal deploy checkpoint_example_gpu.py and invoke it in a separate program:  # checkpoint_example_gpu_invoke.py import modal  f = modal.Function.lookup(\"checkpointing-example-gpu\", \"my_func\") out = f.remote() print(out) Copy $ python checkpoint_example_gpu_invoke.py [[[-0.36640385 -0.24137188 -0.6954788  ... -1.0166625  -0.8883544     0.33047855]   [-0.3282281   0.08182691 -0.3306965  ... -0.8255221  -0.4351501     0.0962218 ]   [-0.8809221  -0.5154405  -0.6154     ... -0.7522941   0.00946673     0.49462798]   [-0.24658898 -0.07173603 -0.10466571 ... -0.4261331  -0.18401104     0.36228186]   [-0.7935802   0.92049414 -0.25087643 ... -0.7505677  -0.12453364     0.35892618]]] Copy Checkpoint compatibility  Modal will create a memory checkpoint for every new version of your function. Deploying your function anew, even without code changes, will trigger a new checkpointing operation when you run your function.  Additionally, you may see your function being checkpointed multiple times during your first few invocations. That happens because Modal will create a memory checkpoint for every CPU type and runtime version in our fleet. We typically only need 1-3 checkpoints to cover our entire fleet, though. The cold boot benefits should greatly outweight the penalty of creating multiple checkpoints.  Known limitations  Checkpointing is still in beta, please report any issues in Modal Slack.  No internet availability  We have disabled internet access in order to prevent checkpointing errors so your functions will have no internet access during the checkpointing phase.  In practice, this means that your import sequence and imports() code can’t require an internet connection. You will see an error if it does.  If you need to download files from the internet and then load them into memory, you can do so by calling a function when building your image and then accessing those files in imports(). Refer to GPU functions for a practical example.  No GPUs available during checkpointing  It’s currently not possible to checkpoint GPU memory. We avoid exposing GPU devices to your function during the checkpointing stage. NVIDIA drivers are available, but no GPU devices are. This can be a problem if you need the GPU — for example, you may need to compile a package. We suggest using the Image.run_function method and store outputs in disk as part of your image. You can then load these into CPU memory and successfully checkpoint your function. Then, when invoking your function, you can move objects to GPU memory (see GPU functions) for more details.  Checkpointing (beta) Enabling checkpointing Checkpointing imports() state GPU functions Checkpoint compatibility Known limitations No internet availability No GPUs available during checkpointing © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/environments","title":"Environments (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Environments (beta)  Environments are sub-divisons of workspaces, allowing you to deploy the same app (or set of apps) in multiple instances for different purposes without changing your code. Typical use cases for environments include having one dev environment and one prod environment, preventing overwriting production apps when developing new features, while still being able to deploy changes to a “live” and potentially complex structure of apps.  Each environment has its own set of Secrets and any object lookups performed from an app in an environment will by default look for objects in the same environment.  By default, every workspace has a single environment called “main”. New environments can be created on the CLI:  modal environment create dev Copy  (You can run modal environment --help for more info)  Once created, environments show up as a dropdown menu in the navbar of the [/home](Modal dashboard), letting you set browse all Modal apps and Secrets filtered by which environment they were deployed to.  Most CLI commands also support an --env flag letting you specify which environment you intend to interact with, e.g.:  modal run --env=dev app.py modal nfs create --env=dev storage Copy  Note that if you have multiple environments in your workspace and try to interact with it without specifying an environment, an error will be raised.  To set a default environment for your current CLI profile you can use modal config set-environment, e.g.:  modal config set-environment dev Copy  Alternatively, you can set the MODAL_ENVIRONMENT environment variable.  Cross environment lookups  It’s possible to explicitly look up objects from other environments than your app runs in:  production_secret = modal.Secret.from_name(     \"my-secret\",     environment_name=\"main\" )  modal.Function.lookup(     \"my_app\",     \"some_function\",     environment_name=\"dev\" ) Copy  However, the environment_name argument is optional and omitting it will use the environment from the object’s associated app or calling context.  Environments (beta) Cross environment lookups © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/timeouts","title":"Timeouts | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Timeouts  All Modal Function executions have a default execution timeout of 300 seconds (5 minutes), but users may specify timeout durations between 10 seconds and 24 hours.  import time   @stub.function() def f():     time.sleep(599)  # Timeout!   @stub.function(timeout=600) def f():     time.sleep(599)     print(\"*Just* made it!\") Copy  The timeout duration is a measure of a Function’s execution time. It does not include scheduling time or any other period besides the time your code is executing in Modal. This duration is also per execution attempt, meaning Functions configured with modal.Retries will start new execution timeouts on each retry. For example, an infinite-looping Function with a 100 second timeout and 3 allowed retries will run for least 400 seconds within Modal.  Handling timeouts  After exhausting any specified retries, a timeout in a Function will produce a modal.exception.FunctionTimeoutError which you may catch in your code.  import modal.exception   @stub.function(timeout=100) def f():     time.sleep(200)  # Timeout!   @stub.local_entrypoint() def main():     try:         f.remote()     except modal.exception.FunctionTimeoutError:         # Handle the timeout. Copy Timeout accuracy  Functions will run for at least as long as their timeout allows, but they may run a handful of seconds longer. If you require accurate and precise timeout durations on your Function executions, it is recommended that you implement timeout logic in your user code.  Timeouts Handling timeouts Timeout accuracy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/cold-start","title":"Cold start performance | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Cold start performance  For a deployed function or a web endpoint, Modal will spin up as many containers as needed to handle the current number of concurrent requests. Starting up containers incurs a cold-start time of ~1s. Any logic in global scope (such as imports) and the container enter function will be executed next. In the case of loading large models from the image, this can take a few seconds depending on the size of the model, because the file will be copied over the network to the worker running your job.  After the cold-start, subsequent requests to the same container will see lower response latency (~50-200ms), until the container is shut down after a period of inactivity. Modal currently exposes two parameters to control how many cold starts users experience: container_idle_timeout and keep_warm.  Container idle timeout  By default, Modal containers spin down after 60 seconds of inactivity. This can be overridden explicitly by setting the container_idle_timeout value on the @function decorator. This can be set to any integer value between 2 and 1200, and is measured in seconds.  import modal  stub = modal.Stub()  @stub.function(container_idle_timeout=300) def my_idle_f():     return {\"hello\": \"world\"} Copy Warm pool  If you want to have some containers running at all times to mitigate the cold-start penalty, you could set the keep_warm value on the @function decorator. This configures a given minimum number of containers that will always be up for your function, but Modal will still scale up (and spin down) more containers if the demand for your function exceeds the keep_warm value, as usual.  from modal import Stub, web_endpoint  stub = Stub()  @stub.function(keep_warm=3) @web_endpoint() def my_warm_f():     return {\"hello\": \"world\"} Copy Functions with slow start-up and keep_warm  The guarantee that keep_warm provides is that there are always at least n containers up that have finished starting up. If your function does expensive / slow initialization the first time it receives an input (e.g. if you use a pre-trained model, and this model needs to be loaded into memory the first time you use it), you’d observe that those function calls will still be slow.  To avoid this, you can use a container enter method to perform the expensive initialization. This will ensure that the initialization is performed before the container is deemed ready for the warm pool.  Memory checkpointing  Checkpointing is a developer preview feature that can significantly reduce cold start times. Refer to the page Checkpointing for details.  Cold start performance Container idle timeout Warm pool Functions with slow start-up and keep_warm Memory checkpointing © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide#features","title":"Introduction to Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Introduction to Modal  Modal lets you run code in the cloud without having to think about infrastructure.  Features Run any code remotely within seconds. Define container environments in code (or use one of our pre-built backends). Scale up horizontally to thousands of containers. Deploy and monitor persistent cron jobs. Attach GPUs with a single line of code. Serve your functions as web endpoints. Use powerful primitives like distributed dictionaries and queues. Getting started  The nicest thing about all of this is that you don’t have to set up any infrastructure. Just:  Create an account at modal.com Install the modal Python package Set up a token  …and you can start running jobs right away.  Modal is currently Python-only, but we may support other languages in the future.  How does it work?  Modal takes your code, puts it in a container, and executes it in the cloud.  Where does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.  Introduction to Modal Features Getting started How does it work? See it in action Hello, world!  A simple web scraper  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/preemption","title":"Preemption | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Preemption  All Modal functions are subject to preemption. If a preemption event interrupts a running function, Modal will gracefully terminate the function and restart it on the same input.  Preemptions are rare, but it is always possible that your function is interrupted. Long-running functions such as model training functions should take particular care tolerate interruptions as likelihood of interruption increases with function runtime.  Preparing for interruptions  Design your applications to be fault and preemption tolerant. Modal will send an interrupt signal (SIGINT) to your application when preemption occurs. In Python applications, this signal is by default propogated as a KeyboardInterrupt, which you can handle in your code to perform cleanup.  Other best practices for handling preemptions include:  Divide long-running operations into small tasks or use checkpoints so that you can save your work frequently. Ensure preemptible operations are safely retryable (ie. idempotent). Running uninterruptible functions  We currently don’t have a way for functions to avoid the possibility of interruption, but it’s a planned feature. If you require functions guaranteed to run without interruption, please reach out!  Preemption Preparing for interruptions Running uninterruptible functions © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/managing-deployments","title":"Managing deployments | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Managing deployments  Once you’ve finished using modal run or modal serve to iterate on your Modal code, it’s time to deploy. A Modal deployment creates and then persists an application and its objects, providing the following benefits:  Repeated application function executions will be grouped under the deployment, aiding observability and usage tracking. Programmatically triggering lots of ephemeral app runs can clutter your web and CLI interfaces. Function calls are much faster because deployed functions are persistent and reused, not created on-demand by calls. Learn how to trigger deployed functions in Invoking deployed functions. Scheduled functions will continue scheduling separate from any local iteration you do, and will notify you on failure. Web endpoints keep running when you close your laptop, and their URL address matches the deployment name. Creating deployments  Deployments are created using the modal deploy command.   % modal deploy whisper_pod_transcriber.main ✓ Initialized. View app page at https://modal.com/apps/ap-PYc2Tb7JrkskFUI8U5w0KG. ✓ Created objects. ├── 🔨 Created populate_podcast_metadata. ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber at /root/whisper_pod_transcriber ├── 🔨 Created fastapi_app => https://modal-labs-whisper-pod-transcriber-fastapi-app.modal.run ├── 🔨 Mounted /home/ubuntu/whisper_pod_transcriber/whisper_frontend/dist at /assets ├── 🔨 Created search_podcast. ├── 🔨 Created refresh_index. ├── 🔨 Created transcribe_segment. ├── 🔨 Created transcribe_episode.. └── 🔨 Created fetch_episodes. ✓ App deployed! 🎉  View Deployment: https://modal.com/apps/modal-labs/whisper-pod-transcriber Copy  Running this command on an existing deployment will redeploy the app, incrementing its version. For detail on how live deployed apps transition between versions, see the Updating deployments section.  Deployments can also be created using Modal’s client library.  Viewing deployments  Deployments can be viewed either on the apps web page or by using the modal app list command.  Updating deployments  A deployment can deploy new apps or redeploy new versions of an existing deployed app. It’s useful to understand how Modal handles the transition between versions of running deployment. Modal deployments only take a few seconds, but we still ensure things run smoothly in this short deployment period.  A running deployed app will continue running and accepting requests while a deployment is happening. Existing function executions will also keep running. They will not be terminated by the deployment because they’re outdated.  However, any existing container running the old version of the app will be marked by Modal as outdated and will become ineligible to serve new requests. These outdated containers will become idle and gracefully terminate.  Any warm pool containers will also be cycled during a deployment, as the previous version’s warm pool are now outdated.  Stopping deployments  Deployed apps can be stopped in the web UI by clicking the red delete button on the deployment’s app page, or alternatively by using the modal app stop command.  Stopped deployments are eventually garbage collected.  Managing deployments Creating deployments Viewing deployments Updating deployments Stopping deployments © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/network-file-systems","title":"Network file systems | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Network file systems  Modal lets you create writeable volumes that can be simultaneously attached to multiple Modal functions. These are helpful for use cases such as:  Storing datasets Keeping a shared cache for expensive computations Leveraging POSIX filesystem APIs for both local and remote data storage Basic example  The modal.NetworkFileSystem constructor initializes an empty volume. This can be mounted within a function by providing a mapping between mount paths and NetworkFileSystem objects. For example, to use a NetworkFileSystem to initialize a shared shelve disk cache:  import shelve import modal  volume = modal.NetworkFileSystem.new()  @stub.function(network_file_systems={\"/root/cache\": volume}) def expensive_computation(key: str):     with shelve.open(\"/root/cache/shelve\") as cache:         cached_val = cache.get(key)      if cached_val is not None:         return cached_val      # cache miss; populate value     ... Copy  The above implements basic disk caching, but be aware that shelve does not guarantee correctness in the event of concurrent read/write operations. To protect against concurrent write conflicts, the flufl.lock package is useful. An example of that library’s usage is in the Datasette example.  Persisting volumes  By default, a modal.NetworkFileSystem lives as long as the app it’s defined in, just like any other Modal object. However in many situations you might want to persist file data between runs of the app. To do this, you can use the persisted method on the NetworkFileSystem object. For example, to durably store trained model checkpoints when running a model training job:  import modal  volume = modal.NetworkFileSystem.persisted(\"job-storage-vol\")  stub = modal.Stub()  MODEL_DIR = \"/models\"  @stub.function(     network_file_systems={MODEL_DIR: volume}, ) def run_training():     ...     ...     trainer.save(MODEL_DIR) Copy Deleting volumes  To remove a persisted network file system, deleting all its data, you must “stop” it. This can be done via the network file system’s dashboard app page or the CLI.  For example, a file system with the name my-vol that lives in the e-corp workspace could be stopped (i.e. deleted) by going to its dashboard page at https://modal.com/apps/e-corp/my-vol and clicking the trash icon. Alternatively, you can use the file system’s app ID with modal app stop.  (Network File Systems are currently a specialized app type within Modal, which is why deleting one is done by stopping an app.)  Further examples The Modal Podcast Transcriber uses a persisted network file system to durably store raw audio, metadata, and finished transcriptions. Network file systems Basic example Persisting volumes Deleting volumes Further examples © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/webhooks","title":"Web endpoints | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Web endpoints  Modal gives you a few ways to expose functions as web endpoints. You can turn any Modal function into a web endpoint with a single line of code, or you can serve a full app using a framework like FastAPI, Django, or Flask.  All web endpoints on Modal have a limit of 300 requests per second (rps). Get in touch if you need higher limits.  Note that if you wish to invoke a Modal function from another Python application, you can deploy and invoke the function directly with our client library.  @web_endpoint  The easiest way to create a web endpoint from an existing function is to use the @modal.web_endpoint decorator.  from modal import Stub, web_endpoint  stub = Stub()  @stub.function() @web_endpoint() def f():     return \"Hello world!\" Copy  This decorator wraps the Modal function in a FastAPI application.  Developing with modal serve  You can run this code as an ephemeral app, by running the command  modal serve server_script.py Copy  Where server_script.py is the file name of your code. This will create an ephemeral app for the duration of your script (until you hit Ctrl-C to stop it). It creates a temporary URL that you can use like any other REST endpoint. This URL is on the public internet.  The modal serve command will live-update an app when any of its supporting files change.  Live updating is particularly useful when working with apps containing web endpoints, as any changes made to web endpoint handlers will show up almost immediately, without requiring a manual restart of the app.  Deploying a web server  You can also deploy your app and create a persistent web endpoint in the cloud by running modal deploy:  Passing arguments to web endpoints  When using @web_endpoint, you can use query parameters just like in FastAPI which will be passed to your function as arguments. For instance  from modal import Stub, web_endpoint  stub = Stub()  @stub.function() @web_endpoint() def square(x: int):     return {\"square\": x**2} Copy  If you hit this with an urlencoded query string with the “x” param present, it will send that to the function:  % curl 'https://modal-labs--web-endpoint-get-py-square-erikbern-dev.modal.run?x=42' {\"square\":1764} Copy  If you want to use a POST request, you can use the method argument to @web_endpoint to set the HTTP verb. To accept any valid JSON, you can use Dict as your type annotation and FastAPI will handle the rest.  from typing import Dict  from modal import Stub, web_endpoint  stub = Stub()  @stub.function() @web_endpoint(method=\"POST\") def square(item: Dict):     return {\"square\": item['x']**2} Copy  This now creates an endpoint that lets us hit it using JSON:  % curl 'https://modal-labs--web-endpoint-post-py-square-erikbern-dev.modal.run' -X POST -H 'Content-Type: application/json' -d '{\"x\": 42}' {\"square\":1764} Copy  This is often the easiest way to get started, but note that FastAPI recommends that you use typed Pydantic models in order to get automatic validation and documentation. FastAPI also lets you pass data to web endpoints in other ways, for instance as form data and file uploads.  How do web endpoints run in the cloud?  Note that web endpoints, like everything else on Modal, only run when they need to. When you hit the web endpoint the first time, it will boot up the container, which might take a few seconds. Modal keeps the container alive for a short period in case there are subsequent requests. If there are a lot of requests, Modal might create more containers running in parallel.  Under the hood, Modal wraps your function in a FastAPI application, and so functions you write need to follow the same request and response semantics. This also means you can use all of FastAPI’s powerful features, such as Pydantic models for automatic validation, typed query and path parameters, and response types.  For long running web endpoints (taking more than 150s to complete), Modal by default uses chains of HTTP redirects to keep each request reasonably short lived. For more information see Web endpoint timeouts.  More complex example  Here’s everything together, combining Modal’s abilities to run functions in user-defined containers with the expressivity of FastAPI:  from pydantic import BaseModel from fastapi.responses import HTMLResponse  from modal import Image, Stub, web_endpoint  image = Image.debian_slim().pip_install(\"boto3\") stub = Stub(image=image)   class Item(BaseModel):     name: str     qty: int = 42   @stub.function() @web_endpoint(method=\"POST\") def f(item: Item):     import boto3     # do things with boto3...     return HTMLResponse(f\"<html>Hello, {item.name}!</html>\") Copy  This endpoint definition would be called like so:  curl -d '{\"name\": \"Erik\", \"qty\": 10}' \\     -H \"Content-Type: application/json\" \\     -X POST https://ecorp--web-demo-f-dev.modal.run Copy  Or in Python with the requests library:  import requests  data = {\"name\": \"Erik\", \"qty\": 10} requests.post(\"https://ecorp--web-demo-f-dev.modal.run\", json=data, timeout=10.0) Copy Serving ASGI and WSGI apps  You can also serve any app written in an ASGI or WSGI compatible web application framework on Modal.  ASGI provides support for async web applications. WSGI provides support for synchronous web applications.  ASGI  For ASGI apps, you can create a function decorated with @modal.asgi_app that returns a reference to your web app:  from fastapi import FastAPI, Request from fastapi.responses import HTMLResponse  from modal import Image, Stub, asgi_app  web_app = FastAPI() stub = Stub()  image = Image.debian_slim().pip_install(\"boto3\")   @web_app.post(\"/foo\") async def foo(request: Request):     body = await request.json()     return body   @web_app.get(\"/bar\") async def bar(arg=\"world\"):     return HTMLResponse(f\"<h1>Hello Fast {arg}!</h1>\")   @stub.function(image=image) @asgi_app() def fastapi_app():     return web_app Copy  Now, as before, when you deploy this script as a modal app, you get a URL for your app that you can use:  WSGI  You can serve WSGI apps using the @modal.wsgi_app decorator:  from modal import Image, Stub, wsgi_app  stub = Stub() image = Image.debian_slim().pip_install(\"flask\")   @stub.function(image=image) @wsgi_app() def flask_app():     from flask import Flask, request      web_app = Flask(__name__)      @web_app.get(\"/\")     def home():         return \"Hello Flask World!\"      @web_app.post(\"/echo\")     def echo():         return request.json      return web_app Copy  See Flask’s docs for more information on using Flask as a WSGI app.  WebSockets  Functions annotated with @web_endpoint, @asgi_app, or @wsgi_app also support the WebSocket protocol. Consult your web framework for appropriate documentation on how to use WebSockets with that library.  WebSockets on Modal maintain a single function call per connection, which can be useful for keeping state around. Most of the time, you will want to set your handler function to allow concurrent inputs, which allows multiple simultaneous WebSocket connections to be handled by the same container.  We support the full WebSocket protocol as per RFC 6455, but we do not yet have support for RFC 8441 (WebSockets over HTTP/2) or RFC 7692 (permessage-deflate extension). WebSocket messages can be up to 2 MiB each.  Cold start performance  Consult the guide page on cold start performance for more information on when functions incur cold start penalties, and how to mitigate the impact of them.  Authentication  Modal doesn’t have an first class way to add authentication to web endpoints yet. However, we support standard techniques for securing web servers.  Token-based authentication  This is easy to implement in whichever framework you’re using. For example, if you’re using @modal.web_endpoint or @modal.asgi_app with FastAPI, you can validate a Bearer token like this:  from fastapi import Depends, HTTPException, status, Request from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials  from modal import Secret, Stub, web_endpoint   stub = Stub(\"auth-example\")  auth_scheme = HTTPBearer()   @stub.function(secret=Secret.from_name(\"my-web-auth-token\")) @web_endpoint() async def f(request: Request, token: HTTPAuthorizationCredentials = Depends(auth_scheme)):     import os      print(os.environ[\"AUTH_TOKEN\"])      if token.credentials != os.environ[\"AUTH_TOKEN\"]:         raise HTTPException(             status_code=status.HTTP_401_UNAUTHORIZED,             detail=\"Incorrect bearer token\",             headers={\"WWW-Authenticate\": \"Bearer\"},         )      # Function body     return \"success!\" Copy  This assumes you have a Modal secret named my-web-auth-token created, with contents {AUTH_TOKEN: secret-random-token}. Now, your endpoint will return a 401 status code except when you hit it with the correct Authorization header set (note that you have to prefix the token with Bearer ):  curl --header \"Authorization: Bearer secret-random-token\" https://modal-labs--auth-example-f.modal.run Copy Client IP address  You can access the IP address of the client making the request. This can be used for geolocation, whitelists, blacklists, and rate limits.  from modal import Stub, web_endpoint from fastapi import Request  stub = Stub()   @stub.function() @web_endpoint() def get_ip_address(request: Request):     return f\"Your IP address is {request.client.host}\" Copy Custom domains  Custom domains are available on our Organization and Enterprise plans.  You can use your own domain names with Modal web endpoints. If your plan supports custom domains, visit the Domains tab in your workspace settings to add a domain name to your workspace.  You can use three kinds of domains with Modal:  Apex: root domain names like example.com Subdomain: single subdomain entries such as my-app.example.com, api.example.com, etc. Wildcard domain: either in a subdomain like *.example.com, or in a deeper level like *.modal.example.com  You’ll be asked to update your domain DNS records with your domain name registrar and then validate the configuration in Modal. Once the records have been properly updated and propagated, your custom domain will be ready to use.  You can assign any Modal web endpoint to any registered domain in your workspace with the custom_domains argument.  from modal import Stub, web_endpoint  stub = Stub(\"custom-domains-example\")   @stub.function() @web_endpoint(custom_domains=[\"api.example.com\"]) def hello(message: str):     return {\"message\": f\"hello {message}\"} Copy  You can then run modal deploy to put your web endpoint online, live.  $ curl -s https://api.example.com?message=world {\"message\": \"hello world\"} Copy  Note that Modal automatically generates and renews TLS certificates for your custom domains. Since we do this when your domain is first accessed, there may be an additional 1-2s latency on the first request. Additional requests use a cached certificate.  You can also register multiple domain names and associate them with the same web endpoint.  from modal import Stub, web_endpoint  stub = Stub(\"custom-domains-example-2\")   @stub.function() @web_endpoint(custom_domains=[\"api.example.com\", \"api.example.net\"]) def hello(message: str):     return {\"message\": f\"hello {message}\"} Copy  For Wildcard domains, Modal will automatically resolve arbitrary custom endpoints (and issue TLS certificates). For example, if you add the wildcard domain *.example.com, then you can create any custom domains under example.com:  import random from modal import Stub, web_endpoint  stub = Stub(\"custom-domains-example-2\")  random_domain_name = random.choice(range(10))   @stub.function() @web_endpoint(custom_domains=[f\"{random_domain_name}.example.com\"]) def hello(message: str):     return {\"message\": f\"hello {message}\"} Copy  Custom domains can also be used with ASGI or WSGI apps using the same custom_domains argument.  Web endpoints @web_endpoint Developing with modal serve Deploying a web server Passing arguments to web endpoints How do web endpoints run in the cloud? More complex example Serving ASGI and WSGI apps ASGI WSGI WebSockets Cold start performance Authentication Token-based authentication Client IP address Custom domains Fully featured web apps LLM Voice Chat (React)  Stable Diffusion (Alpine)  Music Generation (React)  Whisper Podcast Transcriber (React)  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/troubleshooting","title":"Troubleshooting | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Troubleshooting “Command not found” errors  If you installed Modal but you’re seeing an error like modal: command not found when trying to run the CLI, this means that the installation location of Python package executables (“binaries”) are not present on your system path. This is a common problem; you need to reconfigure your system’s environment variables to fix it.  One workaround is to use python -m modal.cli instead of modal. However, this is just a patch. There’s no single solution for the problem because Python installs dependencies on different locations depending on your environment. See this popular StackOverflow question for pointers on how to resolve your system path issue.  Custom types defined in __main__  Modal currently uses cloudpickle to transfer objects returned or exceptions raised by functions that are executed in Modal. This gives a lot of flexibility and support for custom data types.  However, any types that are declared in your Python entrypoint file (The one you call on the command line) will currently be redeclared if they are returned from Modal functions, and will therefore have the same structure and type name but not maintain class object identity with your local types. This means that you can’t catch specific custom exception classes:  import modal stub = modal.Stub()  class MyException(Exception):     pass  @stub.function() def raise_custom():     raise MyException()  @stub.local_entrypoint() def main():     try:         raise_custom.remote()     except MyException:  # this will not catch the remote exception         pass     except Exception:  # this will catch it instead, as it's still a subclass of Exception         pass Copy  Nor can you do object equality checks on dataclasses, or isinstance checks:  import modal import dataclasses  @dataclasses.dataclass class MyType:     foo: int  stub = modal.Stub()  @stub.function() def return_custom():     return MyType(foo=10)   @stub.local_entrypoint() def main():     data = return_custom.remote()     assert data == MyType(foo=10)  # false!     assert data.foo == 10  # true!, the type still has the same fields etc.     assert isinstance(data, MyType)  # false! Copy  If this is a problem for you, you can easily solve it by moving your custom type definitions to a separate Python file from the one you trigger to run your Modal code, and import that file instead.  # File: my_types.py import dataclasses  @dataclasses.dataclass class MyType:     foo: int Copy # File: modal_script.py import modal from my_types import MyType  stub = modal.Stub()  @stub.function() def return_custom():     return MyType(foo=10)  @stub.local_entrypoint() def main():     data = return_custom.remote()     assert data == MyType(foo=10)  # true!     assert isinstance(data, MyType)  # true! Copy Function side effects  The same container can be reused for multiple invocations of the same function within an app. This means that if your function has side effects like modifying files on disk, they may or may not be present for subsequent calls to that function. You should not rely on the side effects to be present, but you might have to be careful so they don’t cause problems.  For example, if you create a disk-backed database using sqlite3:  import modal import sqlite3  stub = modal.Stub()  @stub.function() def db_op():     db = sqlite3(\"db_file.sqlite3\")     db.execute(\"CREATE TABLE example (col_1 TEXT)\")     ... Copy  This function can (but will not necessarily) fail on the second invocation with an  OperationalError: table foo already exists  To get around this, take care to either clean up your side effects (e.g. deleting the db file at the end your function call above) or make your functions take them into consideration (e.g. adding an if os.path.exists(\"db_file.sqlite\") condition or randomize the filename above).  Troubleshooting “Command not found” errors Custom types defined in __main__ Function side effects © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/gpu","title":"GPU acceleration | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints GPU acceleration  If you have code or use libraries that are GPU accelerated, you can attach the first available GPU to your function by passing the gpu=\"any\" argument to the @stub.function decorator:  import modal  stub = modal.Stub()  @stub.function(gpu=\"any\") def my_function():     # code here will be executed on a machine with an available GPU     ... Copy Specifying GPU type  When gpu=\"any\" is specified, your function runs in a container with access to a supported GPU. Currently this gives you Nvidia Tesla T4 or A10G instances, and pricing is based on the scheduled instance. If you need more control, you can pick a specific GPU type by changing this argument.  @stub.function(gpu=\"T4\") def my_t4_function():     ...  @stub.function(gpu=\"A10G\") def my_a10g_function():     ...  @stub.function(gpu=\"A100\") def my_a100_function():     ...  @stub.function(gpu=\"L4\") def my_l4_function():     ... Copy  For information on all valid values for the gpu parameter see the modal.gpu reference page.  Specifying GPU count  You may also specify the number of GPUs to attach to your function by using the object form of the gpu parameter for your desired GPU:  @stub.function(gpu=modal.gpu.A10G(count=2)) def my_a10g_function():     ... Copy  Currently A100 and T4 instances support up to 8 GPUs, and A10G support up to 4 GPUs. Note that requesting more than 2 GPUs per container will usually result in larger wait times.  A100 GPUs  Modal’s fastest GPUs are the A100s, which are NVIDIA’s flagship data center chip. They have beefier hardware and more GPU memory.  To request an A100 with 40 GB of GPU memory, replace the gpu=\"any\" argument with gpu=\"A100\":  @stub.function(gpu=\"A100\") def my_a100_function():     ... Copy  Modal also support 80GB A100s:  @stub.function(gpu=modal.gpu.A100(memory=80)) def my_a100_80GB_function():     ... Copy Examples  Take a look at some of our examples that use GPUs:  Fast inference with vLLM (Mistral 7B) Stable Diffusion XL 1.0 Blender video renderer GPU acceleration Specifying GPU type Specifying GPU count A100 GPUs Examples See it in action High-speed inference with vLLM  Stable Diffusion Slackbot  Blender video renderer  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/scale","title":"Scaling out | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Scaling out  Modal has a few different tools that helps with increasing performance of your applications.  Parallel execution of inputs  If your code is running the same function repeatedly with different independent inputs (e.g., a grid search), the easiest way to increase performance is to run those function calls in parallel using Modal’s Function.map() method.  Here is an example if we had a function evaluate_model that takes a single argument:  import modal  stub = modal.Stub()   @stub.function() def evaluate_model(x):     ...   @stub.local_entrypoint() def main():     inputs = list(range(100))     for result in evaluate_model.map(inputs):  # runs many inputs in parallel         ... Copy  In this example, evaluate_model will be called with each of the 100 inputs (the numbers 0 - 99 in this case) roughly in parallel and the results are returned as an iterable with the results ordered in the same way as the inputs.  Out of order results and flatmap  .map() can also be used on Modal functions that wrap generators. One generator will be created per input in the map, and each output from the generator will then be returned as they are created. This means the outputs will not necessarily come in the same order as the inputs. Since a generator can yield zero or more results, the number of outputs will not necessarily match the number of inputs either, like a “flat map”.  Exceptions  By default, if any of the function calls raises an exception, the exception will be propagated. To treat exceptions as successful results and aggregate them in the results list, pass in return_exceptions=True.  @stub.function() def my_func(a):     if a == 2:         raise Exception(\"ohno\")     return a ** 2  @stub.local_entrypoint() def main():     print(list(my_func.map(range(3), return_exceptions=True)))     # [0, 1, UserCodeException(Exception('ohno'))] Copy Starmap  If your function takes multiple variable arguments, you can either use Function.map() with one input iterator per argument, or Function.starmap() with a single input iterator containing sequences (like tuples) that can be spread over the arguments. This works similarly to Python’s built in map and itertools.starmap.  @stub.function() def my_func(a, b):     return a + b  @stub.local_entrypoint() def main():     assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7] Copy Gotchas  Note that .map() is a method on the modal function object itself, so you don’t explicitly call the function.  Incorrect usage:  results = evaluate_model(inputs).map() Copy  Modal’s map is also not the same as using Python’s builtin map(). While the following will technically work, it will execute all inputs in sequence rather than in parallel.  Incorrect usage:  results = map(evaluate_model, inputs) Copy Asynchronous usage  All Modal APIs are available in both blocking and asynchronous variants. If you are comfortable with asynchronous programming, you can use it to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely. See the async guide or the examples for more information about asynchronous usage.  GPU acceleration  Sometimes you can speed up your applications by utilizing GPU acceleration. See the gpu section for more information.  Limiting concurrency  If you want to limit concurrency, you can use the concurrency_limit argument to stub.function. For instance:  stub = modal.Stub()  @stub.function(concurrency_limit=5) def f(x):     print(x) Copy  With this, Modal will run at most 5 concurrent functions at any point.  Scaling out Parallel execution of inputs Out of order results and flatmap Exceptions Starmap Gotchas Asynchronous usage GPU acceleration Limiting concurrency See it in action Auto-scaling LLM inference endpoints  Job queue for OCR  Parallel web scraping  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/webhook-timeouts","title":"Request timeouts | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Request timeouts  Web endpoint (a.k.a. webhook) requests should complete quickly, ideally within a few seconds. All web endpoint function types (web_endpoint, asgi_app, wsgi_app) have a maximum HTTP request timeout of 150 seconds enforced. However, the underlying Modal function can have a longer timeout.  In case the function takes more than 150 seconds to complete, a HTTP status 303 redirect response is returned pointing at the original URL with a special query parameter linking it that request. This is the result URL for your function. Most web browsers allow for up to 20 such redirects, effectively allowing up to 50 minutes (20 * 150 s) for web endpoints before the request times out.  Some libraries and tools might require you to add a flag or option in order to follow redirects automatically, e.g. curl -L ... or http --follow ....  The result URL can be reloaded without triggering a new request. It will block until the request completes.  wait_for_response=False  In cases when you want a web endpoint to trigger and return the result URL immediately, you can add the wait_for_response=False flag to your web_endpoint/asgi_app/wsgi_app decorator. The decorated function will then be immediately triggered and a 202 Accepted status code will be returned, along with a JSON payload containing the result URL: {\"result_url\": \"...\"}.  This can be useful if you for example want to store the result URL somewhere for later access, or immediately redirect a web browser to a URL that won’t trigger a new function call on manual refresh.  Note that once the body is returned from the result URL and is sent successfully as an HTTP response, it may not be accessed again. This is because HTTP has stream semantics, and we support large response bodies. (For responses less than a few MiB, it may work briefly, but this is not supported.)  Polling solutions  Sometimes it can be useful to be able to poll for results rather than wait for a long running HTTP request. The easiest way to do this is to have your web endpoint spawn a modal.Function call and return the function call id that another endpoint can use to poll the submitted function’s status. Here is an example:  import fastapi from modal import Stub, asgi_app from modal.functions import FunctionCall   stub = Stub()  web_app = fastapi.FastAPI()   @stub.function() @asgi_app() def fastapi_app():     return web_app   @stub.function() def slow_operation():     ...   @web_app.post(\"/accept\") async def accept_job(request: fastapi.Request):     call = slow_operation.spawn()     return {\"call_id\": call.object_id}   @web_app.get(\"/result/{call_id}\") async def poll_results(call_id: str):     function_call = FunctionCall.from_id(call_id)     try:         return function_call.get(timeout=0)     except TimeoutError:         http_accepted_code = 202         return fastapi.responses.JSONResponse({}, status_code=http_accepted_code) Copy  Document OCR Web App is an example that uses this pattern.  Request timeouts wait_for_response=False Polling solutions © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/local-data","title":"Passing local data | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Passing local data  If you have a function that needs access to some data not present in your Python files themselves you have a few options for bundling that data with your Modal app.  Passing function arguments  The simplest and most straight-forward way is to read the data from your local script and pass the data to the outermost Modal function call:  import json   @stub.function() def foo(a):     print(sum(a[\"numbers\"]))   @stub.local_entrypoint() def main():     data_structure = json.load(open(\"blob.json\"))     foo.remote(data_structure) Copy  Any data of reasonable size that is serializable through cloudpickle is passable as an argument to Modal functions.  Refer to the section on global variables for how to work with objects in global scope that can only be initialized locally.  Mounting directories  If you want to forward files from your local system, you can do that through modal.Mount objects and the mounts function decorator option:  @stub.function(mounts=[modal.Mount.from_local_dir(\"/user/john/.aws\", remote_path=\"/root/.aws\")]) def aws_stuff():     ... Copy  Note: the mounted directory will not be shared between worker instances, so modifying files or writing new files to a mount will not be reflected in other functions calls with the same mount. For this reason, you should typically treat the Mount as read-only.  Mounting local packages  For the special case of mounting a local package so it’s also available within your Python environment inside the container, Modal provides a create_package_mounts helper function:  import modal import my_local_module  stub = modal.Stub()  @stub.function(mounts=modal.create_package_mounts([\"my_local_module\", \"my_other_module\"])) def f():     my_local_module.do_stuff() Copy Passing local data Passing function arguments Mounting directories Mounting local packages © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/custom-container","title":"Custom containers | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Custom containers  By default, Modal functions are executed in a Debian Linux container with a basic Python installation of the same minor version v3.x as your local Python interpreter.  Oftentimes you might need some third party Python packages, or some other pre-installed dependencies for your function. Modal provides a few different options to customize the container image.  Additional Python packages  The simplest and most common container modification is to add some third party Python package, like pandas. To do this you can create a custom modal.Image by starting with the Image.debian_slim() function, and then extending the image by invoking the pip_install method with a list of all of the packages you need.  from modal import Image  pandas_image = Image.debian_slim().pip_install(\"pandas\", \"numpy\")   @stub.function(image=pandas_image) def my_function():     import pandas as pd     import numpy as np      df = pd.DataFrame()     ... Copy Importing Python packages  You might want to use packages inside your Modal code that you don’t have on your local computer. In the example above, we build a container that uses pandas. But if we don’t have pandas locally, on the computer launching the Modal job, we can’t put import pandas at the top of the script, since it would cause an ImportError.  The easiest solution to this is to put import pandas in the function body instead, as you can see above. This means that pandas is only imported when running inside the remote Modal container, which has pandas installed.  If you have a lot of functions and a lot of Python packages, you might want to keep the imports in the global scope so that every function can use the same imports. In that case, you can use the imports() context manager:  from modal import Image  pandas_image = Image.debian_slim().pip_install(\"pandas\", \"numpy\")   with pandas_image.imports():     import pandas as pd     import numpy as np   @stub.function(image=pandas_image) def my_function():     df = pd.DataFrame() Copy  Note that imports is considered beta.  Shell commands  You can also supply shell commands that should be executed when building the container image. This can be useful for installing additional binary dependencies:  from modal import Image  ffmpeg_image = Image.debian_slim().apt_install(\"ffmpeg\")   @stub.function(image=ffmpeg_image) def process_video():     subprocess.call([\"ffmpeg\", ...]) Copy  Or for preloading custom assets into the container:  from modal import Image  image_with_model = (     Image.debian_slim().apt_install(\"curl\").run_commands(         \"curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalcatface.xml\",     ) )   @stub.function(image=image_with_model) def find_cats():     content = open(\"/haarcascade_frontalcatface.xml\").read()     ... Copy Using existing Docker Hub images  Docker Hub has many pre-built images for common software packages. You can use any public image in your function using Image.from_registry, as long as:  Python 3.7 or above is present, and is available as python pip is installed correctly The image is built for the linux/amd64 platform from modal import Image  sklearn_image = Image.from_registry(\"huanjason/scikit-learn\")   @stub.function(image=sklearn_image) def fit_knn():     from sklearn.neighbors import KNeighborsClassifier     ... Copy  If python or pip isn’t set up properly, we provide an add_python argument that installs a reproducible, standalone build of Python:  from modal import Image  image1 = Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\") image2 = Image.from_registry(\"gisops/valhalla:latest\", add_python=\"3.11\") Copy  The from_registry function can load images from all public registries, such as Nvidia’s nvcr.io, AWS ECR, and GitHub’s ghcr.io.  We also support access to private AWS ECR and GCP Artifact Registry images.  Using Conda instead of pip  Modal provides a pre-built Conda base image, if you would like to use conda for package management. The Python version available is whatever version the official miniconda3 image currently comes with (3.9.12 at this time).  from modal import Image  pymc_image = Image.conda().conda_install(\"theano-pymc==1.1.2\", \"pymc3==3.11.2\")   @stub.function(image=pymc_image) def fit():     import pymc3 as pm     ... Copy Using a Dockerfile  Modal also supports using a Dockerfile using the Image.from_dockerfile function. It takes a path to an existing Dockerfile. For instance:  FROM python:3.9 RUN pip install sklearn Copy from modal import Image  dockerfile_image = Image.from_dockerfile(\"Dockerfile\")   @stub.function(image=dockerfile_image) def fit():     import sklearn     ... Copy Dockerfile command compatibility  Since Modal doesn’t use Docker to build containers, we have our own implementation of the Dockerfile specification. Most Dockerfiles should work out of the box, but there are some differences to be aware of.  First, a few minor Dockerfile commands and flags have not been implemented yet. Please reach out to us if your use case requires any of these.  Next, there are some command-specific things that may be useful when porting a Dockerfile to Modal.  ENTRYPOINT  While the ENTRYPOINT command is supported, there is an additional constraint to the entrypoint script provided: it must also exec the arguments passed to it at some point. This is so that Modal’s own Python entrypoint can run after your own. Most entrypoint scripts in Docker containers are typically “wrapper” scripts so this is already the case.  If you wish to write your own entrypoint script, you can use the following as a template:  #!/usr/bin/env bash  # Your custom startup commands here.  exec \"$@\" # Runs the command passed to the entrypoint script. Copy  If the above file is saved as /usr/bin/my_entrypoint.sh in your container, then you can register it as an entrypoint with ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"] in your Dockerfile, or with dockerfile_commands as an Image build step.  from modal import Image  Image.debian_slim().pip_install(\"foo\").dockerfile_commands('ENTRYPOINT [\"/usr/bin/my_entrypoint.sh\"]') Copy ENV  While simple ENV commands are supported, we don’t currently support environment replacement. This means you can’t yet do ENV PATH=$PATH:/foo.  A work-around is to use ENTRYPOINT instead, and use regular bash commands to achieve this.  Running a function as a build step (beta)  Instead of using shell commands, you can also run a Python function as an image build step using the Image.run_function method. For example, you can use this to download model parameters to your image:  from modal import Image, Secret  def download_models():     import diffusers      pipe = diffusers.StableDiffusionPipeline.from_pretrained(         model_id, use_auth_token=os.environ[\"HF_TOKEN\"]     )     pipe.save_pretrained(\"/model\")   image = (     Image.debian_slim()         .pip_install(\"diffusers[torch]\", \"transformers\", \"ftfy\", \"accelerate\")         .run_function(download_models, secrets=[Secret.from_name(\"huggingface-secret\")]) ) Copy  Any kwargs accepted by @stub.function (such as Mounts, NetworkFileSystems, and resource requests) can be supplied to it. Essentially, this is equivalent to running a Modal function and snapshotting the resulting filesystem as an image.  Please see the reference documentation for an explanation of which changes to your build function trigger image rebuilds.  GPU availability during container build  If a build step should be run on an instance with a GPU (e.g., so that a package can be linked against CUDA libraries), pass a desired GPU type when defining that step:  from modal import Image  image = (     Image.debian_slim()     .pip_install(\"bitsandbytes\", gpu=\"A10G\") ) Copy Forcing an image to rebuild  Modal uses the definition of an image to determine whether it needs to be rebuilt. In some cases, you may want to force an image to rebuild, even if the definition hasn’t changed. You can do this by adding the force_build=True argument to any of the image build steps.  from modal import Image  image = (     Image.debian_slim()     .apt_install(\"git\")     .pip_install(\"slack-sdk\", force_build=True)     .run_commands(\"echo hi\") ) Copy  In the above example, both pip_install(\"slack-sdk\") and run_commands(\"echo hi\") will run again, but apt_install(\"git\") will not. Remember to remove force_build=True after you’ve rebuilt the image, otherwise it will rebuild every time you run your code.  Custom containers Additional Python packages Importing Python packages Shell commands Using existing Docker Hub images Using Conda instead of pip Using a Dockerfile Dockerfile command compatibility ENTRYPOINT ENV Running a function as a build step (beta) GPU availability during container build Forcing an image to rebuild See it in action Dependencies for YouTube processing  Registry image for Algolia indexing  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide#introduction-to-modal","title":"Introduction to Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Introduction to Modal  Modal lets you run code in the cloud without having to think about infrastructure.  Features Run any code remotely within seconds. Define container environments in code (or use one of our pre-built backends). Scale up horizontally to thousands of containers. Deploy and monitor persistent cron jobs. Attach GPUs with a single line of code. Serve your functions as web endpoints. Use powerful primitives like distributed dictionaries and queues. Getting started  The nicest thing about all of this is that you don’t have to set up any infrastructure. Just:  Create an account at modal.com Install the modal Python package Set up a token  …and you can start running jobs right away.  Modal is currently Python-only, but we may support other languages in the future.  How does it work?  Modal takes your code, puts it in a container, and executes it in the cloud.  Where does it run? Modal runs it in its own cloud environment. The benefit is that we solve all the hard infrastructure problems for you, so you don’t have to do anything. You don’t need to mess with Kubernetes, Docker or even an AWS account.  Introduction to Modal Features Getting started How does it work? See it in action Hello, world!  A simple web scraper  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/resources","title":"Reserving CPU and memory | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Reserving CPU and memory  Modal jobs are reserved 128 MiB of memory and 0.1 CPU cores by default. However, if there is free memory or CPU capacity on a worker, containers are free to spike above these limits.  CPU cores  If you have code that you want to run on a larger number of cores, you can request that using the cpu argument. This allows you to specify a floating-point number of CPU cores:  import modal  stub = modal.Stub()  @stub.function(cpu=8.0) def my_function():     # code here will have access to at least 8.0 cores     ... Copy Memory  If you have code that needs more guaranteed memory, you can request it using the memory argument. This expects an integer number of megabytes:  import modal  stub = modal.Stub()  @stub.function(memory=32768) def my_function():     # code here will have access to at least 32 GiB of RAM     ... Copy How much can I request?  For both CPU and memory, a maximum is enforced at function creation time to ensure your application can be scheduled for execution. Requests exceeding the maximum will be rejected with an InvalidError.  As the platform grows, we plan to support larger CPU and memory reservations.  Resource limits  We currently don’t support setting a hard limit on the resource usage of a function. But we do plan to add this feature so that users have more control over the resource consumption of their Modal applications.  Reserving CPU and memory CPU cores Memory How much can I request? Resource limits See it in action Request minimum CPUs for sentiment analysis  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/lifecycle-functions","title":"Container lifecycle functions and parameters | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Container lifecycle functions and parameters  Since Modal reuses the same container for multiple inputs, sometimes you might want to run some code exactly once when the container starts or exits. In addition, you might want to pass some parameters to the startup function that do not change between invocations (e.g. the name of a model that’s slow to load).  To accomplish any of these things, you need to use Modal’s class syntax and the @stub.cls decorator. Specifically, you’ll need to:  Convert your function to a method by making it a member of a class. Decorate the class with @stub.cls(...) with same arguments you previously had for @stub.function(...). Instead of @stub.function on the original method, just use @method. Add the correct method “hooks” to your class based on your need: @enter for one-time initialization (remote) @exit for one-time cleanup (remote) @enter  The container entry handler is called when a new container is started. This is useful for doing one-time initialization, such as loading model weights or importing packages that are only present in that image.  To use with a synchronous Modal app, make your function a member of a class, and apply the @enter() decorator to some class method:  from modal import Stub, enter, method  stub = Stub()  @stub.cls(cpu=8) class Model:     @enter()     def run_this_on_container_startup(self):         self.model = pickle.load(open(\"model.pickle\"))      @method()     def predict(self, x):         return self.model.predict(x)   @stub.local_entrypoint() def main():     Model().predict.remote(x) Copy  When working with an asynchronous Modal app, you may use an async method instead:  from modal import Stub, enter, method  stub = Stub()  @stub.cls(memory=1024) class Processor:     @enter()     async def my_enter_method(self):         self.cache = await load_cache()      @method()     async def run(self, x):         return await do_some_async_stuff(x, self.cache)   @stub.local_entrypoint() async def main():     await Processor().run.remote(x) Copy  Note: The @enter() decorator replaces the earlier __enter__ syntax! Both are supported, but __enter__ will be deprecated soon and phased out.  @exit  The container exit handler is called when a container is about to exit. Just like __exit__ for a context manager, this function takes three additional arguments, exc_type, exc_value, and traceback, that describe the exception that was raised. If the container exited normally, these values are all None.  The exit handler is useful for doing one-time cleanup, such as closing a database connection or saving intermediate results.  The exit handler is also called if the container was stopped due to a user action, or the app exited due to an exception. in this case, the exception type will be a KeyboardInterrupt. Note that the exit handler is given a grace period of 30 seconds to exit. If a handler takes longer than that, the process is killed.  To use with a synchronous Modal app, make your function a member of a class, and apply the @exit() decorator:  from modal import Stub, enter, exit, method  stub = Stub()  @stub.cls() class ETLPipeline:     @enter()     def open_connection(self):         import psycopg2         self.connection = psycopg2.connect(os.environ[\"DATABASE_URI\"])      @method()     def run(self):         # Run some queries         pass      @exit()     def close_connection(self, exc_type, exc_value, traceback):         self.connection.close()   @stub.local_entrypoint() def main():     ETLPipeline().run.remote() Copy  Note: The @exit() decorator replaces the earlier __exit__ syntax! Both are supported, but __exit__ will be deprecated soon and phased out.  @build hook  The @build() decorator lets us define code that runs as a part of building the container image. This might be useful for downloading model weights and storing it as a part of the image:  from modal import Stub, build, enter, method  stub = Stub()  @stub.cls() class Model:     @build()     def download_model(self):         download_model_to_disk()      @enter()     def load_model(self):         load_model_from_disk()      @method()     def predict(self, x):         ... Copy  The @build and @enter decorators can be stacked. This can be useful with tools like tranformers which lets you download model weights over the network but caches the weights locally. By making the initialization method run during image build, we make sure the model weights are cached in the image, which makes containers start faster.  from modal import Stub, build, enter, method  stub = Stub()  @stub.cls() class Model:     @build()     @enter()     def load_model(self):         load_model_from_network(local_cache_dir=\"/\")      @method()     def predict(self, x):         ... Copy __init__  Imagine this scenario: you want to run different variants of a model based on some argument (say the size of the model), but still share the same code for all of these variants.  In other words, instead of defining a single Modal function, you want to define a family of functions parametrized by a set of arguments.  To do this, you can define an __init__ method on your class that accepts some arguments and performs the necessary initialization:  from modal import Stub, method  stub = Stub()  @stub.cls(gpu=\"A100\") class Model():     def __init__(self, model_name: str, size: int) -> None:         self.model = load_model(model_name, size)      @method()     def generate(self):         self.model.generate(...) Copy  Then, you can construct a remote object with the desired parameters, and call the method on it:  @stub.local_entrypoint() def main():     m1 = Model(\"hedgehog\", size=7)     m1.generate.remote()      m2 = Model(\"fox\", size=13)     m2.generate.remote() Copy  Each variant of the model will behave like an independent Modal function. In addition, each pool is uniquely identified by a hash of the parameters. This means that if you constructed a Model with the same parameters in a different context, the calls to generate would be routed to the same set of containers as before.  Note that any method annotated with @enter will still run after __init__.  Arguments to __init__ have a maximum size limit of 16 KiB.  Looking up a parametrized function  If you want to call your parametrized function from a Python script running anywhere, you can use Cls.lookup:  from modal import Cls  Model = Cls.lookup(\"cls-stub\", \"Model\")  # returns a class-like object m = Model(\"snake\", size=12) m.generate.remote() Copy  Web endpoints for parametrized functions is not supported at this point.  Container lifecycle functions and parameters @enter @exit @build hook __init__ Looking up a parametrized function © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/volumes","title":"Volumes (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Volumes (beta)  The modal.Volume is a mutable volume built for high-performance file serving. Like the modal.NetworkFileSystem, these volumes can be simultaneously attached to multiple Modal functions, supporting concurrent reading and writing. But unlike the modal.NetworkFileSystem, the modal.Volume has been designed for fast reads and does not automatically synchronize writes between mounted volumes.  The modal.Volume works best with write-once, read-many I/O workloads.  Volumes work best when they contain less then 50,000 files and directories. The latency to attach or modify a volume scales linearly with the number of files in the volume, and past a few tens of thousands of files the linear component starts to dominate the fixed overhead.  Creating a volume  The easiest way to create a volume and use it as a part of your app is to use the Volume.persisted constructor. This will create a volume with a certain name, unless it already exists. You can then attach it to a function and read/write to/from it:  from modal import Stub, Volume  stub = Stub()  vol = Volume.persisted(\"my-volume\")  @stub.function(volumes={\"/data\": vol}) def run():     with open(\"/data/xyz.txt\", \"w\") as f:         f.write(\"hello\")     vol.commit()  # Needed to make sure all changes are persisted Copy  This data will be persisted and accessible to any other function that mounts the volume.  Volume commits and reloads  Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was first mounted. This reload is handled by invoking the .reload() method on a volume object. Similarly, any volume changes made within a container need to be committed for those the changes to become visible outside the current container. This is handled by invoking the .commit() method on a volume object, or by enabling background commits.  At container creation time the latest state of an attached volume is mounted. If the volume is then subsequently modified by a commit operation in another running container, that volume modification won’t become available until the original container does a .reload().  Consider this example which demonstrates the effect of a reload:  import pathlib import modal  stub = modal.Stub()  volume = modal.Volume.persisted(\"my-volume\")  p = pathlib.Path(\"/root/foo/bar.txt\")   @stub.function(volumes={\"/root/foo\": volume}) def f():     p.write_text(\"hello\")     print(f\"Created {p=}\")     volume.commit()  # Persist changes     print(f\"Committed {p=}\")   @stub.function(volumes={\"/root/foo\": volume}) def g(reload: bool = False):     if reload:         volume.reload()  # Fetch latest changes     if p.exists():         print(f\"{p=} contains '{p.read_text()}'\")     else:         print(f\"{p=} does not exist!\")   @stub.local_entrypoint() def main():     g.remote()  # 1. container for `g` starts     f.remote()  # 2. container for `f` starts, commits file     g.remote(reload=False)  # 3. reuses container for `g`, no reload     g.remote(reload=True)   # 4. reuses container, but reloads to see file. Copy  The output for this example is this:  p=PosixPath('/root/foo/bar.txt') does not exist! Created p=PosixPath('/root/foo/bar.txt') Committed p=PosixPath('/root/foo/bar.txt') p=PosixPath('/root/foo/bar.txt') does not exist! p=PosixPath('/root/foo/bar.txt') contains hello Copy  This code runs two containers, one for f and one for g. Only the last function invocation reads the file created and committed by f because it was configured to reload.  Background commits  Volumes have support for background committing that is in beta. This functionality periodically commits the state of your volume so that your application code does not need to invoke .commit().  This functionality is enabled using the _allow_background_volume_commits flag on @stub.function.  @stub.function(volumes={\"/vol/models\": volume}, _allow_background_volume_commits=True) def train():     p = pathlib.Path(\"/vol/models/dummy.txt\")     p.write_text(\"I will be persisted without volume.commit()!\")     ... Copy  During the execution of the train function shown above, every few seconds the attached volume will be snapshotted and its new changes committed. A final snapshot and commit is also automatically performed on container shutdown.  Being able to persist changes to volumes without changing your application code is especially useful when training or fine-tuning models.  Model serving  A single ML model can be served by simply baking it into a modal.Image at build time using run_function. But if you have dozens of models to serve, or otherwise need to decouple image builds from model storage and serving, use a modal.Volume.  Volumes can be used to save a large number of ML models and later serve any one of them at runtime with much better performance than can be achieved with a modal.NetworkFileSystem.  This snippet below shows the basic structure of the solution.  import modal  stub = modal.Stub() volume = modal.Volume.persisted(\"model-store\") model_store_path = \"/vol/models\"   @stub.function(volumes={model_store_path: volume}, gpu=\"any\") def run_training():     model = train(...)     save(model_store_path, model)     volume.commit()  # Persist changes   @stub.function(volumes={model_store_path: volume}) def inference(model_id: str, request):     try:         model = load_model(model_store_path, model_id)     except NotFound:         volume.reload()  # Fetch latest changes         model = load_model(model_store_path, model_id)     return model.run(request) Copy Model checkpointing  Checkpoints are snapshots of an ML model and can be configured by the callback functions of ML frameworks. You can use saved checkpoints to restart a training job from the last saved checkpoint. This is particularly helpful in managing preemption.  Huggingface transformers  To periodically checkpoint into a modal.Volume, you must:  Enable background commits Set the Trainer’s output_dir to write into the volume’s mount location. import pathlib  VOL_MOUNT_PATH = pathlib.Path(\"/vol\")   @stub.function(     gpu=\"A10g\",     timeout=7_200,     volumes={VOL_MOUNT_PATH: volume}, ) def finetune():     from transformers import Seq2SeqTrainer     ...      training_args = Seq2SeqTrainingArguments(         output_dir=str(VOL_MOUNT_PATH / \"model\"),         ...     )      trainer = Seq2SeqTrainer(         model=model,         args=training_args,         train_dataset=tokenized_xsum_train,         eval_dataset=tokenized_xsum_test,     ) Copy Filesystem consistency Concurrent modification  Concurrent modification from multiple containers is supported, but concurrent modifications of the same files should be avoided. Last write wins in case of concurrent modification of the same file — any data the last writer didn’t have when committing changes will be lost!  The number of commits you can run concurrently is limited. If you run too many concurrent commits each commit will take longer due to contention. If you are committing small changes, avoid doing more than 5 concurrent commits (the number of concurrent commits you can make is proportional to the size of the changes being committed).  As a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to the same file (nor is distributed file locking supported).  While a commit or reload is in progress the volume will appear empty to the container that initiated the commit. That means you cannot read from or write to a volume in a container where a commit or reload is ongoing (note that this only applies to the container where the commit or reload was issued, other containers remain unaffected).  For example, this is not going to work:  volume = modal.Volume.persisted(\"my-volume\")   @stub.function(image=modal.Image.debian_slim().pip_install(\"aiofiles\"), volumes={\"/vol\": volume}) async def concurrent_write_and_commit():     async with aiofiles.open(\"/vol/big.file\", \"w\") as f:         await f.write(\"hello\" * 1024 * 1024 * 500)      async def f():         await asyncio.sleep(0.1)  # Wait for the commit to start         # This is going to fail with:         # PermissionError: [Errno 1] Operation not permitted: '/vol/other.file'         # since the commit is in progress when we attempt the write.         async with aiofiles.open(\"/vol/other.file\", \"w\") as f:             await f.write(\"hello\")      await asyncio.gather(volume.commit.aio(), f()) Copy Busy volume errors  Volume commits cannot be performed while volume files are still open for writing. The commit operation will fail with “volume busy”. The following is a simple example of how a “volume busy” error can occur:  volume = modal.Volume.persisted(\"my-volume\")   @stub.function(volumes={\"/vol\": volume}) def seed_volume():     f = open(\"/vol/data.txt\", \"w\")     f.write(\"hello world\") # file not closed after writing     volume.commit()     f.close()  # closed file too late Copy Persisting volumes  By default, a modal.Volume lives as long as the app it’s defined in, just like any other Modal object. However in many situations you might want to persist file data between runs of the app. To do this, you can use the persisted method on the Volume object.  Further examples Pet Art Dreambooth with Hugging Face and Gradio uses a volume for model storage Volumes (beta) Creating a volume Volume commits and reloads Background commits Model serving Model checkpointing Huggingface transformers Filesystem consistency Concurrent modification Busy volume errors Persisting volumes Further examples See it in action Fine-tuning and serving custom LLaMA models  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/workspaces","title":"Workspaces | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Workspaces  After having signed up to Modal, you’ve automatically received a Personal Workspace that you can use to run and deploy personal apps. To deploy collaborate with others, however, you can create or join additional Workspaces.  Create a Workspace  Modal Workspaces are always associated with a GitHub organization. On the settings page, every GitHub organization to which you’ve granted Modal access will be listed.  If you don’t see the GitHub organization you’re looking for, click “Refresh GitHub Organizations”. This will retrieve from GitHub all organizations of which you are a member and update the displayed list. (If you are an outside collaborator with a GitHub organization, you must be upgraded to an organization member.)  Within the displayed list of organizations, you can click a list item’s “create” button to create a new Modal Workspace.  Inviting new Workspace members  To invite a new Workspace member, first switch your dashboard environment to an organization Workspace. Navigate to the settings page and under the “Invite Member” section enter the email address of the person you would like to invite. Click “invite” to send the invite. Invited members must be a member of the active Workspaces’s associated GitHub organization.  The invited person will receive an email with a link to join your Workspace. The email link will also allow them to sign up to Modal and create a Personal Workspace if they do not have one already.  Create a token for a Workspace  To interact with a Workspace’s resources programmatically, you need to add an API token for that Workspace. Your existing API tokens are displayed on the settings page and new API tokens can be added for a particular Workspace.  After adding a token for a Workspace to your Modal config file you can activate that Workspace’s profile using the CLI (see below).  As an administrator or workspace owner you can manage active tokens for a workspace on the member tokens page. For more information on API token management see the documentation about configuration.  Switching active Workspace  When on the dashboard or using the CLI, the active profile determines which personal or organizational Workspace is associated with your actions.  Dashboard  You can switch between organization Workspaces and your Personal Workspace by using the workspace selector at the top of the dashboard.  CLI  To switch the Workspace associated with CLI commands, use modal profile activate.  Administrating workspace members  Workspaces have three different levels of access privileges:  Owner Administrator User  The user that creates a workspace is automatically set as the Owner for that workspace. The owner can assign any other roles within the workspace, as well as disable other members of the workspace.  An Administrator within a workspace can assign all roles except Owner and can also disable other members of the workspace.  A User of a workspace can not assign any access privileges within the workspace but can otherwise perform any action like running and deploying apps and modify Secrets.  As an Owner or administrator you can administrate the access privileges of other members on the Members settings page  Leaving a Workspace  To leave a Workspace, navigate to the settings page and click “leave” on a listed Workspace.  Workspaces Create a Workspace Inviting new Workspace members Create a token for a Workspace Switching active Workspace Dashboard CLI Administrating workspace members Leaving a Workspace © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/secrets","title":"Secrets | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Secrets  Secrets provide a dictionary of environment variables for images.  Secrets are a secure way to add credentials and other sensitive information to the containers your functions run in. You can create and edit secrets on the dashboard, or programmatically from Python code.  Using secrets  To inject secrets into the container running your function, you add the secret= or secrets=[...] argument to your stub.function annotation. For deployed secrets (typically secrets defined on the Modal website) you can refer to your secrets using Secret.from_name(secret_name).  For example, if you have a secret called secret-keys containing the key MY_PASSWORD:  import os import modal  stub = modal.Stub()   @stub.function(secret=modal.Secret.from_name(\"secret-keys\")) def some_function():     secret_key = os.environ[\"MY_PASSWORD\"]     ... Copy  Each secret can contain multiple keys and values but you can also inject multiple secrets, allowing you to separate secrets into smaller reusable units:  @stub.function(secrets=[     modal.Secret.from_name(\"my-secret-name\"),     modal.Secret.from_name(\"other-secret\"), ]) def other_function():     ... Copy  The secrets are applied in order, so key-values from later modal.Secret objects in the list will overwrite earlier key-values in the case of a clash. For example, if both modal.Secret objects above contained the key FOO, then the value from \"other-secret\" would always be present in os.environ[\"FOO\"].  Programmatic creation of secrets  In addition to defining secrets on the modal web page, you can programmatically create a secret directly in your script and send it along to your function using Secret.from_dict(...). This can be useful if you want to send secrets from your local development machine to the remote Modal app.  import os import modal  stub = modal.Stub()  if modal.is_local():     local_secret = modal.Secret.from_dict({\"FOO\": os.environ[\"LOCAL_FOO\"]}) else:     local_secret = modal.Secret.from_dict({})   @stub.function(secret=local_secret) def some_function():     print(os.environ[\"FOO\"]) Copy  You can also use Secret.from_dotenv() to load any secrets defined in an .env file:  @stub.function(secret=modal.Secret.from_dotenv()) def some_other_function():     print(os.environ[\"USERNAME\"]) Copy Secrets Using secrets Programmatic creation of secrets See it in action OpenAI secret for LangChain RAG  HuggingFace access token for gated models  Write to Google Sheets  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/global-variables","title":"Global variables | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Global variables  There are cases where you might want objects or data available in global scope. For example:  You need to use the data in a scheduled function (scheduled functions don’t accept arguments) You need to construct objects (e.g. Secrets) in global scope to use as function annotations You don’t want to clutter many function signatures with some common arguments they all use, and pass the same arguments through many layers of function calls.  For these cases, you can use the modal.is_local function, which returns True if the app is running locally (initializing) or False if the app is executing in the cloud.  For instance, you can use a modal.Dict object to store one or multiple objects by key (similar to a Python dict) for later access in Modal functions:  import json import random  if modal.is_local():     with open(\"list.json\", \"r\") as f:         foo_list = json.load(f)  # reads from local disk on the development machine     stub.data_dict = modal.Dict({\"foo\": foo_list})   @stub.function(schedule=modal.Period(days=1)) def daily_random_entry():     print(random.choice(stub.data_dict[\"foo\"])) Copy  Similarly, to create a modal.Secret that you can pass to your function decorators to create environment variables, you can run:  import os  if modal.is_local():     pg_password = modal.Secret.from_dict({\"PGPASS\": os.environ[\"MY_LOCAL_PASSWORD\"]}) else:     pg_password = modal.Secret.from_dict({})   @stub.function(secret=pg_password) def get_secret_data():     connection = psycopg2.connect(password=os.environ[\"PGPASS\"])     ... Copy Warning about regular module globals  If you try to construct a global in module scope using some local data without using something like modal.is_local, it might have unexpected effects since your Python modules will be not only be loaded on your local machine, but also on the remote worker.  E.g., this will typically not work:  # blob.json doesn't exist on the remote worker, so this will cause an error there data_blob = open(\"blob.json\", \"r\").read()  @stub.function() def foo():     print(data_blob) Copy Global variables Warning about regular module globals © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/retries","title":"Failures and retries | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Failures and retries  When you call a function over a sequence of inputs with Function.map(), sometimes errors can happen during function execution. Exceptions from within the remote function are propagated to the caller, so you can handle them with a try-except statement (refer to section on custom types for more on how to catch user-defined exceptions):  @stub.function() def f(i):     raise ValueError()  @stub.local_entrypoint() def main():     try:         for _ in f.map([1, 2, 3]):             pass     except ValueError:         print(\"Exception handled\") Copy Function retries  You can configure Modal to automatically retry function failures if you set the retries option when declaring your function:  @stub.function(retries=3) def my_flaky_function():     pass Copy  When used with Function.map(), each input is retried up to the max number of retries specified.  The basic configuration shown provides a fixed 1s delay between retry attempts. For fine-grained control over retry delays, including exponential backoff configuration, use modal.Retries.  Container crashes  In the case of a container crash on start-up (for example, while handling imports in global scope before the function can be run), the error will be propagated to the caller immediately, since it’s likely a user error.  If a container crashes after start-up (for example, due to an out of memory error), Modal will reschedule the container and any work it was currently assigned, unless the crash rate of the container exceeds a certain limit.  Failures and retries Function retries Container crashes © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/developing-debugging","title":"Developing and debugging | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Developing and debugging  Modal makes it easy to run apps in the cloud, try code changes in the cloud, and debug remotely executing code as if it were right there on your laptop. To speed boost your inner dev loop, this guide provides a rundown of tools and techniques for developing and debugging software in Modal.  Interactivity  You can launch a Modal Function interactively and have it drop you right into the middle of the action, at an interesting callsite or the site of a runtime detonation.  Interactive functions  It is possible to start the interactive Python debugger or start an IPython REPL right in the middle of a Modal function.  The Python debugger is initiated with the language built-in breakpoint() function:  @stub.function(interactive=True) def f():     breakpoint()     print(\"step1\")     print(\"step2\") Copy  An IPython REPL is started like so:  @stub.function(interactive=True) def f():     model = expensive_function()     # play around with `model`     import IPython     IPython.embed() Copy  Don’t worry about installing IPython in your containers, that’s not necessary. It is provided by Modal’s execution environment.  Interactive shell  Modal lets you run interactive commands on your container images from the terminal. This is handy for debugging issues with your image, interactively refining build commands, and exploring the contents of NetworkFileSystems and Mounts.  modal shell  The primary interface for accessing this feature is the modal shell CLI command, which accepts a function name in your app (or prompts you to select one, if none is provided), and runs an interactive command on the same image as the function, with the same Secrets, NetworkFileSystems and Mounts attached as the selected function.  The default command is /bin/bash, but you can override this with any other command of your choice using the --cmd flag.  Note that this command does not attach a shell to an existing instance of the function, but instead creates a fresh instance of the underlying image. We might support the former soon - please reach out to us if that would be useful to you.  Live updating Hot reloading with modal serve  Modal has the command modal serve <filename.py>, which creates a loop that live updates an app when any of the supporting files change.  Live updating works with web endpoints, syncing your changes as you make them, and it also works well with cron schedules and job queues.  from modal import Stub, web_endpoint  stub = Stub()  @stub.function() @web_endpoint() def f():     return \"I update on file edit!\"  @stub.function(schedule=modal.Period(seconds=5)) def run_me():     print(\"I also update on file edit!\") Copy  If you edit this file, the modal serve command will detect the change and update the code, without having to restart the command.  Observability  Each running Modal app, including all ephemeral apps, streams logs and resource metrics back to you for viewing.  On start, an app will log a dashboard link that will take you its app page.  $ python3 main.py ✓ Initialized. View app page at https://modal.com/apps/ap-XYZ1234. ... Copy  From this page you can access the following:  logs, both from your application and system-level logs from Modal compute resource metrics (CPU, RAM, GPU) function call history, including historical success/failure counts Developing and debugging Interactivity Interactive functions Interactive shell modal shell Live updating Hot reloading with modal serve Observability © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/tunnels","title":"Tunnels (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Tunnels (beta)  Modal allows you to expose live TCP ports on a Modal container. This is done by creating a tunnel that forwards the port to the public Internet.  from modal import Stub, forward  stub = Stub()   @stub.function() def start_app():     # Inside this `with` block, port 8000 on the container can be accessed by     # the address at `tunnel.url`, which is randomly assigned.     with forward(8000) as tunnel:         print(f\"tunnel.url        = {tunnel.url}\")         print(f\"tunnel.tls_socket = {tunnel.tls_socket}\")         # ... start some web server at port 8000, using any framework Copy  Tunnels are direct connections and terminate TLS automatically. Within a few milliseconds of container startup, this function prints a message such as:  tunnel.url        = https://wtqcahqwhd4tu0.r5.modal.host tunnel.tls_socket = ('wtqcahqwhd4tu0.r5.modal.host', 443) Copy Build with tunnels  Tunnels are the fastest way to get a low-latency, direct connection to a running container. You can use them to run live browser applications with WebSockets, interactive terminals, Jupyter notebooks, VS Code servers, and more.  As a quick example, here is how you would expose a Jupyter notebook:  import os import secrets import subprocess  from modal import Image, Stub, forward   stub = Stub() stub.image = Image.debian_slim().pip_install(\"jupyterlab\")   @stub.function() def run_jupyter():     token = secrets.token_urlsafe(13)     with forward(8888) as tunnel:         url = tunnel.url + \"/?token=\" + token         print(f\"Starting Jupyter at {url}\")         subprocess.run(             [                 \"jupyter\",                 \"lab\",                 \"--no-browser\",                 \"--allow-root\",                 \"--ip=0.0.0.0\",                 \"--port=8888\",                 \"--LabApp.allow_origin='*'\",                 \"--LabApp.allow_remote_access=1\",             ],             env={**os.environ, \"JUPYTER_TOKEN\": token, \"SHELL\": \"/bin/bash\"},             stderr=subprocess.DEVNULL,         ) Copy  When you run the function, it starts Jupyter and gives you the public URL. It’s as simple as that.  All Modal features are supported. If you need GPUs, pass gpu= to the @stub.function() decorator. If you need more CPUs, RAM, or to attach volumes or network file systems, those also just work.  Programmable startup  The tunnel API is completely on-demand, so you can start them as the result of a web request.  For example, you could make something like Jupyter Hub without leaving Modal, giving your users their own Jupyter notebooks when they visit a URL:  from fastapi import HTTPException from fastapi.responses import RedirectResponse from modal import Queue, Stub, web_endpoint   stub = Stub() stub.q = Queue.new()   @stub.function(timeout=900)  # 15 minutes def run_jupyter():     # ... as before, but return the URL on stub.q   @stub.function() @web_endpoint(method=\"POST\") def jupyter_hub():     # ... do some validation on the secret or bearer token      if is_valid:         run_jupyter.spawn()         url = stub.q.get()         return RedirectResponse(url, status_code=303)      else:         raise HTTPException(401, \"Not authenticated\") Copy  This gives every user who sends a POST request to the web endpoint their own Jupyter notebook server, on a fully isolated Modal container.  You could do the same with VS Code and get some basic version of an instant, serverless IDE!  Advanced: Unencrypted TCP tunnels  By default, tunnels are only exposed to the Internet at a secure random URL, and connections have automatic TLS (the “S” in HTTPS). However, sometimes you might need to expose a protocol like an SSH server that goes directly over TCP. In this case, we have support for unencrypted tunnels:  with forward(8000, unencrypted=True) as tunnel:     print(f\"tunnel.tcp_socket = {tunnel.tcp_socket}\") Copy  Might produce an output like:  tunnel.tcp_socket = ('r3.modal.host', 23447) Copy  You can then connect over TCP, for example with nc r2.modal.host 23447. Unlike encrypted TLS sockets, these cannot be given a non-guessable, cryptographically random URL due to how the TCP protocol works, so they are assigned a random port number instead.  Pricing  Modal only charges for containers based on the resources you use. There is no additional charge for having an active tunnel.  For example, if you start a Jupyter notebook on port 8888 and access it via tunnel, you can use it for an hour for development (with 0.01 CPUs) and then actually run an intensive job with 16 CPUs for one minute. The amount you would be billed for in that hour is 0.01 + 16 * (1/60) = 0.28 CPUs, even though you had access to 16 CPUs without needing to restart your notebook.  Security  Tunnels are run on Modal’s private global network of Internet relays. On startup, your container will connect to the nearest tunnel so you get the minimum latency, very similar in performance to a direct connection with the machine.  This makes them ideal for live debugging sessions, using web-based terminals like ttyd.  The generated URLs are cryptographically random, but they are also public on the Internet, so anyone can access your application if they are given the URL.  We do not currently do any detection of requests above L4, so if you are running a web server, we will not add special proxy HTTP headers or translate HTTP/2. You’re just getting the TLS-encrypted TCP stream directly!  Tunnels (beta) Build with tunnels Programmable startup Advanced: Unencrypted TCP tunnels Pricing Security © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/private-registries","title":"Private registries | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Private registries  Modal provides the Image.from_registry function, which pulls most common images available from public registries, like Docker Hub and GitHub Container Registry.  We also support private image registries, starting with AWS Elastic Container Registry (ECR) and GCP Artifact Registry.  Elastic Container Registry (ECR)  You can pull images from your AWS ECR account by specifying the full image URI as follows:  aws_secret = modal.Secret.from_name(\"my-aws-secret\") image = (     modal.Image         .from_aws_ecr(             \"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:latest\",             secret=aws_secret)         .pip_install(\"torch\", \"huggingface\") )  stub = modal.Stub(image=image) Copy  As shown above, you also need to use a Modal Secret containing the environment variables AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY, and AWS_REGION. The AWS IAM user account associated with those keys must have access to the private registry you want to access.  The user needs to have the following read-only policies:  {   \"Version\": \"2012-10-17\",   \"Statement\": [     {       \"Action\": [\"ecr:GetAuthorizationToken\"],       \"Effect\": \"Allow\",       \"Resource\": \"*\"     },     {       \"Effect\": \"Allow\",       \"Action\": [         \"ecr:BatchCheckLayerAvailability\",         \"ecr:GetDownloadUrlForLayer\",         \"ecr:GetRepositoryPolicy\",         \"ecr:DescribeRepositories\",         \"ecr:ListImages\",         \"ecr:DescribeImages\",         \"ecr:BatchGetImage\",         \"ecr:GetLifecyclePolicy\",         \"ecr:GetLifecyclePolicyPreview\",         \"ecr:ListTagsForResource\",         \"ecr:DescribeImageScanFindings\"       ],       \"Resource\": \"<MY-REGISTRY-ARN>\"     }   ] } Copy  You can use the IAM configuration above as a template for creating an IAM user. You can then generate an access key and create a Modal Secret using the AWS integration option. Modal will use your access keys to generate an ephemeral ECR token. That token is only used to pull image layers at the time a new image is built. We don’t store this token but will cache the image once it has been pulled.  Images on ECR must be private and follow image configuration requirements.  Private registries Elastic Container Registry (ECR) See it in action Registry image for Algolia indexing  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/environment_variables","title":"Environment variables | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Environment variables Runtime environment variables  The Modal runtime sets several environment variables during initialization. The keys for these environment variables are reserved and cannot be overridden by your function configuration.  The following variables provide information about the function runtime environment:  MODAL_CLOUD_PROVIDER — Modal executes functions across a number of cloud providers (AWS, GCP, OCI). This variable specifies which cloud provider the Modal function is running within. MODAL_ENVIRONMENT — The name of the Modal Environment the function is running within. MODAL_IMAGE_ID — The ID of the modal.Image used by the Modal Function. MODAL_REGION — This will correspond to a geographic area identifier from the cloud provider associated with the function (see above). For AWS, the identifier is a “region”. For GCP it is a “zone”, and for OCI it is an “availability domain”. Example values are ‘us-east-1’ (AWS), ‘us-central1’ (GCP), ‘us-ashburn-1’ (OCI). MODAL_TASK_ID — The ID of the container running the Modal Function. Container image environment variables  Container image layers used by a Modal function’s modal.Image will set environment variables which will be present within your function’s runtime environment. For example, the debian_slim image sets the GPG_KEY variable.  To override image variables or set new ones, use the .env method provided by modal.Image.  Environment variables Runtime environment variables Container image environment variables © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/continuous-deployment","title":"Continuous deployment | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Continuous deployment  It’s a common pattern to auto-deploy your Modal app as part of a CI/CD pipeline. To get you started, below is a guide to doing continuous deployment of a Modal app in GitHub.  GitHub Actions  Here’s a sample GitHub Actions workflow that deploys your app on every push to the main branch.  This requires you to create a Modal token and add it as a secret for your Github Actions workflow.  After setting up secrets, create a new workflow file in your repository at .github/workflows/ci-cd.yml with the following contents:  name: CI/CD  on:   push:     branches:       - main  jobs:   deploy:     name: Deploy     runs-on: ubuntu-latest     env:       MODAL_TOKEN_ID: ${{ secrets.MODAL_TOKEN_ID }}       MODAL_TOKEN_SECRET: ${{ secrets.MODAL_TOKEN_SECRET }}      steps:       - name: Checkout Repository         uses: actions/checkout@v4        - name: Install Python         uses: actions/setup-python@v5         with:           python-version: \"3.10\"        - name: Install Modal         run: |           python -m pip install --upgrade pip           pip install modal        - name: Deploy job         run: |           modal deploy my_package.my_file Copy  Be sure to replace my_package.my_file with your actual entrypoint.  If you use multiple Modal Environments, you can additionally specify the target environment in the YAML using MODAL_ENVIRONMENT=xyz.  Continuous deployment GitHub Actions © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/webhook-urls","title":"Web endpoint URLs | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Web endpoint URLs User-specified URLs  Users have partial control over the URL of a web endpoint. For the web_endpoint, asgi_app and wsgi_app decorators, an optional label keyword argument can be provided, which allocates the URL https://<workspace>--<label>.modal.run for your endpoint.  For example, within a Modal workspace with slug my-workspace, the following code will deploy an endpoint at URL https://my-workspace--foo-bar.modal.run:  from modal import Stub, web_endpoint  stub = Stub(name=\"xyz\")  @stub.function() @web_endpoint(label=\"foo-bar\") def my_f():     ... Copy Auto-generated URLs  If a label is not specified, web endpoints receive a generated URL under the modal.run domain. These generated URLs contain a subdomain label that is unique to the running endpoint.  Deployed apps  Web endpoints served by a deployed application have subdomains composed of the following parts:  Workspace name slug: ECorp → ecorp App name slug: text_to_speech → text-to-speech Function name slug: flask_app → flask-app  The deployed web endpoint URL for this example is https://ecorp--text-to-speech-flask-app.modal.run.  This URL is reserved for the deployed application’s web endpoint. Running the application during development will not replace the deployed application’s association with the deployed URL.  Ephemeral apps  Web endpoints run in ephemeral apps have predictable, unique subdomains, distinct from the subdomain of any associated deployment.  The subdomain of a web endpoint running in an ephemeral app is composed of the following parts:  Workspace name slug: ECorp → ecorp App name slug: text_to_speech → text-to-speech Function name slug: flask_app → flask-app [Optional] Member name slug, in cases where app runs in a shared workspace: erikbern [Optional] An instance identifier, ensuring uniqueness in the presence of concurrent ephemeral app runs: 1234abcd A -dev suffix, calling out that this web endpoint is not deployed.  Combining the example parts, the ephemeral web endpoint’s URL could be https://ecorp--text-to-speech-flask-app-erikbern-1234abcd-dev.modal.run.  The components of an ephemeral web endpoint subdomain ensure URL predictability during development and testing, in both personal and shared workspaces. Multiple copies of a web endpoint can be active when a Modal app is run in two or more terminal windows, or when multiple developers are iterating on the same application codebase. Every copy will be uniquely addressable while running.  Stealing  If an emphemeral web endpoint is running and another ephemeral web endpoint is created seeking the same web endpoint label, the new web endpoint function will steal the running web endpoint’s label.  This ensures the latest iteration of an ephemeral web endpoint function is serving requests, while older ones stop recieving web traffic.  (Previously concurrently running ephemeral app web endpoints would include conflict-avoiding hashes.)  Truncation  If a generated subdomain label is longer than 63 characters, it will be truncated.  For example, the following subdomain label is too long, 67 characters: ecorp--text-to-speech-really-really-long-function-name-erikbern-dev.  The truncation happens by calculating a SHA-256 hash of the overlong label, then taking the first 6 characters of this hash. The overlong subdomain label is truncated to 56 characters, and then joined by a dash to the hash prefix.  The combination of the label hashing and truncation provides a unique list of 63 characters, complying with both DNS system limits and uniqueness requirements.  Web endpoint URLs User-specified URLs Auto-generated URLs Deployed apps Ephemeral apps Stealing Truncation See it in action Custom URL for LLM frontend  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/security","title":"Security at Modal | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Security at Modal  The document outlines Modal’s security commitments.  Application Security (AppSec)  AppSec is the practice of building software that is secure by design, secured during development, secured with testing and review, and deployed securely.  We build our software using memory-safe programming languages, including Rust (for our worker runtime and storage infrastructure) and Python (for our API servers and Modal client). Software dependencies are audited by Github’s Dependabot. We make decisions that minimize our attack surface. Most interactions with Modal are well-described in a gRPC API, and occur through modal, our open-source command-line tool and Python client library. We have automated synthetic monitoring test applications that continously check for network and application isolation within our runtime. We use HTTPS for secure connections. Modal forces HTTPS for all services using TLS (SSL), including our public website and the Dashboard to ensure secure connections. Modal’s client library connects to Modal’s servers over TLS and verify TLS certificates on each connection. Internal code reviews are performed using a modern, PR-based development workflow (Github), and engage external penetration testing firms to assess our software security. Corporate Security (CorpSec)  CorpSec is the practice of making sure Modal employees have secure access to Modal company infrastructure, and also that exposed channels to Modal are secured. CorpSec controls are the primary concern of standards such as SOC2.  Access to our services and applications is gated on a SSO Identity Provider (IdP). We mandata phishing-resistant multi-factor authentication (MFA) in all enrolled IdP accounts. We regularly audit access to internal systems. Employee laptops are protected by full disk encryption using FileVault2, and managed by Secureframe MDM. Network & Infrastructure Security (InfraSec)  InfraSec is the practice of ensuring a hardened, minimal attack surface for components we deploy on our network.  Modal uses logging and metrics observability providers, including Datadog and Sentry.io. Compute jobs at Modal are containerized and virtualized using gVisor, the sandboxing technology developed at Google and used in their Google Cloud Run and Google Kubernetes Engine cloud services. We conduct annual business continuity and security incident exercises. Vulnerability Remediation  Security vulnerabilities directly affecting Modal’s systems and services will be patched or otherwise remediated within a timeframe appropriate for the severity of the vulnerability, subject to the public availability of a patch or other remediation mechanisms.  If there is a CVSS severity rating accompanying a vulnerability disclosure, we rely on that as a starting point, but may upgrade or downgrade the severity using our best judgement.  Severity: Timeframe Critical: 24 hours High: 1 week Medium: 1 month Low: 3 months Informational: 3 months or longer SOC 2  We have successfully completed a System and Organization Controls (SOC) 2 Type 1 audit. Contact us at security@modal.com for more details or access to the report.  PCI  Payment Card Industry Data Security Standard (PCI) is a standard that defines the security and privacy requirements for payment card processing.  Modal uses Stripe to securely process transactions and trusts their commitment to best-in-class security. We do not store personal credit card information for any of our customers. Stripe is certified as “PCI Service Provider Level 1”, which is the highest level of certification in the payments industry.  Questions?  Email us!  Security at Modal Application Security (AppSec) Corporate Security (CorpSec) Network & Infrastructure Security (InfraSec) Vulnerability Remediation Severity: Timeframe SOC 2 PCI Questions? © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/concurrent-inputs","title":"Concurrent inputs on a single container (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Concurrent inputs on a single container (beta)  This guide explores why and how to configure containers to process multiple inputs simultaneously.  Default parallelism  Modal offers beautifully simple parallelism: when there is a large backlog of inputs enqueued, the number of containers scales up automatically. This is the ideal source of parallelism in the majority of cases.  When to use concurrent inputs  There are, however, a few cases where it is ideal to run multiple inputs on each container concurrently.  One use case is hosting web applications where the endpoints are not CPU-bound - for example, making an asynchronous request to a deployed Modal function or querying a database. Only a handful of containers can handle hundreds of simultaneous requests for such applications if we allow concurrent inputs.  Another use case is to support continuous batching on GPU-accelerated containers. Frameworks such as vLLM allow us to push higher token throughputs by maximizing compute in each forward pass. In LLMs, this means each GPU step can generate tokens for multiple user queries; in diffusion models, we can denoise multiple images concurrently. In order to take full advantage of this, containers need to be processing multiple inputs concurrently.  Configuring concurrent inputs  To configure functions to allow each container to process n inputs concurrently, we set allow_concurrent_inputs=n on the function decorator.  If the function is synchronous, the Modal container will execute concurrent inputs on separate threads. As such, one must take care that function implementation itself is thread-safe.  Similarly, if the function is asynchronous, the Modal container will execute the concurrent inputs on separate asyncio tasks.  # Each container executes up to 10 inputs in separate threads @stub.function(allow_concurrent_inputs=10) def sleep_sync():     # Function must be thread-safe     time.sleep(1)  # Each container executes up to 10 inputs in separate async tasks @stub.function(allow_concurrent_inputs=10) async def sleep_async():     await asyncio.sleep(1) Copy Concurrent inputs on a single container (beta) Default parallelism When to use concurrent inputs Configuring concurrent inputs See it in action Single GPU serving concurrent requests  Responsive web app on one low-cost container  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/async","title":"Asynchronous API usage | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Asynchronous API usage  All of the functions in Modal are available in both standard (blocking) and asynchronous variants. The async interface can be accessed by appending .aio to any function in the Modal API.  For example, instead of my_modal_funcion.remote(\"hello\") in a blocking context, you can use await my_modal_function.remote.aio(\"hello\") to get an asynchronous coroutine response, for use with Python’s asyncio library.  import asyncio import modal  stub = modal.Stub()   @stub.function() async def myfunc():     ...   @stub.local_entrypoint() async def main():     # execute 100 remote calls to myfunc in parallel     await asyncio.gather(*[myfunc.remote.aio() for i in range(100)]) Copy  This is an advanced feature. If you are comfortable with asynchronous programming, you can use this to create arbitrary parallel execution patterns, with the added benefit that any Modal functions will be executed remotely.  Async functions  Regardless if you use an async runtime (like asyncio) in your usage of Modal itself, you are free to define your stub.function-decorated function bodies as either async or blocking. Both kinds of definitions will work for remote Modal function calls from both any context.  An async function can call a blocking function, and vice versa.  @stub.function() def blocking_function():     return 42   @stub.function() async def async_function():     x = await blocking_function.remote.aio()     return x * 10   @stub.local_entrypoint() def blocking_main():     print(async_function.remote())  # => 420 Copy  If a function is configured to support multiple concurrent inputs per container, the behavior varies slightly between blocking and async contexts:  In a blocking context, concurrent inputs will run on separate Python threads. These are subject to the GIL, but they can still lead to race conditions if used with non-threadsafe objects. In an async context, concurrent inputs are simply scheduled as coroutines on the executor thread. Everything remains single-threaded. Asynchronous API usage Async functions © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/notebooks","title":"Jupyter notebooks | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Jupyter notebooks  You can use the Modal client library in notebook environments like Jupyter! Just import it and use as normal. However, there are some limitations to this.  Known issues  Modal functions must be defined and executed within a single notebook cell.  Currently you cannot spread your Modal functions and stub.run() app execution across multiple cells. Code from other cells can be referred to, but all @stub.function() decorations should be in a single cell.  Modal functions can only be called remotely.  Invoking the wrapped function locally does not work.  @stub.function() def f():     return \"hello\"  with stub.run():     assert \"hello\" == f.remote()  # remote call works ✔️     f()  # raises error in notebook ⚠️ Copy  Interactive shell and interactive functions are not supported.  These can only be run within a live terminal session, so they are not supported in notebooks.  If you encounter issues not documented above, first check your Modal client version is >=0.49.2142. Also, try restarting the notebook kernel, as it may be in a broken state, which is common in notebook development.  If the issue persists, contact us in Slack.  We are working on removing these limitations so that writing Modal applications in a notebook feels just like developing in regular Python modules and scripts.  Jupyter inside Modal  You can run Jupyter in Modal using the modal launch command. For example:  $ modal launch jupyter --gpu a10g Copy  That will start a Jupyter instance with a A10G GPU attached. You’ll be able to access the app with via a Modal Tunnel URL. Jupyter will stop running whenever you stop Modal call in your terminal.  See --help for additional options.  Further examples Basic demonstration of running Modal in a notebook Running Jupyter server within a Modal function Jupyter notebooks Known issues Jupyter inside Modal Further examples © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/guide/sandbox","title":"Dynamic sandboxes (beta) | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Introduction Custom container images Custom container images Private registries GPUs and other resources GPU acceleration Reserving CPU and memory Scaling out Scaling out Dicts and queues Concurrent inputs (beta) Secrets and environment variables Secrets Environment variables Deployment Managing deployments Invoke deployed functions Continuous deployment Scheduling and cron jobs Web endpoints Web endpoints Streaming web endpoints Web endpoint URLs Request timeouts Network tunnels (beta) Data sharing and storage Passing local data Network file systems Volumes Dynamic sandboxes (beta) Reliability and robustness Failures and retries Preemption Timeouts Troubleshooting Security Other topics File and project structure Developing and debugging Cold start performance Checkpointing (beta) Workspaces Environments (beta) Jupyter notebooks Asynchronous usage Global variables Container lifecycle and parameters Apps, stubs, and entrypoints Dynamic sandboxes (beta)  In addition to the function interface, Modal has a direct interface for defining containers at runtime and running arbitrary code inside them.  This can be useful if, for example, you want to:  Run code generated by a language model with a list of dynamically generated requirements. Check out a git repository and run a command against it, like a test suite, or npm lint. Use Modal to orchestrate containers that don’t have or use Python.  Each individual job is called a Sandbox, and can be created using the spawn_sandbox function on a running app:  @stub.local_entrypoint() def main():     sb = stub.spawn_sandbox(         \"bash\",         \"-c\",         \"cd /repo && pytest .\",         image=modal.Image.debian_slim().pip_install(\"pandas\"),         mounts=[modal.Mount.from_local_dir(\"./my_repo\", remote_path=\"/repo\")],         timeout=600, # 10 minutes     )      sb.wait()      if sb.returncode != 0:         print(f\"Tests failed with code {sb.returncode}\")         print(sb.stderr.read()) Copy  It’s useful to note that the Sandbox object returned above has an interface similar to Python’s asyncio.subprocess.Process API, and can be used in a similar way.  Parameters  spawn_sandbox currently supports the following parameters:  workdir: Working directory for the sandbox. Defaults to /. image: The container image to use for the sandbox. mounts: List of read-only mounts to mount into the sandbox. secrets: List of secrets to make available to the sandbox. Be careful with this option, as it can expose secrets to untrusted code. network_file_systems: Dictionary mapping mount paths to Network File Systems to be attached. gpu: Optional configuration for GPU accelerator to attach. cpu: Minimum number of cores the sandbox should be allocated. memory: Minimum amount of memory the sandbox should be allocated. timeout: Timeout in seconds before the sandbox is killed. Defaults to 300 seconds (5 minutes). block_network: Whether the sandbox’s network access should be blocked. Defaults to False. Dynamically defined environments  Note that any valid Image or Mount can be used with a sandbox, even if those images or mounts have not previously been defined. This also means that images and mounts can be built from requirements at runtime. For example, you could use a language model to write some code and define your image, and then spawn a sandbox with it. Check out devlooper for a concrete example of this.  Returning or persisting data  Modal NetworkFileSystems can be attached to sandboxes. If you want to give the caller access to files written by the sandbox, you could create an ephemeral NetworkFileSystem that will be garbage collected when the app finishes:  @stub.local_entrypoint() def main():     stub.nfs = modal.NetworkFileSystem.new()     sb = stub.spawn_sandbox(         \"bash\",         \"-c\",         \"echo foo > /cache/a.txt\",         network_file_systems={\"/cache\": stub.nfs},     )     sb.wait()     for data in nfs.read_file(\"/a.txt\"):         print(data) Copy  Alternatively, if you want to persist files between sandbox invocations (useful if you’re building a stateful code interpreter, for example), you can use create a persisted NetworkFileSystem with a dynamically assigned label:  stub.nfs = modal.NetworkFileSystem.persisted(f\"vol-{session_id}\") sb = stub.spawn_sandbox(     \"bash\",     \"-c\",     \"echo foo > /cache/a.txt\",     network_file_systems={\"/cache\": stub.nfs}, ) Copy Isolation and security  Sandboxes can be used to run untrusted code, such as code from third parties or generated by a language model. Unlike regular Function runners, Sandbox runners do not have the ability to spawn new containers, or otherwise perform operations in your workspace. These runners are also torn down after execution, and never reused across calls. A sandbox’s network access can also be blocked with block_network=True.  Output  The interface for accessing sandbox output is via the stdout and stderr attributes on the sandbox object. These are LogsReader objects, and at the moment have a single method read, which returns the entire output stream. In the future, we plan to add support for streaming output and input.  sandbox = stub.spawn_sandbox(\"echo\", \"hello\") sandbox.wait()  print(sandbox.stdout.read()) Copy Dynamic sandboxes (beta) Parameters Dynamically defined environments Returning or persisting data Isolation and security Output See it in action Safe LLM code execution  © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.method","title":"modal.method | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.method def method(     _warn_parentheses_missing=None,     *,     # Set this to True if it's a non-generator function returning     # a [sync/async] generator object     is_generator: Optional[bool] = None,     keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm. ) -> Callable[[Callable[..., Any]], _PartialFunction]: Copy  Decorator for methods that should be transformed into a Modal Function registered against this class’s stub.  Usage:  @stub.cls(cpu=8) class MyCls:      @modal.method()     def f(self):         ... Copy modal.method © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Cls","title":"modal.Cls | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Cls class Cls(modal.object.Object) Copy def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_local @staticmethod def from_local(user_cls, stub, decorator: Callable[[PartialFunction, type], _Function]) -> \"_Cls\": Copy from_name @classmethod def from_name(     cls: Type[\"_Cls\"],     app_name: str,     tag: Optional[str] = None,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None,     workspace: Optional[str] = None, ) -> \"_Cls\": Copy  Retrieve a class with a given name and tag.  Class = modal.Cls.from_name(\"other-app\", \"Class\") Copy with_options def with_options(     self: \"_Cls\",     gpu: GPU_T = None,     secrets: Collection[_Secret] = (),     volumes: Dict[Union[str, os.PathLike], _Volume] = {},     retries: Optional[Union[int, Retries]] = None,     timeout: Optional[int] = None,     concurrency_limit: Optional[int] = None,     allow_concurrent_inputs: Optional[int] = None,     container_idle_timeout: Optional[int] = None,     keep_warm: Optional[int] = None,     allow_background_volume_commits: bool = False, ) -> \"_Cls\": Copy lookup @classmethod def lookup(     cls: Type[\"_Cls\"],     app_name: str,     tag: Optional[str] = None,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None,     workspace: Optional[str] = None, ) -> \"_Cls\": Copy  Lookup a class with a given name and tag.  Class = modal.Cls.lookup(\"other-app\", \"Class\") Copy remote def remote(self, *args, **kwargs): Copy modal.Cls clone from_id deps from_local from_name with_options lookup remote © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Mount","title":"modal.Mount | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Mount class Mount(modal.object.StatefulObject) Copy  Create a mount for a local directory or file that can be attached to one or more Modal functions.  Usage  import modal import os stub = modal.Stub()  @stub.function(mounts=[modal.Mount.from_local_dir(\"~/foo\", remote_path=\"/root/foo\")]) def f():     # `/root/foo` has the contents of `~/foo`.     print(os.listdir(\"/root/foo/\")) Copy  Modal syncs the contents of the local directory every time the app runs, but uses the hash of the file’s contents to skip uploading files that have been uploaded before.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy add_local_dir @typechecked def add_local_dir(     self,     local_path: Union[str, Path],     *,     # Where the directory is placed within in the mount     remote_path: Union[str, PurePosixPath, None] = None,     # Filter function for file selection; defaults to including all files     condition: Optional[Callable[[str], bool]] = None,     # add files from subdirectories as well     recursive: bool = True, ) -> \"_Mount\": Copy  Add a local directory to the Mount object.  from_local_dir @staticmethod @typechecked def from_local_dir(     local_path: Union[str, Path],     *,     # Where the directory is placed within in the mount     remote_path: Union[str, PurePosixPath, None] = None,     # Filter function for file selection - default all files     condition: Optional[Callable[[str], bool]] = None,     # add files from subdirectories as well     recursive: bool = True, ) -> \"_Mount\": Copy  Create a Mount from a local directory.  Usage  assets = modal.Mount.from_local_dir(     \"~/assets\",     condition=lambda pth: not \".venv\" in pth,     remote_path=\"/assets\", ) Copy add_local_file @typechecked def add_local_file(     self, local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None ) -> \"_Mount\": Copy  Add a local file to the Mount object.  from_local_file @staticmethod @typechecked def from_local_file(local_path: Union[str, Path], remote_path: Union[str, PurePosixPath, None] = None) -> \"_Mount\": Copy  Create a Mount mounting a single local file.  Usage  # Mount the DBT profile in user's home directory into container. dbt_profiles = modal.Mount.from_local_file(     local_path=\"~/profiles.yml\",     remote_path=\"/root/dbt_profile/profiles.yml\"), ) Copy from_local_python_packages @staticmethod def from_local_python_packages(*module_names: str) -> \"_Mount\": Copy  Returns a modal.Mount that makes local modules listed in module_names available inside the container. This works by mounting the local path of each module’s package to a directory inside the container that’s on PYTHONPATH.  Usage  import modal import my_local_module  stub = modal.Stub()  @stub.function(mounts=[     modal.Mount.from_local_python_packages(\"my_local_module\", \"my_other_module\"), ]) def f():     my_local_module.do_stuff() Copy modal.Mount clone from_id deps from_name lookup add_local_dir from_local_dir add_local_file from_local_file from_local_python_packages © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.config","title":"modal.config | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.config  Modal intentionally keeps configurability to a minimum.  The main configuration options are the API tokens: the token id and the token secret. These can be configured in two ways:  By running the modal token set command. This writes the tokens to .modal.toml file in your home directory. By setting the environment variables MODAL_TOKEN_ID and MODAL_TOKEN_SECRET. This takes precedence over the previous method. .modal.toml  The .modal.toml file is generally stored in your home directory. It should look like this::  [default] token_id = \"ak-12345...\" token_secret = \"as-12345...\" Copy  You can create this file manually, or you can run the modal token set ... command (see below).  Setting tokens using the CLI  You can set a token by running the command::  modal token set \\   --token-id <token id> \\   --token-secret <token secret> Copy  This will write the token id and secret to .modal.toml.  If the token id or secret is provided as the string - (a single dash), then it will be read in a secret way from stdin instead.  Other configuration options  Other possible configuration options are:  loglevel (in the .toml file) / MODAL_LOGLEVEL (as an env var). Defaults to WARNING. Set this to DEBUG to see a bunch of internal output. logs_timeout (in the .toml file) / MODAL_LOGS_TIMEOUT (as an env var). Defaults to 10. Number of seconds to wait for logs to drain when closing the session, before giving up. automount (in the .toml file) / MODAL_AUTOMOUNT (as an env var). Defaults to True. By default, Modal automatically mounts modules imported in the current scope, that are deemed to be “local”. This can be turned off by setting this to False. server_url (in the .toml file) / MODAL_SERVER_URL (as an env var). Defaults to https://api.modal.com. Not typically meant to be used. Meta-configuration  Some “meta-options” are set using environment variables only:  MODAL_CONFIG_PATH lets you override the location of the .toml file, by default ~/.modal.toml. MODAL_PROFILE lets you use multiple sections in the .toml file and switch between them. It defaults to “default”. modal.config.Config class Config(object) Copy  Singleton that holds configuration used by Modal internally.  def __init__(self): Copy get def get(self, key, profile=None): Copy  Looks up a configuration value.  Will check (in decreasing order of priority):  Any environment variable of the form MODAL_FOO_BAR Settings in the user’s .toml configuration file The default value of the setting override_locally def override_locally(self, key: str, value: str):     # Override setting in this process by overriding environment variable for the setting     #     # Does NOT write back to settings file etc. Copy to_dict def to_dict(self): Copy modal.config.config_profiles def config_profiles(): Copy  List the available modal profiles in the .modal.toml file.  modal.config.config_set_active_profile def config_set_active_profile(env: str): Copy  Set the user’s active modal profile by writing it to the .modal.toml file.  modal.config .modal.toml Setting tokens using the CLI Other configuration options Meta-configuration modal.config.Config get override_locally to_dict modal.config.config_profiles modal.config.config_set_active_profile © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.exit","title":"modal.exit | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.exit @typechecked def exit(_warn_parentheses_missing=None) -> Callable[[ExitHandlerType], _PartialFunction]: Copy modal.exit © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/container","title":"modal container | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal container  Manage running containers.  Usage:  modal container [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  list: List all containers that are currently running modal container list  List all containers that are currently running  Usage:  modal container list [OPTIONS] Copy  Options:  --help: Show this message and exit. modal container modal container list © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/nfs","title":"modal nfs | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal nfs  Read and edit modal.NetworkFileSystem file systems.  Usage:  modal nfs [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  create: Create a named network file system. get: Download a file from a network file system. list: List the names of all network file systems. ls: List files and directories in a network file system. put: Upload a file or directory to a network file system. rm: Delete a file or directory from a network file system. modal nfs create  Create a named network file system.  Usage:  modal nfs create [OPTIONS] NAME Copy  Arguments:  NAME: [required]  Options:  --cloud TEXT: Cloud provider to create the file system in. One of aws|gcp. [default: aws] --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal nfs get  Download a file from a network file system.  Specifying a glob pattern (using any * or ** patterns) as the remote_path will download all matching files, preserving the source directory structure for the matched files.  For example, to download an entire network file system into dump_volume:  modal nfs get <volume-name> \"**\" dump_volume Copy  Use ”-” (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).  Usage:  modal nfs get [OPTIONS] VOLUME_NAME REMOTE_PATH [LOCAL_DESTINATION] Copy  Arguments:  VOLUME_NAME: [required] REMOTE_PATH: [required] [LOCAL_DESTINATION]: [default: .]  Options:  --force / --no-force: [default: no-force] --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal nfs list  List the names of all network file systems.  Usage:  modal nfs list [OPTIONS] Copy  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --json / --no-json: [default: no-json] --help: Show this message and exit. modal nfs ls  List files and directories in a network file system.  Usage:  modal nfs ls [OPTIONS] VOLUME_NAME [PATH] Copy  Arguments:  VOLUME_NAME: [required] [PATH]: [default: /]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal nfs put  Upload a file or directory to a network file system.  Remote parent directories will be created as needed.  Ending the REMOTE_PATH with a forward slash (/), it’s assumed to be a directory and the file will be uploaded with its current name under that directory.  Usage:  modal nfs put [OPTIONS] VOLUME_NAME LOCAL_PATH [REMOTE_PATH] Copy  Arguments:  VOLUME_NAME: [required] LOCAL_PATH: [required] [REMOTE_PATH]: [default: /]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal nfs rm  Delete a file or directory from a network file system.  Usage:  modal nfs rm [OPTIONS] VOLUME_NAME REMOTE_PATH Copy  Arguments:  VOLUME_NAME: [required] REMOTE_PATH: [required]  Options:  -r, --recursive: Delete directory recursively --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal nfs modal nfs create modal nfs get modal nfs list modal nfs ls modal nfs put modal nfs rm © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/shell","title":"modal shell | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config View on GitHub modal shell  Run an interactive shell inside a Modal image.  Examples:  Start a shell inside the default Debian-based image:  modal shell Copy  Start a bash shell using the spec for my_function in your stub:  modal shell hello_world.py::my_function Copy  Note that you can select the function interactively if you omit the function name.  Start a python shell:  modal shell hello_world.py --cmd=python Copy  Usage:  modal shell [OPTIONS] FUNC_REF Copy  Arguments:  FUNC_REF: Path to a Python file with a Stub or Modal function whose container to run.  Options:  --cmd TEXT: Command to run inside the Modal image. [default: /bin/bash] --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --image TEXT: Container image tag for inside the shell (if not using FUNC_REF). --add-python TEXT: Add Python to the image (if not using FUNC_REF). --cpu INTEGER: Number of CPUs to allocate to the shell (if not using FUNC_REF). --memory INTEGER: Memory to allocate for the shell, in MiB (if not using FUNC_REF). --gpu TEXT: GPUs to request for the shell, if any. Examples are any, a10g, a100:4 (if not using FUNC_REF). --cloud TEXT: Cloud provider to run the function on. Possible values are aws, gcp, oci, auto (if not using FUNC_REF). --help: Show this message and exit. modal shell © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/secret","title":"modal secret | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal secret  Manage secrets.  Usage:  modal secret [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  create: Create a new secret, or overwrite an existing one. list: List your published secrets. modal secret create  Create a new secret, or overwrite an existing one.  Usage:  modal secret create [OPTIONS] SECRET_NAME KEYVALUES... Copy  Arguments:  SECRET_NAME: [required] KEYVALUES...: Space-separated KEY=VALUE items [required]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal secret list  List your published secrets.  Usage:  modal secret list [OPTIONS] Copy  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --json / --no-json: [default: no-json] --help: Show this message and exit. modal secret modal secret create modal secret list © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Proxy","title":"modal.Proxy | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Proxy class Proxy(modal.object.StatefulObject) Copy  Proxy objects are used to setup secure tunnel connections to a private remote address, for example a database.  Currently modal.Proxy objects must be setup with the assistance of Modal staff. If you require a proxy please contact us.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy modal.Proxy clone from_id deps from_name lookup © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Client","title":"modal.Client | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Client class Client(object) Copy def __init__(     self,     server_url,     client_type,     credentials,     version=__version__,     *,     no_verify=False, ): Copy modal.Client © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/token","title":"modal token | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal token  Manage tokens.  Usage:  modal token [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  new: Creates a new token by using an authenticated web session. set: Set account credentials for connecting to Modal. modal token new  Creates a new token by using an authenticated web session.  Usage:  modal token new [OPTIONS] Copy  Options:  --profile TEXT: Modal profile to set credentials for. You can switch the currently active Modal profile with the modal profile command. If unspecified, uses default profile. --no-verify / --no-no-verify: [default: no-no-verify] --source TEXT --help: Show this message and exit. modal token set  Set account credentials for connecting to Modal. If not provided with the command, you will be prompted to enter your credentials.  Usage:  modal token set [OPTIONS] Copy  Options:  --token-id TEXT: Account token ID. --token-secret TEXT: Account token secret. --profile TEXT: Modal profile to set credentials for. You can switch the currently active Modal profile with the modal profile command. If unspecified, uses default profile. --no-verify / --no-no-verify: [default: no-no-verify] --help: Show this message and exit. modal token modal token new modal token set © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/app","title":"modal app | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config View on GitHub modal app  Manage deployed and running apps.  Usage:  modal app [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  list: List all running or recently running Modal apps for the current account logs: Output logs for a running app. stop: Stop an app. modal app list  List all running or recently running Modal apps for the current account  Usage:  modal app list [OPTIONS] Copy  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --json / --no-json: [default: no-json] --help: Show this message and exit. modal app logs  Output logs for a running app.  Usage:  modal app logs [OPTIONS] APP_ID Copy  Arguments:  APP_ID: [required]  Options:  --help: Show this message and exit. modal app stop  Stop an app.  Usage:  modal app stop [OPTIONS] APP_ID Copy  Arguments:  APP_ID: [required]  Options:  --help: Show this message and exit. modal app modal app list modal app logs modal app stop © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Period","title":"modal.Period | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Period class Period(modal.schedule.Schedule) Copy  Create a schedule that runs every given time interval.  Usage  import modal stub = modal.Stub()  @stub.function(schedule=modal.Period(days=1)) def f():     print(\"This function will run every day\")  modal.Period(hours=4)          # runs every 4 hours modal.Period(minutes=15)       # runs every 15 minutes modal.Period(seconds=math.pi)  # runs every 3.141592653589793 seconds Copy  Only seconds can be a float. All other arguments are integers.  Note that days=1 will trigger the function the same time every day. This is not have the same behavior as seconds=84000 since days have different lengths due to daylight savings and leap seconds. Similarly, using months=1 will trigger the function on the same day each month.  This behaves similar to the dateutil package.  def __init__(     self,     years: int = 0,     months: int = 0,     weeks: int = 0,     days: int = 0,     hours: int = 0,     minutes: int = 0,     seconds: float = 0, ) -> None: Copy modal.Period © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/config","title":"modal config | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal config  Manage client configuration for the current profile.  Refer to https://modal.com/docs/reference/modal.config for a full explanation of what these options mean, and how to set them.  Usage:  modal config [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  set-environment: Set the default Modal environment for the active profile show: Show configuration values for the current profile (debug command). modal config set-environment  Set the default Modal environment for the active profile  The default environment of a profile is used when no —env flag is passed to modal run, modal deploy etc.  If no default environment is set, and there exists multiple environments in a workspace, an error will be raised when running a command that requires an environment.  Usage:  modal config set-environment [OPTIONS] ENVIRONMENT_NAME Copy  Arguments:  ENVIRONMENT_NAME: [required]  Options:  --help: Show this message and exit. modal config show  Show configuration values for the current profile (debug command).  Usage:  modal config show [OPTIONS] Copy  Options:  --help: Show this message and exit. modal config modal config set-environment modal config show © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/volume","title":"modal volume | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal volume  [Beta] Read and edit modal.Volume volumes.  This command is in preview and may change in the future.  Previous users of modal.NetworkFileSystem should replace their usage with the modal nfs command instead.  Usage:  modal volume [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  create: Create a named, persistent modal.Volume. get: Download files from a modal.Volume. list: List the details of all modal.Volume volumes in an environment. ls: List files and directories in a modal.Volume volume. put: Upload a file or directory to a volume. rm: Delete a file or directory from a volume. modal volume create  Create a named, persistent modal.Volume.  Usage:  modal volume create [OPTIONS] NAME Copy  Arguments:  NAME: [required]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal volume get  Download files from a modal.Volume.  Specifying a glob pattern (using any * or ** patterns) as the remote_path will download all matching files, preserving the source directory structure for the matched files.  Example  modal volume get <volume-name> logs/april-12-1.txt . modal volume get <volume-name> \"**\" dump_volume Copy  Use ”-” (a hyphen) as LOCAL_DESTINATION to write contents of file to stdout (only for non-glob paths).  Usage:  modal volume get [OPTIONS] VOLUME_NAME REMOTE_PATH [LOCAL_DESTINATION] Copy  Arguments:  VOLUME_NAME: [required] REMOTE_PATH: [required] [LOCAL_DESTINATION]: [default: .]  Options:  --force / --no-force: [default: no-force] --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal volume list  List the details of all modal.Volume volumes in an environment.  Usage:  modal volume list [OPTIONS] Copy  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --json / --no-json: [default: no-json] --help: Show this message and exit. modal volume ls  List files and directories in a modal.Volume volume.  Usage:  modal volume ls [OPTIONS] VOLUME_NAME [PATH] Copy  Arguments:  VOLUME_NAME: [required] [PATH]: [default: /]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal volume put  Upload a file or directory to a volume.  Remote parent directories will be created as needed.  Ending the REMOTE_PATH with a forward slash (/), it’s assumed to be a directory and the file will be uploaded with its current name under that directory.  Usage:  modal volume put [OPTIONS] VOLUME_NAME LOCAL_PATH [REMOTE_PATH] Copy  Arguments:  VOLUME_NAME: [required] LOCAL_PATH: [required] [REMOTE_PATH]: [default: /]  Options:  --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal volume rm  Delete a file or directory from a volume.  Usage:  modal volume rm [OPTIONS] VOLUME_NAME REMOTE_PATH Copy  Arguments:  VOLUME_NAME: [required] REMOTE_PATH: [required]  Options:  -r, --recursive: Delete directory recursively --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal volume modal volume create modal volume get modal volume list modal volume ls modal volume put modal volume rm © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Cron","title":"modal.Cron | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Cron class Cron(modal.schedule.Schedule) Copy  Cron jobs are a type of schedule, specified using the Unix cron tab syntax.  The alternative schedule type is the modal.Period.  Usage  import modal stub = modal.Stub()   @stub.function(schedule=modal.Cron(\"* * * * *\")) def f():     print(\"This function will run every minute\") Copy  We can specify different schedules with cron strings, for example:  modal.Cron(\"5 4 * * *\")  # run at 4:05am every night modal.Cron(\"0 9 * * 4\")  # runs every Thursday 9am Copy def __init__(self, cron_string: str) -> None: Copy  Construct a schedule that runs according to a cron expression string.  modal.Cron © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/environment","title":"modal environment | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal environment  Create and interact with Environments  Environments are sub-divisons of workspaces, allowing you to deploy the same app in different namespaces. Each environment has their own set of Secrets and any lookups performed from an app in an environment will by default look for entities in the same environment.  Typical use cases for environments include having one for development and one for production, to prevent overwriting production apps when developing new features while still being able to deploy changes to a live environment.  Usage:  modal environment [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  create: Create a new environment in the current workspace delete: Delete an environment in the current workspace list: List all environments in the current workspace update: Update the name or web suffix of an environment modal environment create  Create a new environment in the current workspace  Usage:  modal environment create [OPTIONS] NAME Copy  Arguments:  NAME: Name of the new environment. Must be unique. Case sensitive [required]  Options:  --help: Show this message and exit. modal environment delete  Delete an environment in the current workspace  Deletes all apps in the selected environment and deletes the environment irrevocably.  Usage:  modal environment delete [OPTIONS] NAME Copy  Arguments:  NAME: Name of the environment to be deleted. Case sensitive [required]  Options:  --confirm / --no-confirm: Set this flag to delete without prompting for confirmation [default: no-confirm] --help: Show this message and exit. modal environment list  List all environments in the current workspace  Usage:  modal environment list [OPTIONS] Copy  Options:  --json / --no-json: [default: no-json] --help: Show this message and exit. modal environment update  Update the name or web suffix of an environment  Usage:  modal environment update [OPTIONS] CURRENT_NAME Copy  Arguments:  CURRENT_NAME: [required]  Options:  --set-name TEXT: New name of the environment --set-web-suffix TEXT: New web suffix of environment (empty string is no suffix) --help: Show this message and exit. modal environment modal environment create modal environment delete modal environment list modal environment update © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Dict","title":"modal.dict | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.dict modal.dict.Dict class Dict(modal.object.StatefulObject) Copy  Distributed dictionary for storage in Modal apps.  Keys and values can be essentially any object, so long as they can be serialized by cloudpickle, including Modal objects.  Lifetime of a Dict and its items  A Dict matches the lifetime of the app it is attached to, but invididual keys expire after 30 days. Because of this, Dicts are best not used for long-term storage. All data is deleted when the app is stopped.  Usage  Create a new Dict with Dict.new(), then assign it to a stub or function.  from modal import Dict, Stub  stub = Stub() stub.my_dict = Dict.new()  @stub.local_entrypoint() def main():     stub.my_dict[\"some key\"] = \"some value\"     stub.my_dict[123] = 456      assert stub.my_dict[\"some key\"] == \"some value\"     assert stub.my_dict[123] == 456 Copy  For more examples, see the guide.  clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy new @typechecked @staticmethod def new(data: Optional[dict] = None) -> \"_Dict\": Copy  Create a new Dict, optionally with initial data.  persisted @staticmethod def persisted(     label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None ) -> \"_Dict\": Copy  Deploy a Modal app containing this object.  The deployed object can then be imported from other apps, or by calling Dict.from_name(label) from that same app.  Examples  # In one app: stub.dict = Dict.persisted(\"my-dict\")  # Later, in another app or Python file: stub.dict = Dict.from_name(\"my-dict\") Copy clear @live_method def clear(self) -> None: Copy  Remove all items from the modal.Dict.  get @live_method def get(self, key: Any, default: Optional[Any] = None) -> Any: Copy  Get the value associated with a key.  Returns default if key does not exist.  contains @live_method def contains(self, key: Any) -> bool: Copy  Return if a key is present.  len @live_method def len(self) -> int: Copy  Return the length of the dictionary, including any expired keys.  update @live_method def update(self, **kwargs) -> None: Copy  Update the dictionary with additional items.  put @live_method def put(self, key: Any, value: Any) -> None: Copy  Add a specific key-value pair to the dictionary.  pop @live_method def pop(self, key: Any) -> Any: Copy  Remove a key from the dictionary, returning the value if it exists.  modal.dict modal.dict.Dict clone from_id deps from_name lookup new persisted clear get contains len update put pop © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/launch","title":"modal launch | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal launch  [Preview] Open a serverless app instance on Modal.  This command is in preview and may change in the future.  Usage:  modal launch [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  jupyter: Start Jupyter Lab on Modal. vscode: Start VS Code on Modal. modal launch jupyter  Start Jupyter Lab on Modal.  Usage:  modal launch jupyter [OPTIONS] Copy  Options:  --cpu INTEGER: [default: 8] --memory INTEGER: [default: 32768] --gpu TEXT --timeout INTEGER: [default: 3600] --image TEXT: [default: ubuntu:22.04] --add-python TEXT: [default: 3.11] --help: Show this message and exit. modal launch vscode  Start VS Code on Modal.  Usage:  modal launch vscode [OPTIONS] Copy  Options:  --cpu INTEGER: [default: 8] --memory INTEGER: [default: 32768] --gpu TEXT --timeout INTEGER: [default: 3600] --help: Show this message and exit. modal launch modal launch jupyter modal launch vscode © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/deploy","title":"modal deploy | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal deploy  Deploy a Modal stub as an application.  Usage:  modal deploy [OPTIONS] STUB_REF Copy  Arguments:  STUB_REF: Path to a Python file with a stub. [required]  Options:  --name TEXT: Name of the deployment. --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --public / --no-public: [beta] Publicize the deployment so other workspaces can lookup the function. [default: no-public] --help: Show this message and exit. modal deploy © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.exception","title":"modal.exception | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.exception modal.exception.AuthError class AuthError(modal.exception.Error) Copy  Raised when a client has missing or invalid authentication.  modal.exception.ConnectionError class ConnectionError(modal.exception.Error) Copy  Raised when an issue occurs while connecting to the Modal servers.  modal.exception.DeprecationError class DeprecationError(UserWarning) Copy  UserWarning category emitted when a deprecated Modal feature or API is used.  modal.exception.ExecutionError class ExecutionError(modal.exception.Error) Copy  Raised when something unexpected happened during runtime.  modal.exception.FunctionTimeoutError class FunctionTimeoutError(modal.exception.TimeoutError) Copy  Raised when a Function exceeds its execution duration limit and times out.  modal.exception.InvalidError class InvalidError(modal.exception.Error) Copy  Raised when user does something invalid.  modal.exception.MountUploadTimeoutError class MountUploadTimeoutError(modal.exception.TimeoutError) Copy  Raised when a Mount upload times out.  modal.exception.NotFoundError class NotFoundError(modal.exception.Error) Copy  Raised when a requested resource was not found.  modal.exception.PendingDeprecationError class PendingDeprecationError(UserWarning) Copy  Soon to be deprecated feature. Only used intermittently because of multi-repo concerns.  modal.exception.RemoteError class RemoteError(modal.exception.Error) Copy  Raised when an error occurs on the Modal server.  modal.exception.SandboxTerminatedError class SandboxTerminatedError(modal.exception.Error) Copy  Raised when a Sandbox is terminated for an internal reason.  modal.exception.SandboxTimeoutError class SandboxTimeoutError(modal.exception.TimeoutError) Copy  Raised when a Sandbox exceeds its execution duration limit and times out.  modal.exception.TimeoutError class TimeoutError(modal.exception.Error) Copy  Base class for Modal timeouts.  modal.exception.VersionError class VersionError(modal.exception.Error) Copy  Raised when the current client version of Modal is unsupported.  modal.exception.VolumeUploadTimeoutError class VolumeUploadTimeoutError(modal.exception.TimeoutError) Copy  Raised when a Volume upload times out.  modal.exception.deprecation_error def deprecation_error(deprecated_on: date, msg: str): Copy modal.exception.deprecation_warning def deprecation_warning(deprecated_on: date, msg: str, pending=False): Copy  Utility for getting the proper stack entry.  See the implementation of the built-in warnings.warn.  modal.exception.simulate_preemption def simulate_preemption(wait_seconds: int, jitter_seconds: int = 0): Copy  Utility for simulating a preemption interrupt after wait_seconds seconds. The first interrupt is the SIGINT/SIGTERM signal. After 30 seconds a second interrupt will trigger. This second interrupt simulates SIGKILL, and should not be caught. Optionally add between zero and jitter_seconds seconds of additional waiting before first interrupt.  Usage:  import time from modal.exception import simulate_preemption  simulate_preemption(3)  try:     time.sleep(4) except KeyboardInterrupt:     print(\"got preempted\") # Handle interrupt     raise Copy  See https://modal.com/docs/guide/preemption for more details on preemption handling.  modal.exception modal.exception.AuthError modal.exception.ConnectionError modal.exception.DeprecationError modal.exception.ExecutionError modal.exception.FunctionTimeoutError modal.exception.InvalidError modal.exception.MountUploadTimeoutError modal.exception.NotFoundError modal.exception.PendingDeprecationError modal.exception.RemoteError modal.exception.SandboxTerminatedError modal.exception.SandboxTimeoutError modal.exception.TimeoutError modal.exception.VersionError modal.exception.VolumeUploadTimeoutError modal.exception.deprecation_error modal.exception.deprecation_warning modal.exception.simulate_preemption © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Stub","title":"modal.Stub | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Stub class Stub(object) Copy  A Stub is a description of how to create a Modal application.  The stub object principally describes Modal objects (Function, Image, Secret, etc.) associated with the application. It has three responsibilities:  Syncing of identities across processes (your local Python interpreter and every Modal worker active in your application). Making Objects stay alive and not be garbage collected for as long as the app lives (see App lifetime below). Manage log collection for everything that happens inside your code.  Registering functions with an app  The most common way to explicitly register an Object with an app is through the @stub.function() decorator. It both registers the annotated function itself and other passed objects, like schedules and secrets, with the app:  import modal  stub = modal.Stub()  @stub.function(     secret=modal.Secret.from_name(\"some_secret\"),     schedule=modal.Period(days=1), ) def foo():     pass Copy  In this example, the secret and schedule are registered with the app.  @typechecked def __init__(     self,     name: Optional[str] = None,     *,     image: Optional[_Image] = None,  # default image for all functions (default is `modal.Image.debian_slim()`)     mounts: Sequence[_Mount] = [],  # default mounts for all functions     secrets: Sequence[_Secret] = [],  # default secrets for all functions     **indexed_objects: _Object,  # any Modal Object dependencies (Dict, Queue, etc.) ) -> None: Copy  Construct a new app stub, optionally with default image, mounts, secrets  Any “indexed_objects” objects are loaded as part of running or deploying the app, and are accessible by name on the running container app, e.g.:  stub = modal.Stub(key_value_store=modal.Dict.new())  @stub.function() def store_something(key: str, value: str):     stub.app.key_value_store.put(key, value) Copy name @property def name(self) -> Optional[str]: Copy  The user-provided name of the Stub.  app @property def app(self): Copy  stub.app is deprecated: use e.g. stub.obj instead of stub.app.obj if you need to access objects on the running app.  app_id @property def app_id(self) -> Optional[str]: Copy  Return the app_id, if the stub is running.  description @property def description(self) -> Optional[str]: Copy  The Stub’s name, if available, or a fallback descriptive identifier.  set_description def set_description(self, description: str): Copy image @property def image(self) -> _Image:     # Exists to get the type inference working for `stub.image`     # Will also keep this one after we remove [get/set][item/attr] Copy get_objects def get_objects(self) -> List[Tuple[str, _Object]]: Copy  Used by the container app to initialize objects.  is_inside @typechecked def is_inside(self, image: Optional[_Image] = None): Copy  Deprecated: use Image.imports() instead! Usage:  my_image = modal.Image.debian_slim().pip_install(\"torch\") with my_image.imports():     import torch Copy run @contextmanager def run(     self,     client: Optional[_Client] = None,     stdout=None,     show_progress: bool = True,     detach: bool = False,     output_mgr: Optional[OutputManager] = None, ) -> AsyncGenerator[\"_Stub\", None]: Copy  Context manager that runs an app on Modal.  Use this as the main entry point for your Modal application. All calls to Modal functions should be made within the scope of this context manager, and they will correspond to the current app.  Note that this method used to return a separate “App” object. This is no longer useful since you can use the stub itself for access to all objects. For backwards compatibility reasons, it returns the same stub.  registered_functions @property def registered_functions(self) -> Dict[str, _Function]: Copy  All modal.Function objects registered on the stub.  registered_classes @property def registered_classes(self) -> Dict[str, _Function]: Copy  All modal.Cls objects registered on the stub.  registered_entrypoints @property def registered_entrypoints(self) -> Dict[str, _LocalEntrypoint]: Copy  All local CLI entrypoints registered on the stub.  registered_web_endpoints @property def registered_web_endpoints(self) -> List[str]: Copy  Names of web endpoint (ie. webhook) functions registered on the stub.  local_entrypoint def local_entrypoint(     self, _warn_parentheses_missing=None, *, name: Optional[str] = None ) -> Callable[[Callable[..., Any]], None]: Copy  Decorate a function to be used as a CLI entrypoint for a Modal App.  These functions can be used to define code that runs locally to set up the app, and act as an entrypoint to start Modal functions from. Note that regular Modal functions can also be used as CLI entrypoints, but unlike local_entrypoint, those functions are executed remotely directly.  Example  @stub.local_entrypoint() def main():     some_modal_function.remote() Copy  You can call the function using modal run directly from the CLI:  modal run stub_module.py Copy  Note that an explicit stub.run() is not needed, as an app is automatically created for you.  Multiple Entrypoints  If you have multiple local_entrypoint functions, you can qualify the name of your stub and function:  modal run stub_module.py::stub.some_other_function Copy  Parsing Arguments  If your entrypoint function take arguments with primitive types, modal run automatically parses them as CLI options. For example, the following function can be called with modal run stub_module.py --foo 1 --bar \"hello\":  @stub.local_entrypoint() def main(foo: int, bar: str):     some_modal_function.call(foo, bar) Copy  Currently, str, int, float, bool, and datetime.datetime are supported. Use modal run stub_module.py --help for more information on usage.  function @typechecked def function(     self,     _warn_parentheses_missing=None,     *,     image: Optional[_Image] = None,  # The image to run as the container for the function     schedule: Optional[Schedule] = None,  # An optional Modal Schedule for the function     secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container     secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed     gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)     serialized: bool = False,  # Whether to send the function over using cloudpickle.     mounts: Sequence[_Mount] = (),     shared_volumes: Dict[         Union[str, os.PathLike], _NetworkFileSystem     ] = {},  # Deprecated, use `network_file_systems` instead     network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},     allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.     volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Experimental. Do not use!     cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.     memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.     proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.     retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.     concurrency_limit: Optional[         int     ] = None,  # An optional maximum number of concurrent containers running the function (use keep_warm for minimum).     allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.     container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.     timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.     interactive: bool = False,  # Whether to run the function in interactive mode.     keep_warm: Optional[         int     ] = None,  # An optional minimum number of containers to always keep warm (use concurrency_limit for maximum).     name: Optional[str] = None,  # Sets the Modal name of the function within the stub     is_generator: Optional[         bool     ] = None,  # Set this to True if it's a non-generator function returning a [sync/async] generator object     cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.     checkpointing_enabled: bool = False,  # Enable memory checkpointing for faster cold starts.     _allow_background_volume_commits: bool = False, ) -> Callable[..., _Function]: Copy  Decorator to register a new Modal function with this stub.  cls def cls(     self,     _warn_parentheses_missing=None,     *,     image: Optional[_Image] = None,  # The image to run as the container for the function     secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container     secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed     gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)     serialized: bool = False,  # Whether to send the function over using cloudpickle.     mounts: Sequence[_Mount] = (),     shared_volumes: Dict[         Union[str, os.PathLike], _NetworkFileSystem     ] = {},  # Deprecated, use `network_file_systems` instead     network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},     allow_cross_region_volumes: bool = False,  # Whether using network file systems from other regions is allowed.     volumes: Dict[Union[str, os.PathLike], _Volume] = {},  # Experimental. Do not use!     cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.     memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.     proxy: Optional[_Proxy] = None,  # Reference to a Modal Proxy to use in front of this function.     retries: Optional[Union[int, Retries]] = None,  # Number of times to retry each input in case of failure.     concurrency_limit: Optional[int] = None,  # Limit for max concurrent containers running the function.     allow_concurrent_inputs: Optional[int] = None,  # Number of inputs the container may fetch to run concurrently.     container_idle_timeout: Optional[int] = None,  # Timeout for idle containers waiting for inputs to shut down.     timeout: Optional[int] = None,  # Maximum execution time of the function in seconds.     interactive: bool = False,  # Whether to run the function in interactive mode.     keep_warm: Optional[int] = None,  # An optional number of containers to always keep warm.     cloud: Optional[str] = None,  # Cloud provider to run the function on. Possible values are aws, gcp, oci, auto.     checkpointing_enabled: bool = False,  # Enable memory checkpointing for faster cold starts. ) -> Callable[[CLS_T], _Cls]: Copy spawn_sandbox def spawn_sandbox(     self,     *entrypoint_args: str,     image: Optional[_Image] = None,  # The image to run as the container for the sandbox.     mounts: Sequence[_Mount] = (),  # Mounts to attach to the sandbox.     secrets: Sequence[_Secret] = (),  # Environment variables to inject into the sandbox.     network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},     timeout: Optional[int] = None,  # Maximum execution time of the sandbox in seconds.     workdir: Optional[str] = None,  # Working directory of the sandbox.     gpu: GPU_T = None,     cloud: Optional[str] = None,     cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.     memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.     block_network: bool = False,  # Whether to block network access ) -> _Sandbox: Copy  Sandboxes are a way to run arbitrary commands in dynamically defined environments.  This function returns a SandboxHandle, which can be used to interact with the running sandbox.  Refer to the docs on how to spawn and use sandboxes.  modal.Stub name app app_id description set_description image get_objects is_inside run registered_functions registered_classes registered_entrypoints registered_web_endpoints local_entrypoint function cls spawn_sandbox © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.asgi_app","title":"modal.asgi_app | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.asgi_app @typechecked def asgi_app(     _warn_parentheses_missing=None,     *,     label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.     wait_for_response: bool = True,  # Whether requests should wait for and return the function response.     custom_domains: Optional[         Iterable[str]     ] = None,  # Create an endpoint using a custom domain fully-qualified domain name. ) -> Callable[[Callable[..., Any]], _PartialFunction]: Copy  Decorator for registering an ASGI app with a Modal function.  Asynchronous Server Gateway Interface (ASGI) is a standard for Python synchronous and asynchronous apps, supported by all popular Python web libraries. This is an advanced decorator that gives full flexibility in defining one or more web endpoints on Modal.  Usage:  from typing import Callable  @stub.function() @modal.asgi_app() def create_asgi() -> Callable:     ... Copy  To learn how to use Modal with popular web frameworks, see the guide on web endpoints.  modal.asgi_app © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Secret","title":"modal.secret | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.secret modal.secret.Secret class Secret(modal.object.StatefulObject) Copy  Secrets provide a dictionary of environment variables for images.  Secrets are a secure way to add credentials and other sensitive information to the containers your functions run in. You can create and edit secrets on the dashboard, or programmatically from Python code.  See the secrets guide page for more information.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy from_dict @typechecked @staticmethod def from_dict(     env_dict: Dict[         str, Union[str, None]     ] = {},  # dict of entries to be inserted as environment variables in functions using the secret ): Copy  Create a secret from a str-str dictionary. Values can also be None, which is ignored.  Usage:  @stub.function(secret=modal.Secret.from_dict({\"FOO\": \"bar\"}) def run():     print(os.environ[\"FOO\"]) Copy from_dotenv @staticmethod def from_dotenv(path=None): Copy  Create secrets from a .env file automatically.  If no argument is provided, it will use the current working directory as the starting point for finding a .env file. Note that it does not use the location of the module calling Secret.from_dotenv.  If called with an argument, it will use that as a starting point for finding .env files. In particular, you can call it like this:  @stub.function(secret=modal.Secret.from_dotenv(__file__)) def run():     print(os.environ[\"USERNAME\"])  # Assumes USERNAME is defined in your .env file Copy  This will use the location of the script calling modal.Secret.from_dotenv as a starting point for finding the .env file.  modal.secret modal.secret.Secret clone from_id deps from_name lookup from_dict from_dotenv © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference#api-reference","title":"API Reference | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config API Reference  This is the API reference for the modal Python package, which allows you to run distributed applications on Modal.  The reference is intended to be limited to low-level descriptions of various programmatic functionality. If you’re just getting started with Modal, we would instead recommend looking at the guide first.  API Reference © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.build","title":"modal.build | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.build @typechecked def build(     _warn_parentheses_missing=None, ) -> Callable[[Union[Callable[[Any], Any], _PartialFunction]], _PartialFunction]: Copy modal.build © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Sandbox","title":"modal.sandbox | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.sandbox modal.sandbox.LogsReader class LogsReader(object) Copy  Provides an interface to buffer and fetch logs from a sandbox stream (stdout or stderr).  read def read(self) -> str: Copy  Fetch and return contents of the entire stream.  Usage  sandbox = stub.app.spawn_sandbox(\"echo\", \"hello\") sandbox.wait()  print(sandbox.stdout.read()) Copy modal.sandbox.Sandbox class Sandbox(modal.object.Object) Copy  A Sandbox object lets you interact with a running sandbox. This API is similar to Python’s asyncio.subprocess.Process.  Refer to the guide on how to spawn and use sandboxes.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy wait def wait(self, raise_on_termination: bool = True): Copy  Wait for the sandbox to finish running.  terminate def terminate(self): Copy  Terminate sandbox execution.  This is a no-op if the sandbox has already finished running.  poll def poll(self) -> Optional[int]: Copy  Check if the sandbox has finished running.  Returns None if the sandbox is still running, else returns the exit code.  stdout @property def stdout(self) -> _LogsReader: Copy  LogsReader for the sandbox’s stdout stream.  stderr @property def stderr(self) -> _LogsReader: Copy  LogsReader for the sandbox’s stderr stream.  returncode @property def returncode(self) -> Optional[int]: Copy  Return code of the sandbox process if it has finished running, else None.  modal.sandbox modal.sandbox.LogsReader read modal.sandbox.Sandbox clone from_id deps wait terminate poll stdout stderr returncode © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.call_graph","title":"modal.call_graph | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.call_graph modal.call_graph.InputInfo class InputInfo(object) Copy  Simple data structure storing information about a function input.  def __init__(self, input_id: str, task_id: str, status: modal.call_graph.InputStatus, function_name: str, module_name: str, children: List[ForwardRef('InputInfo')]) -> None Copy modal.call_graph.InputStatus class InputStatus(enum.IntEnum) Copy  Enum representing status of a function input.  The possible values are:  PENDING SUCCESS FAILURE TERMINATED TIMEOUT modal.call_graph modal.call_graph.InputInfo modal.call_graph.InputStatus © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.NetworkFileSystem","title":"modal.NetworkFileSystem | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.NetworkFileSystem class NetworkFileSystem(modal.object.StatefulObject) Copy  A shared, writable file system accessible by one or more Modal functions.  By attaching this file system as a mount to one or more functions, they can share and persist data with each other.  Usage  import modal  volume = modal.NetworkFileSystem.new() stub = modal.Stub()  @stub.function(network_file_systems={\"/root/foo\": volume}) def f():     pass  @stub.function(network_file_systems={\"/root/goo\": volume}) def g():     pass Copy  It is often the case that you would want to persist a network file system object separately from the currently attached app. Refer to the persistence guide section to see how to persist this object across app runs.  Also see the CLI methods for accessing network file systems:  modal nfs --help Copy  A NetworkFileSystem can also be useful for some local scripting scenarios, e.g.:  vol = modal.NetworkFileSystem.lookup(\"my-network-file-system\") for chunk in vol.read_file(\"my_db_dump.csv\"):     ... Copy def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy new @typechecked @staticmethod def new(cloud: Optional[str] = None) -> \"_NetworkFileSystem\": Copy  Construct a new network file system, which is empty by default.  persisted @staticmethod def persisted(     label: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None,     cloud: Optional[str] = None, ) -> \"_NetworkFileSystem\": Copy  Deploy a Modal app containing this object.  The deployed object can then be imported from other apps, or by calling NetworkFileSystem.from_name(label) from that same app.  Examples  # In one app: volume = NetworkFileSystem.persisted(\"my-volume\")  # Later, in another app or Python file: volume = NetworkFileSystem.from_name(\"my-volume\")  @stub.function(network_file_systems={\"/vol\": volume}) def f():     pass Copy persist def persist(     self,     label: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None,     cloud: Optional[str] = None, ): Copy  NetworkFileSystem().persist(\"my-volume\") is deprecated. Use NetworkFileSystem.persisted(\"my-volume\") instead.  write_file @live_method def write_file(self, remote_path: str, fp: BinaryIO) -> int: Copy  Write from a file object to a path on the network file system, atomically.  Will create any needed parent directories automatically.  If remote_path ends with / it’s assumed to be a directory and the file will be uploaded with its current name to that directory.  read_file @live_method_gen def read_file(self, path: str) -> Iterator[bytes]: Copy  Read a file from the network file system  iterdir @live_method_gen def iterdir(self, path: str) -> Iterator[api_pb2.SharedVolumeListFilesEntry]: Copy  Iterate over all files in a directory in the network file system.  Passing a directory path lists all files in the directory (names are relative to the directory) Passing a file path returns a list containing only that file’s listing description Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths) add_local_file @live_method def add_local_file(     self, local_path: Union[Path, str], remote_path: Optional[Union[str, PurePosixPath, None]] = None ): Copy add_local_dir @live_method def add_local_dir(     self,     local_path: Union[Path, str],     remote_path: Optional[Union[str, PurePosixPath, None]] = None, ): Copy listdir @live_method def listdir(self, path: str) -> List[api_pb2.SharedVolumeListFilesEntry]: Copy  List all files in a directory in the network file system.  Passing a directory path lists all files in the directory (names are relative to the directory) Passing a file path returns a list containing only that file’s listing description Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths) remove_file @live_method def remove_file(self, path: str, recursive=False): Copy  Remove a file in a network file system.  modal.NetworkFileSystem clone from_id deps from_name lookup new persisted persist write_file read_file iterdir add_local_file add_local_dir listdir remove_file © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/profile","title":"modal profile | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal profile  Set the active Modal profile.  Usage:  modal profile [OPTIONS] COMMAND [ARGS]... Copy  Options:  --help: Show this message and exit.  Commands:  activate: Change the active Modal profile. current: Print the active Modal profile. list: List all Modal profiles that are defined. modal profile activate  Change the active Modal profile.  Usage:  modal profile activate [OPTIONS] PROFILE Copy  Arguments:  PROFILE: Modal profile to activate. [required]  Options:  --help: Show this message and exit. modal profile current  Print the active Modal profile.  Usage:  modal profile current [OPTIONS] Copy  Options:  --help: Show this message and exit. modal profile list  List all Modal profiles that are defined.  Usage:  modal profile list [OPTIONS] Copy  Options:  --json / --no-json: [default: no-json] --help: Show this message and exit. modal profile modal profile activate modal profile current modal profile list © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.forward","title":"modal.forward | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.forward @contextmanager def forward(port: int, *, unencrypted: bool = False, client: Optional[_Client] = None) -> Iterator[Tunnel]: Copy  Expose a port publicly from inside a running Modal container, with TLS.  If unencrypted is set, this also exposes the TCP socket without encryption on a random port number. This can be used to SSH into a container. Note that it is on the public Internet, so make sure you are using a secure protocol over TCP.  This is an EXPERIMENTAL API and may change in the future.  Usage:  from flask import Flask from modal import Image, Stub, forward  stub = Stub(image=Image.debian_slim().pip_install(\"Flask\")) app = Flask(__name__)   @app.route(\"/\") def hello_world():     return \"Hello, World!\"   @stub.function() def run_app():     # Start a web server inside the container at port 8000. `modal.forward(8000)` lets us     # expose that port to the world at a random HTTPS URL.     with forward(8000) as tunnel:         print(\"Server listening at\", tunnel.url)         app.run(\"0.0.0.0\", 8000)      # When the context manager exits, the port is no longer exposed. Copy  Raw TCP usage:  import socket import threading from modal import Stub, forward   def run_echo_server(port: int):     \"\"\"Run a TCP echo server listening on the given port.\"\"\"     sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)     sock.bind((\"0.0.0.0\", port))     sock.listen(1)      while True:         conn, addr = sock.accept()         print(\"Connection from:\", addr)          # Start a new thread to handle the connection         def handle(conn):             with conn:                 while True:                     data = conn.recv(1024)                     if not data:                         break                     conn.sendall(data)          threading.Thread(target=handle, args=(conn,)).start()   stub = Stub()   @stub.function() def tcp_tunnel():     # This exposes port 8000 to public Internet traffic over TCP.     with forward(8000, unencrypted=True) as tunnel:         # You can connect to this TCP socket from outside the container, for example, using `nc`:         #  nc <HOST> <PORT>         print(\"TCP tunnel listening at:\", tunnel.tcp_socket)         run_echo_server(8000) Copy modal.forward © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.gpu","title":"modal.gpu | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.gpu  GPU configuration shortcodes  The following are the valid str values for the gpu parameter of @stub.function.  “t4” → GPU(T4, count=1) “l4” → GPU(L4, count=1) “a100” → GPU(A100-40GB, count=1) “a10g” → GPU(A10G, count=1) “inf2” → GPU(INFERENTIA2, count=1) “any” → GPU(Any, count=1)  Other configurations can be created using the constructors documented below.  modal.gpu.A100 class A100(modal.gpu._GPUConfig) Copy  NVIDIA A100 Tensor Core GPU class.  The most powerful GPU available in the cloud. Available in 40GiB and 80GiB GPU memory configurations.  def __init__(     self,     *,     count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU.     memory: int = 0,  # Set this to 80 if you want to use the 80GB version. Otherwise defaults to 40. ): Copy modal.gpu.A10G class A10G(modal.gpu._GPUConfig) Copy  NVIDIA A10G Tensor Core GPU class.  A10G GPUs deliver up to 3.3x better ML training performance, 3x better ML inference performance, and 3x better graphics performance, in comparison to NVIDIA T4 GPUs.  def __init__(     self,     *,     count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU. ): Copy modal.gpu.Any class Any(modal.gpu._GPUConfig) Copy  Selects any one of the GPU classes available within Modal, according to availability.  def __init__(self, *, count: int = 1): Copy modal.gpu.L4 class L4(modal.gpu._GPUConfig) Copy  NVIDIA L4 GPU class.  Mid-tier GPU option, providing 24GiB of GPU memory.  def __init__(     self,     count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU. ): Copy modal.gpu.T4 class T4(modal.gpu._GPUConfig) Copy  NVIDIA T4 GPU class.  Low-cost GPU option, providing 16GiB of GPU memory.  def __init__(     self,     count: int = 1,  # Number of GPUs per container. Defaults to 1. Useful if you have very large models that don't fit on a single GPU. ): Copy modal.gpu modal.gpu.A100 modal.gpu.A10G modal.gpu.Any modal.gpu.L4 modal.gpu.T4 © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.is_local","title":"modal.is_local | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.is_local def is_local() -> bool: Copy  Returns if we are currently on the machine launching/deploying a Modal app  Returns True when executed locally on the user’s machine. Returns False when executed from a Modal container in the cloud.  modal.is_local © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Function","title":"modal.functions | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.functions modal.functions.Function class Function(modal.object.Object) Copy  Functions are the basic units of serverless execution on Modal.  Generally, you will not construct a Function directly. Instead, use the @stub.function() decorator on the Stub object for your application.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_parametrized def from_parametrized(     self,     obj,     from_other_workspace: bool,     options: Optional[api_pb2.FunctionOptions],     args: Iterable[Any],     kwargs: Dict[str, Any], ) -> \"_Function\": Copy from_name @classmethod def from_name(     cls: Type[\"_Function\"],     app_name: str,     tag: Optional[str] = None,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> \"_Function\": Copy  Retrieve a function with a given name and tag.  other_function = modal.Function.from_name(\"other-app\", \"function\") Copy lookup @classmethod def lookup(     cls: Type[\"_Function\"],     app_name: str,     tag: Optional[str] = None,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> \"_Function\": Copy  Lookup a function with a given name and tag.  other_function = modal.Function.lookup(\"other-app\", \"function\") Copy stub @property def stub(self) -> \"modal.stub._Stub\": Copy info @property def info(self) -> FunctionInfo: Copy web_url @property def web_url(self) -> str: Copy  URL of a Function running as a web endpoint.  is_generator @property def is_generator(self) -> bool: Copy map @warn_if_generator_is_not_consumed @live_method_gen def map(     self,     *input_iterators,  # one input iterator per argument in the mapped-over function/generator     kwargs={},  # any extra keyword arguments for the function     order_outputs=None,  # defaults to True for regular functions, False for generators     return_exceptions=False,  # whether to propogate exceptions (False) or aggregate them in the results list (True) ) -> AsyncGenerator[Any, None]: Copy  Parallel map over a set of inputs.  Takes one iterator argument per argument in the function being mapped over.  Example:  @stub.function() def my_func(a):     return a ** 2  @stub.local_entrypoint() def main():     assert list(my_func.map([1, 2, 3, 4])) == [1, 4, 9, 16] Copy  If applied to a stub.function, map() returns one result per input and the output order is guaranteed to be the same as the input order. Set order_outputs=False to return results in the order that they are completed instead.  By yielding zero or more than once, mapping over generators can also be used as a “flat map”.  @stub.function() def flat(i: int):     choices = [[], [\"once\"], [\"two\", \"times\"], [\"three\", \"times\", \"a lady\"]]     for item in choices[i % len(choices)]:         yield item  @stub.local_entrypoint() def main():     for item in flat.map(range(10)):         print(item) Copy  return_exceptions can be used to treat exceptions as successful results:  @stub.function() def my_func(a):     if a == 2:         raise Exception(\"ohno\")     return a ** 2  @stub.local_entrypoint() def main():     # [0, 1, UserCodeException(Exception('ohno'))]     print(list(my_func.map(range(3), return_exceptions=True))) Copy for_each def for_each(self, *input_iterators, kwargs={}, ignore_exceptions=False): Copy  Execute function for all outputs, ignoring outputs  Convenient alias for .map() in cases where the function just needs to be called. as the caller doesn’t have to consume the generator to process the inputs.  starmap @warn_if_generator_is_not_consumed @live_method_gen def starmap(     self, input_iterator, kwargs={}, order_outputs=None, return_exceptions=False ) -> AsyncGenerator[Any, None]: Copy  Like map but spreads arguments over multiple function arguments  Assumes every input is a sequence (e.g. a tuple).  Example:  @stub.function() def my_func(a, b):     return a + b  @stub.local_entrypoint() def main():     assert list(my_func.starmap([(1, 2), (3, 4)])) == [3, 7] Copy remote @live_method def remote(self, *args, **kwargs) -> Awaitable[Any]: Copy  Calls the function remotely, executing it with the given arguments and returning the execution’s result.  remote_gen @live_method_gen def remote_gen(self, *args, **kwargs) -> AsyncGenerator[Any, None]: Copy  Calls the generator remotely, executing it with the given arguments and returning the execution’s result.  call def call(self, *args, **kwargs) -> Awaitable[Any]: Copy  Deprecated. Use f.remote or f.remote_gen instead.  shell @live_method def shell(self, *args, **kwargs) -> None: Copy local @synchronizer.nowrap def local(self, *args, **kwargs) -> Any: Copy  Calls the function locally, executing it with the given arguments and returning the execution’s result. This method allows a caller to execute the standard Python function wrapped by Modal.  spawn @live_method def spawn(self, *args, **kwargs) -> Optional[\"_FunctionCall\"]: Copy  Calls the function with the given arguments, without waiting for the results.  Returns a modal.functions.FunctionCall object, that can later be polled or waited for using .get(timeout=...). Conceptually similar to multiprocessing.pool.apply_async, or a Future/Promise in other contexts.  Note: .spawn() on a modal generator function does call and execute the generator, but does not currently return a function handle for polling the result.  get_raw_f def get_raw_f(self) -> Callable[..., Any]: Copy  Return the inner Python object wrapped by this Modal Function.  get_current_stats @live_method def get_current_stats(self) -> FunctionStats: Copy  Return a FunctionStats object describing the current function’s queue and runner counts.  modal.functions.FunctionCall class FunctionCall(modal.object.Object) Copy  A reference to an executed function call.  Constructed using .spawn(...) on a Modal function with the same arguments that a function normally takes. Acts as a reference to an ongoing function call that can be passed around and used to poll or fetch function results at some later time.  Conceptually similar to a Future/Promise/AsyncResult in other contexts and languages.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy get def get(self, timeout: Optional[float] = None): Copy  Get the result of the function call.  This function waits indefinitely by default. It takes an optional timeout argument that specifies the maximum number of seconds to wait, which can be set to 0 to poll for an output immediately.  The returned coroutine is not cancellation-safe.  get_call_graph def get_call_graph(self) -> List[InputInfo]: Copy  Returns a structure representing the call graph from a given root call ID, along with the status of execution for each node.  See modal.call_graph reference page for documentation on the structure of the returned InputInfo items.  cancel def cancel(self): Copy modal.functions.FunctionStats class FunctionStats(object) Copy  Simple data structure storing stats for a running function.  def __init__(self, backlog: int, num_active_runners: int, num_total_runners: int) -> None Copy modal.functions.current_function_call_id def current_function_call_id() -> Optional[str]: Copy  Returns the function call ID for the current input.  Can only be called from Modal function (i.e. in a container context).  from modal import current_function_call_id  @stub.function() def process_stuff():     print(f\"Starting to process input from {current_function_call_id()}\") Copy modal.functions.current_input_id def current_input_id() -> Optional[str]: Copy  Returns the input ID for the current input.  Can only be called from Modal function (i.e. in a container context).  from modal import current_input_id  @stub.function() def process_stuff():     print(f\"Starting to process {current_input_id()}\") Copy modal.functions.gather async def gather(*function_calls: _FunctionCall): Copy  Wait until all Modal function calls have results before returning  Accepts a variable number of FunctionCall objects as returned by Function.spawn().  Returns a list of results from each function call, or raises an exception of the first failing function call.  E.g.  function_call_1 = slow_func_1.spawn() function_call_2 = slow_func_2.spawn()  result_1, result_2 = gather(function_call_1, function_call_2) Copy modal.functions modal.functions.Function clone from_id deps from_parametrized from_name lookup stub info web_url is_generator map for_each starmap remote remote_gen call shell local spawn get_raw_f get_current_stats modal.functions.FunctionCall clone from_id deps get get_call_graph cancel modal.functions.FunctionStats modal.functions.current_function_call_id modal.functions.current_input_id modal.functions.gather © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Queue","title":"modal.queue | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.queue modal.queue.Queue class Queue(modal.object.StatefulObject) Copy  Distributed, FIFO queue for data flow in Modal apps.  The queue can contain any object serializable by cloudpickle, including Modal objects.  Usage  Create a new Queue with Queue.new(), then assign it to a stub or function.  from modal import Queue, Stub  stub = Stub() stub.my_queue = Queue.new()  @stub.local_entrypoint() def main():     stub.my_queue.put(\"some value\")     stub.my_queue.put(123)      assert stub.my_queue.get() == \"some value\"     assert stub.my_queue.get() == 123 Copy  For more examples, see the guide.  clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy new @staticmethod def new(): Copy  Create an empty Queue.  persisted @staticmethod def persisted(     label: str, namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE, environment_name: Optional[str] = None ) -> \"_Queue\": Copy  Deploy a Modal app containing this object.  The deployed object can then be imported from other apps, or by calling Queue.from_name(label) from that same app.  Examples  # In one app: stub.queue = Queue.persisted(\"my-queue\")  # Later, in another app or Python file: stub.queue = Queue.from_name(\"my-queue\") Copy get @live_method def get(self, block: bool = True, timeout: Optional[float] = None) -> Optional[Any]: Copy  Remove and return the next object in the queue.  If block is True (the default) and the queue is empty, get will wait indefinitely for an object, or until timeout if specified. Raises a native queue.Empty exception if the timeout is reached.  If block is False, get returns None immediately if the queue is empty. The timeout is ignored in this case.  get_many @live_method def get_many(self, n_values: int, block: bool = True, timeout: Optional[float] = None) -> List[Any]: Copy  Remove and return up to n_values objects from the queue.  If there are fewer than n_values items in the queue, return all of them.  If block is True (the default) and the queue is empty, get will wait indefinitely for at least 1 object to be present, or until timeout if specified. Raises the stdlib’s queue.Empty exception if the timeout is reached.  If block is False, get returns None immediately if the queue is empty. The timeout is ignored in this case.  put @live_method def put(self, v: Any, block: bool = True, timeout: Optional[float] = None) -> None: Copy  Add an object to the end of the queue.  If block is True and the queue is full, this method will retry indefinitely or until timeout if specified. Raises the stdlib’s queue.Full exception if the timeout is reached. If blocking it is not recommended to omit the timeout, as the operation could wait indefinitely.  If block is False, this method raises queue.Full immediately if the queue is full. The timeout is ignored in this case.  put_many @live_method def put_many(self, vs: List[Any], block: bool = True, timeout: Optional[float] = None) -> None: Copy  Add several objects to the end of the queue.  If block is True and the queue is full, this method will retry indefinitely or until timeout if specified. Raises the stdlib’s queue.Full exception if the timeout is reached. If blocking it is not recommended to omit the timeout, as the operation could wait indefinitely.  If block is False, this method raises queue.Full immediately if the queue is full. The timeout is ignored in this case.  len @live_method def len(self) -> int: Copy  Return the number of objects in the queue.  modal.queue modal.queue.Queue clone from_id deps from_name lookup new persisted get get_many put put_many len © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Image","title":"modal.Image | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Image class Image(modal.object.Object) Copy  Base class for container images to run functions in.  Do not construct this class directly; instead use one of its static factory methods, such as modal.Image.debian_slim, modal.Image.from_registry, or modal.Image.conda.  def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy extend def extend(self, **kwargs) -> \"_Image\": Copy  Extend an image (named “base”) with additional options or commands.  This is a low-level command. Generally, you should prefer using functions like Image.pip_install or Image.apt_install if possible.  Example  image = modal.Image.debian_slim().extend(     dockerfile_commands=[         \"FROM base\",         \"WORKDIR /pkg\",         'RUN echo \"hello world\" > hello.txt',     ],     secrets=[secret1, secret2], ) Copy copy_mount @typechecked def copy_mount(self, mount: _Mount, remote_path: Union[str, Path] = \".\") -> \"_Image\": Copy  Copy the entire contents of a modal.Mount into an image. Useful when files only available locally are required during the image build process.  Example  static_images_dir = \"./static\" # place all static images in root of mount mount = modal.Mount.from_local_dir(static_images_dir, remote_path=\"/\") # place mount's contents into /static directory of image. image = modal.Image.debian_slim().copy_mount(mount, remote_path=\"/static\") Copy copy_local_file def copy_local_file(self, local_path: Union[str, Path], remote_path: Union[str, Path] = \"./\") -> \"_Image\": Copy  Copy a file into the image as a part of building it.  This works in a similar way to COPY in a Dockerfile.  copy_local_dir def copy_local_dir(self, local_path: Union[str, Path], remote_path: Union[str, Path] = \".\") -> \"_Image\": Copy  Copy a directory into the image as a part of building the image.  This works in a similar way to COPY in a Dockerfile.  pip_install @typechecked def pip_install(     self,     *packages: Union[str, List[str]],  # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]     find_links: Optional[str] = None,  # Passes -f (--find-links) pip install     index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install     extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install     pre: bool = False,  # Passes --pre (allow pre-releases) to pip install     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install a list of Python packages using pip.  Example  image = modal.Image.debian_slim().pip_install(\"click\", \"httpx~=0.23.3\") Copy pip_install_private_repos @typechecked def pip_install_private_repos(     self,     *repositories: str,     git_user: str,     find_links: Optional[str] = None,  # Passes -f (--find-links) pip install     index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install     extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install     pre: bool = False,  # Passes --pre (allow pre-releases) to pip install     gpu: GPU_T = None,     secrets: Sequence[_Secret] = [],     force_build: bool = False, ) -> \"_Image\": Copy  Install a list of Python packages from private git repositories using pip.  This method currently supports Github and Gitlab only.  Github: Provide a modal.Secret that contains a GITHUB_TOKEN key-value pair Gitlab: Provide a modal.Secret that contains a GITLAB_TOKEN key-value pair  These API tokens should have permissions to read the list of private repositories provided as arguments.  We recommend using Github’s ‘fine-grained’ access tokens. These tokens are repo-scoped, and avoid granting read permission across all of a user’s private repos.  Example  image = (     modal.Image     .debian_slim()     .pip_install_private_repos(         \"github.com/ecorp/private-one@1.0.0\",         \"github.com/ecorp/private-two@main\"         \"github.com/ecorp/private-three@d4776502\"         # install from 'inner' directory on default branch.         \"github.com/ecorp/private-four#subdirectory=inner\",         git_user=\"erikbern\",         secrets=[modal.Secret.from_name(\"github-read-private\")],     ) ) Copy pip_install_from_requirements @typechecked def pip_install_from_requirements(     self,     requirements_txt: str,  # Path to a requirements.txt file.     find_links: Optional[str] = None,  # Passes -f (--find-links) pip install     *,     index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install     extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install     pre: bool = False,  # Passes --pre (allow pre-releases) to pip install     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install a list of Python packages from a local requirements.txt file.  pip_install_from_pyproject @typechecked def pip_install_from_pyproject(     self,     pyproject_toml: str,     optional_dependencies: List[str] = [],     *,     find_links: Optional[str] = None,  # Passes -f (--find-links) pip install     index_url: Optional[str] = None,  # Passes -i (--index-url) to pip install     extra_index_url: Optional[str] = None,  # Passes --extra-index-url to pip install     pre: bool = False,  # Passes --pre (allow pre-releases) to pip install     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install dependencies specified by a local pyproject.toml file.  optional_dependencies is a list of the keys of the optional-dependencies section(s) of the pyproject.toml file (e.g. test, doc, experiment, etc). When provided, all of the packages in each listed section are installed as well.  poetry_install_from_file @typechecked def poetry_install_from_file(     self,     poetry_pyproject_toml: str,     # Path to the lockfile. If not provided, uses poetry.lock in the same directory.     poetry_lockfile: Optional[str] = None,     # If set to True, it will not use poetry.lock     ignore_lockfile: bool = False,     # If set to True, use old installer. See https://github.com/python-poetry/poetry/issues/3336     old_installer: bool = False,     force_build: bool = False,     # Selected optional dependency groups to install (See https://python-poetry.org/docs/cli/#install)     with_: List[str] = [],     # Selected optional dependency groups to exclude (See https://python-poetry.org/docs/cli/#install)     without: List[str] = [],     # Only install dependency groups specifed in this list.     only: List[str] = [],     *,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install poetry dependencies specified by a local pyproject.toml file.  If not provided as argument the path to the lockfile is inferred. However, the file has to exist, unless ignore_lockfile is set to True.  Note that the root project of the poetry project is not installed, only the dependencies. For including local packages see modal.Mount.from_local_python_packages  dockerfile_commands @typechecked def dockerfile_commands(     self,     *dockerfile_commands: Union[str, List[str]],     context_files: Dict[str, str] = {},     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None,     # modal.Mount with local files to supply as build context for COPY commands     context_mount: Optional[_Mount] = None,     force_build: bool = False, ) -> \"_Image\": Copy  Extend an image with arbitrary Dockerfile-like commands.  run_commands @typechecked def run_commands(     self,     *commands: Union[str, List[str]],     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None,     force_build: bool = False, ) -> \"_Image\": Copy  Extend an image with a list of shell commands to run.  conda @staticmethod @typechecked def conda(python_version: str = \"3.9\", force_build: bool = False) -> \"_Image\": Copy  A Conda base image, using miniconda3 and derived from the official Docker Hub image. In most cases, using Image.micromamba() with micromamba_install is recommended over Image.conda(), as it leads to significantly faster image build times.  conda_install @typechecked def conda_install(     self,     *packages: Union[str, List[str]],  # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]     channels: List[str] = [],  # A list of Conda channels, eg. [\"conda-forge\", \"nvidia\"]     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install a list of additional packages using Conda. Note that in most cases, using Image.micromamba() with micromamba_install is recommended over conda_install, as it leads to significantly faster image build times.  conda_update_from_environment @typechecked def conda_update_from_environment(     self,     environment_yml: str,     force_build: bool = False,     *,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Update a Conda environment using dependencies from a given environment.yml file.  micromamba @staticmethod @typechecked def micromamba(     python_version: str = \"3.9\",     force_build: bool = False, ) -> \"_Image\": Copy  A Micromamba base image. Micromamba allows for fast building of small Conda-based containers. In most cases it will be faster than using Image.conda().  micromamba_install @typechecked def micromamba_install(     self,     # A list of Python packages, eg. [\"numpy\", \"matplotlib>=3.5.0\"]     *packages: Union[str, List[str]],     # A list of Conda channels, eg. [\"conda-forge\", \"nvidia\"]     channels: List[str] = [],     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install a list of additional packages using micromamba.  from_registry @staticmethod @typechecked def from_registry(     tag: str,     *,     secret: Optional[_Secret] = None,     setup_dockerfile_commands: List[str] = [],     force_build: bool = False,     add_python: Optional[str] = None,     **kwargs, ) -> \"_Image\": Copy  Build a Modal image from a public image registry, such as Docker Hub.  The image must be built for the linux/amd64 platform and have Python 3.7 or above installed and available on PATH as python. It should also have pip.  If your image does not come with Python installed, you can use the add_python parameter to specify a version of Python to add to the image. Supported versions are 3.8, 3.9, 3.10, and 3.11. For Alpine-based images, use 3.8-musl through 3.11-musl, which are statically-linked Python installations.  You may also use setup_dockerfile_commands to run Dockerfile commands before the remaining commands run. This might be useful if you want a custom Python installation or to set a SHELL. Prefer run_commands() when possible though.  To authenticate against a private registry with static credentials, you may set the secret parameter to a modal.Secret containing a username (REGISTRY_USERNAME) and an access token or password (REGISTRY_PASSWORD).  To authenticate against private registries with credentials from a cloud provider, use Image.from_gcp_artifact_registry() or Image.from_aws_ecr().  Examples  modal.Image.from_registry(\"python:3.11-slim-bookworm\") modal.Image.from_registry(\"ubuntu:22.04\", add_python=\"3.11\") modal.Image.from_registry(\"alpine:3.18.3\", add_python=\"3.11-musl\") Copy from_dockerhub @staticmethod @typechecked def from_dockerhub(     tag: str,     setup_dockerfile_commands: List[str] = [],     force_build: bool = False,     **kwargs, ): Copy from_gcp_artifact_registry @staticmethod @typechecked def from_gcp_artifact_registry(     tag: str,     secret: Optional[_Secret] = None,     *,     setup_dockerfile_commands: List[str] = [],     force_build: bool = False,     add_python: Optional[str] = None,     **kwargs, ) -> \"_Image\": Copy  Build a Modal image from a private image in GCP Artifact Registry.  You will need to pass a modal.Secret containing your GCP service account key as SERVICE_ACCOUNT_JSON. This can be done from the Secrets page. The service account needs to have at least an “Artifact Registry Reader” role.  See Image.from_registry() for information about the other parameters.  Example  modal.Image.from_gcp_artifact_registry(     \"us-east1-docker.pkg.dev/my-project-1234/my-repo/my-image:my-version\",     secret=modal.Secret.from_name(\"my-gcp-secret\"),     add_python=\"3.11\", ) Copy from_aws_ecr @staticmethod @typechecked def from_aws_ecr(     tag: str,     secret: Optional[_Secret] = None,     *,     setup_dockerfile_commands: List[str] = [],     force_build: bool = False,     add_python: Optional[str] = None,     **kwargs, ) -> \"_Image\": Copy  Build a Modal image from a private image in AWS Elastic Container Registry (ECR).  You will need to pass a modal.Secret containing an AWS key (AWS_ACCESS_KEY_ID) and secret (AWS_SECRET_ACCESS_KEY) with permissions to access the target ECR registry.  IAM configuration details can be found in the AWS documentation for “Private repository policies”.  See Image.from_registry() for information about the other parameters.  Example  modal.Image.from_aws_ecr(     \"000000000000.dkr.ecr.us-east-1.amazonaws.com/my-private-registry:my-version\",     secret=modal.Secret.from_name(\"aws\"),     add_python=\"3.11\", ) Copy from_dockerfile @staticmethod @typechecked def from_dockerfile(     path: Union[str, Path],     context_mount: Optional[         _Mount     ] = None,  # modal.Mount with local files to supply as build context for COPY commands     force_build: bool = False,     *,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None,     add_python: Optional[str] = None, ) -> \"_Image\": Copy  Build a Modal image from a local Dockerfile.  If your Dockerfile does not have Python installed, you can use the add_python parameter to specify a version of Python to add to the image. Supported versions are 3.8, 3.9, 3.10, and 3.11. For Alpine-based images, use 3.8-musl through 3.11-musl, which are statically-linked Python installations.  Example  image = modal.Image.from_dockerfile(\"./Dockerfile\", add_python=\"3.10\") Copy debian_slim @staticmethod @typechecked def debian_slim(python_version: Optional[str] = None, force_build: bool = False) -> \"_Image\": Copy  Default image, based on the official python:X.Y.Z-slim-bullseye Docker images.  apt_install @typechecked def apt_install(     self,     *packages: Union[str, List[str]],  # A list of packages, e.g. [\"ssh\", \"libpq-dev\"]     force_build: bool = False,     secrets: Sequence[_Secret] = [],     gpu: GPU_T = None, ) -> \"_Image\": Copy  Install a list of Debian packages using apt.  Example  image = modal.Image.debian_slim().apt_install(\"git\") Copy run_function @typechecked def run_function(     self,     raw_f: Callable[[], Any],     *,     secret: Optional[_Secret] = None,  # An optional Modal Secret with environment variables for the container     secrets: Sequence[_Secret] = (),  # Plural version of `secret` when multiple secrets are needed     gpu: GPU_T = None,  # GPU specification as string (\"any\", \"T4\", \"A10G\", ...) or object (`modal.GPU.A100()`, ...)     mounts: Sequence[_Mount] = (),     shared_volumes: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},     network_file_systems: Dict[Union[str, os.PathLike], _NetworkFileSystem] = {},     cpu: Optional[float] = None,  # How many CPU cores to request. This is a soft limit.     memory: Optional[int] = None,  # How much memory to request, in MiB. This is a soft limit.     timeout: Optional[int] = 86400,  # Maximum execution time of the function in seconds.     force_build: bool = False, ) -> \"_Image\": Copy  Run user-defined function raw_f as an image build step. The function runs just like an ordinary Modal function, and any kwargs accepted by @stub.function (such as Mounts, NetworkFileSystems, and resource requests) can be supplied to it. After it finishes execution, a snapshot of the resulting container file system is saved as an image.  Note  Only the source code of raw_f, the contents of **kwargs, and any referenced global variables are used to determine whether the image has changed and needs to be rebuilt. If this function references other functions or variables, the image will not be rebuilt if you make changes to them. You can force a rebuild by changing the function’s source code itself.  Example   def my_build_function():     open(\"model.pt\", \"w\").write(\"parameters!\")  image = (     modal.Image         .debian_slim()         .pip_install(\"torch\")         .run_function(my_build_function, secrets=[...], mounts=[...]) ) Copy env @typechecked def env(self, vars: Dict[str, str]) -> \"_Image\": Copy  Sets the environmental variables of the image.  Example  image = (     modal.Image.conda()         .env({\"CONDA_OVERRIDE_CUDA\": \"11.2\"})         .conda_install(\"jax\", \"cuda-nvcc\", channels=[\"conda-forge\", \"nvidia\"])         .pip_install(\"dm-haiku\", \"optax\") ) Copy workdir @typechecked def workdir(self, path: str) -> \"_Image\": Copy  Sets the working directory for subequent image build steps.  Example  image = (     modal.Image.debian_slim()         .run_commands(\"git clone https://xyz app\")         .workdir(\"/app\")         .run_commands(\"yarn install\") ) Copy imports @contextlib.contextmanager def imports(self): Copy run_inside def run_inside(self): Copy  Image.run_inside is deprecated - use Image.imports instead.  Usage:  with image.imports():     import torch Copy modal.Image clone from_id deps extend copy_mount copy_local_file copy_local_dir pip_install pip_install_private_repos pip_install_from_requirements pip_install_from_pyproject poetry_install_from_file dockerfile_commands run_commands conda conda_install conda_update_from_environment micromamba micromamba_install from_registry from_dockerhub from_gcp_artifact_registry from_aws_ecr from_dockerfile debian_slim apt_install run_function env workdir imports run_inside © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Volume","title":"modal.Volume | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Volume class Volume(modal.object.StatefulObject) Copy  A writeable volume that can be used to share files between one or more Modal functions.  The contents of a volume is exposed as a filesystem. You can use it to share data between different functions, or to persist durable state across several instances of the same function.  Unlike a networked filesystem, you need to explicitly reload the volume to see changes made since it was mounted. Similarly, you need to explicitly commit any changes you make to the volume for the changes to become visible outside the current container.  Concurrent modification is supported, but concurrent modifications of the same files should be avoided! Last write wins in case of concurrent modification of the same file - any data the last writer didn’t have when committing changes will be lost!  As a result, volumes are typically not a good fit for use cases where you need to make concurrent modifications to the same file (nor is distributed file locking supported).  Volumes can only be committed and reloaded if there are no open files for the volume - attempting to reload or commit with open files will result in an error.  Usage  import modal  stub = modal.Stub() stub.volume = modal.Volume.new()  @stub.function(volumes={\"/root/foo\": stub.volume}) def f():     with open(\"/root/foo/bar.txt\", \"w\") as f:         f.write(\"hello\")     stub.app.volume.commit()  # Persist changes  @stub.function(volumes={\"/root/foo\": stub.volume}) def g():     stub.app.volume.reload()  # Fetch latest changes     with open(\"/root/foo/bar.txt\", \"r\") as f:         print(f.read()) Copy def __init__(self, *args, **kwargs): Copy clone def clone(self: O) -> O: Copy  Clone a given hydrated object.  from_id @classmethod def from_id(cls: Type[O], object_id: str, client: Optional[_Client] = None) -> O: Copy  Retrieve an object from its unique ID (accessed through obj.object_id).  deps @property def deps(self) -> Callable[..., List[\"_Object\"]]: Copy from_name @classmethod def from_name(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> O: Copy  Retrieve an object with a given name.  Useful for referencing secrets, as well as calling a function from a different app. Use this when attaching the object to a stub or function.  Examples  stub.my_secret = Secret.from_name(\"my-secret\") stub.my_volume = Volume.from_name(\"my-volume\") stub.my_queue = Queue.from_name(\"my-queue\") stub.my_dict = Dict.from_name(\"my-dict\") Copy lookup @classmethod def lookup(     cls: Type[O],     app_name: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client: Optional[_Client] = None,     environment_name: Optional[str] = None, ) -> O: Copy  Lookup an object with a given name.  This is a general-purpose method for objects like functions, network file systems, and secrets. It gives a reference to the object in a running app.  Examples  my_secret = Secret.lookup(\"my-secret\") my_volume = Volume.lookup(\"my-volume\") my_queue = Queue.lookup(\"my-queue\") my_dict = Dict.lookup(\"my-dict\") Copy new @staticmethod def new() -> \"_Volume\": Copy  Construct a new volume, which is empty by default.  persisted @staticmethod def persisted(     label: str,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     environment_name: Optional[str] = None, ) -> \"_Volume\": Copy  Deploy a Modal app containing this object. This object can then be imported from other apps using the returned reference, or by calling modal.Volume.from_name(label) (or the equivalent method on respective class).  Example Usage  import modal  volume = modal.Volume.persisted(\"my-volume\")  stub = modal.Stub()  # Volume refers to the same object, even across instances of `stub`. @stub.function(volumes={\"/vol\": volume}) def f():     pass Copy commit @live_method def commit(self): Copy  Commit changes to the volume and fetch any other changes made to the volume by other containers.  Unless background commits are enabled, committing always triggers a reload after saving changes.  If successful, the changes made are now persisted in durable storage and available to other containers accessing the volume.  Committing will fail if there are open files for the volume.  reload @live_method def reload(self): Copy  Make latest committed state of volume available in the running container.  Uncommitted changes to the volume, such as new or modified files, will be preserved during reload. Uncommitted changes will shadow any changes made by other writers - e.g. if you have an uncommitted modified a file that was also updated by another writer you will not see the other change.  Reloading will fail if there are open files for the volume.  iterdir @live_method_gen def iterdir(self, path: str) -> Iterator[api_pb2.VolumeListFilesEntry]: Copy  Iterate over all files in a directory in the volume.  Passing a directory path lists all files in the directory (names are relative to the directory) Passing a file path returns a list containing only that file’s listing description Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths) listdir @live_method def listdir(self, path: str) -> List[api_pb2.VolumeListFilesEntry]: Copy  List all files under a path prefix in the modal.Volume.  Passing a directory path lists all files in the directory Passing a file path returns a list containing only that file’s listing description Passing a glob path (including at least one * or ** sequence) returns all files matching that glob path (using absolute paths) read_file @live_method_gen def read_file(self, path: Union[str, bytes]) -> Iterator[bytes]: Copy  Read a file from the modal.Volume.  Example:  vol = modal.Volume.lookup(\"my-modal-volume\") data = b\"\" for chunk in vol.read_file(\"1mb.csv\"):     data += chunk print(len(data))  # == 1024 * 1024 Copy remove_file @live_method def remove_file(self, path: Union[str, bytes], recursive: bool = False) -> None: Copy  Remove a file or directory from a volume.  modal.Volume clone from_id deps from_name lookup new persisted commit reload iterdir listdir read_file remove_file © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.web_endpoint","title":"modal.web_endpoint | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.web_endpoint @typechecked def web_endpoint(     _warn_parentheses_missing=None,     *,     method: str = \"GET\",  # REST method for the created endpoint.     label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.     wait_for_response: bool = True,  # Whether requests should wait for and return the function response.     custom_domains: Optional[         Iterable[str]     ] = None,  # Create an endpoint using a custom domain fully-qualified domain name. ) -> Callable[[Callable[..., Any]], _PartialFunction]: Copy  Register a basic web endpoint with this application.  This is the simple way to create a web endpoint on Modal. The function behaves as a FastAPI handler and should return a response object to the caller.  Endpoints created with @stub.web_endpoint are meant to be simple, single request handlers and automatically have CORS enabled. For more flexibility, use @stub.asgi_app.  To learn how to use Modal with popular web frameworks, see the guide on web endpoints.  modal.web_endpoint © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/serve","title":"modal serve | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal serve  Run a web endpoint(s) associated with a Modal stub and hot-reload code.  Examples:  modal serve hello_world.py Copy  Usage:  modal serve [OPTIONS] STUB_REF Copy  Arguments:  STUB_REF: Path to a Python file with a stub. [required]  Options:  --timeout FLOAT --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal serve © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.create_package_mounts","title":"modal.create_package_mounts | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.create_package_mounts @typechecked def create_package_mounts(module_names: Sequence[str]): Copy modal.create_package_mounts © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.enter","title":"modal.enter | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.enter @typechecked def enter(     _warn_parentheses_missing=None, ) -> Callable[[Union[Callable[[Any], Any], _PartialFunction]], _PartialFunction]: Copy modal.enter © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Error","title":"modal.Error | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Error class Error(Exception) Copy  Base error class for all Modal errors.  Usage  import modal  try:     ... except modal.Error:     # Catch any exception raised by Modal's systems.     print(\"Responding to error...\") Copy modal.Error © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Tunnel","title":"modal.Tunnel | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Tunnel class Tunnel(object) Copy  A port forwarded from within a running Modal container. Created by modal.forward().  This is an EXPERIMENTAL API and may change in the future.  def __init__(self, host: str, port: int, unencrypted_host: str, unencrypted_port: int) -> None Copy url @property def url(self) -> str: Copy  Get the public HTTPS URL of the forwarded port.  tls_socket @property def tls_socket(self) -> Tuple[str, int]: Copy  Get the public TLS socket as a (host, port) tuple.  tcp_socket @property def tcp_socket(self) -> Tuple[str, int]: Copy  Get the public TCP socket as a (host, port) tuple.  modal.Tunnel url tls_socket tcp_socket © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/run","title":"modal run | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal run  Run a Modal function or local entrypoint.  FUNC_REF should be of the format {file or module}::{function name}. Alternatively, you can refer to the function via the stub:  {file or module}::{stub variable name}.{function name}  Examples:  To run the hello_world function (or local entrypoint) in my_app.py:  modal run my_app.py::hello_world Copy  If your module only has a single stub called stub and your stub has a single local entrypoint (or single function), you can omit the stub and function parts:  modal run my_app.py Copy  Instead of pointing to a file, you can also use the Python module path:  modal run my_project.my_app Copy  Usage:  modal run [OPTIONS] FUNC_REF Copy  Options:  -q, --quiet: Don’t show Modal progress indicators. -d, --detach: Don’t stop the app if the local process dies or disconnects. -e, --env TEXT: Environment to interact with.  If not specified, Modal will use the default environment of your current profile, or the MODAL_ENVIRONMENT variable. Otherwise, raises an error if the workspace has multiple environments.  --help: Show this message and exit. modal run © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.Retries","title":"modal.Retries | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.Retries class Retries(object) Copy  Adds a retry policy to a Modal function.  Usage  import modal stub = modal.Stub()  # Basic configuration. # This sets a policy of max 4 retries with 1-second delay between failures. @stub.function(retries=4) def f():     pass   # Fixed-interval retries with 3-second delay between failures. @stub.function(     retries=modal.Retries(         max_retries=2,         backoff_coefficient=1.0,         initial_delay=3.0,     ) ) def g():     pass   # Exponential backoff, with retry delay doubling after each failure. @stub.function(     retries=modal.Retries(         max_retries=4,         backoff_coefficient=2.0,         initial_delay=1.0,     ) ) def h():     pass Copy def __init__(     self,     *,     # The maximum number of retries that can be made in the presence of failures.     max_retries: int,     # Coefficent controlling how much the retry delay increases each retry attempt.     # A backoff coefficient of 1.0 creates fixed-delay retries where the delay period will always equal the initial delay.     backoff_coefficient: float = 2.0,     # Number of seconds that must elapse before the first retry occurs.     initial_delay: float = 1.0,     # Maximum length of retry delay in seconds, preventing the delay from growing infinitely.     max_delay: float = 60.0, ): Copy  Construct a new retries policy, supporting exponential and fixed-interval delays via a backoff coefficient.  modal.Retries © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/changelog","title":"Changelog | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config Changelog  This changelog documents user-facing updates (features, enhancements, fixes, and deprecations) to the modal client library. Patch releases are made on every change.  The client library is still in pre-1.0 development, and sometimes breaking changes are necessary. We try to minimize them and publish deprecation warnings / migration guides in advance, typically providing a transition window of several months.  We appreciate your patience while we speedily work towards a stable release of the client.  Latest 0.56.4590 (2024-01-13)  modal serve: Setting MODAL_LOGLEVEL=DEBUG now displays which files cause an app reload during serve  0.56.4570 (2024-01-12) modal run cli command now properly propagates --env values to object lookups in global scope of user code Changelog Latest 0.56.4590 (2024-01-13) 0.56.4570 (2024-01-12) © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/cli/setup","title":"modal setup | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Search K Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal setup  Bootstrap Modal’s configuration.  Usage:  modal setup [OPTIONS] Copy  Options:  --profile TEXT --help: Show this message and exit. modal setup © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.wsgi_app","title":"modal.wsgi_app | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.wsgi_app @typechecked def wsgi_app(     _warn_parentheses_missing=None,     *,     label: Optional[str] = None,  # Label for created endpoint. Final subdomain will be <workspace>--<label>.modal.run.     wait_for_response: bool = True,  # Whether requests should wait for and return the function response.     custom_domains: Optional[         Iterable[str]     ] = None,  # Create an endpoint using a custom domain fully-qualified domain name. ) -> Callable[[Callable[..., Any]], _PartialFunction]: Copy  Decorator for registering a WSGI app with a Modal function.  Web Server Gateway Interface (WSGI) is a standard for synchronous Python web apps. It has been succeeded by the ASGI interface which is compatible with ASGI and supports additional functionality such as web sockets. Modal supports ASGI via asgi_app.  Usage:  from typing import Callable  @stub.function() @modal.wsgi_app() def create_wsgi() -> Callable:     ... Copy  To learn how to use this decorator with popular web frameworks, see the guide on web endpoints.  modal.wsgi_app © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.SharedVolume","title":"modal.shared_volume | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.shared_volume modal.shared_volume.SharedVolume class SharedVolume(object) Copy def __init__(self, *args, **kwargs): Copy  SharedVolume(...) is deprecated. Please use NetworkFileSystem.new(...) instead.  new @staticmethod def new(*args, **kwargs): Copy  SharedVolume.new(...) is deprecated. Please use NetworkFileSystem.new(...) instead.  persisted @staticmethod def persisted(*args, **kwargs): Copy  SharedVolume.persisted(...) is deprecated. Please use NetworkFileSystem.persisted(...) instead.  modal.shared_volume modal.shared_volume.SharedVolume new persisted © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"},{"url":"https://modal.com/docs/reference/modal.runner","title":"modal.runner | Modal Docs","date":"2024-01-16","content":"Examples Guide Reference Log In Sign Up Changelog CLI Reference modal app modal config modal container modal deploy modal environment modal launch modal nfs modal profile modal run modal secret modal serve modal setup modal shell modal token modal volume API Reference modal.Client modal.Cls modal.Cron modal.Dict modal.Error modal.Function modal.Image modal.Mount modal.NetworkFileSystem modal.Period modal.Proxy modal.Queue modal.Retries modal.Sandbox modal.Secret modal.SharedVolume modal.Stub modal.Tunnel modal.Volume modal.asgi_app modal.build modal.call_graph modal.create_package_mounts modal.enter modal.exit modal.forward modal.gpu modal.is_local modal.method modal.runner modal.web_endpoint modal.wsgi_app modal.exception modal.config modal.runner modal.runner.DeployResult class DeployResult(object) Copy  Dataclass representing the result of deploying an app.  def __init__(self, app_id: str) -> None Copy modal.runner.deploy_stub async def deploy_stub(     stub: _Stub,     name: str = None,     namespace=api_pb2.DEPLOYMENT_NAMESPACE_WORKSPACE,     client=None,     stdout=None,     show_progress=True,     environment_name: Optional[str] = None,     public: bool = False, ) -> DeployResult: Copy  Deploy an app and export its objects persistently.  Typically, using the command-line tool modal deploy <module or script> should be used, instead of this method.  Usage:  if __name__ == \"__main__\":     deploy_stub(stub) Copy  Deployment has two primary purposes:  Persists all of the objects in the app, allowing them to live past the current app run. For schedules this enables headless “cron”-like functionality where scheduled functions continue to be invoked after the client has disconnected. Allows for certain kinds of these objects, deployment objects, to be referred to and used by other apps. modal.runner.interactive_shell async def interactive_shell(_function: _Function, cmd: str, environment_name: str = \"\"): Copy  Run an interactive shell (like bash) within the image for this app.  This is useful for online debugging and interactive exploration of the contents of this image. If cmd is optionally provided, it will be run instead of the default shell inside this image.  Example  import modal  stub = modal.Stub(image=modal.Image.debian_slim().apt_install(\"vim\")) Copy  You can now run this using  modal shell script.py --cmd /bin/bash Copy modal.runner modal.runner.DeployResult modal.runner.deploy_stub modal.runner.interactive_shell © 2024 About Status Documentation Slack Community Pricing Examples Terms Privacy"}]